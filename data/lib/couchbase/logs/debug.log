[ns_server:info,2019-03-13T10:24:42.431Z,nonode@nohost:<0.89.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2019-03-13T10:24:42.449Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10},
 {ipv6,false}]
[ns_server:warn,2019-03-13T10:24:42.449Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.450Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:warn,2019-03-13T10:24:42.451Z,nonode@nohost:<0.89.0>:ns_server:log_pending:32]not overriding parameter ipv6, which is given from command line
[error_logger:info,2019-03-13T10:24:42.471Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.138.0>},
                       {name,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:42.481Z,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {4,9,125}
Runtime info: [{otp_release,"R16B03-1"},
               {erl_version,"5.10.4.0.0.1"},
               {erl_version_long,
                   "Erlang R16B03-1 (erts-5.10.4.0.0.1) [source-6d69bef] [64-bit] [smp:4:4] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2019,3,13},{10,24,42}}},
               {memory,
                   [{total,110404888},
                    {processes,9290504},
                    {processes_used,9289456},
                    {system,101114384},
                    {atom,339441},
                    {atom_used,322567},
                    {binary,51752},
                    {code,7796127},
                    {ets,2241560}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,path_config,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster',io_lib_fread,'ale_logger-xdcr',
                    otp_internal,ns_log_sink,ale_disk_sink,misc,couch_util,
                    ns_server,filelib,cpu_sup,memsup,disksup,os_mon,io,
                    release_handler,overload,alarm_handler,sasl,timer,
                    tftp_sup,httpd_sup,httpc_handler_sup,httpc_cookie,
                    inets_trace,httpc_manager,httpc,httpc_profile_sup,
                    httpc_sup,ftp_sup,inets_sup,inets_app,ssl,lhttpc_manager,
                    lhttpc_sup,lhttpc,tls_connection_sup,ssl_session_cache,
                    ssl_pkix_db,ssl_manager,ssl_sup,ssl_app,crypto_server,
                    crypto_sup,crypto_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_validator,beam_z,
                    beam_flatten,beam_trim,beam_receive,beam_bsm,beam_peep,
                    beam_dead,beam_split,beam_type,beam_bool,beam_except,
                    beam_clean,beam_utils,beam_block,beam_jump,beam_a,
                    v3_codegen,v3_life,v3_kernel,sys_core_dsetel,erl_bifs,
                    sys_core_fold,cerl_trees,sys_core_inline,core_lib,cerl,
                    v3_core,erl_bits,erl_expand_records,sys_pre_expand,sofs,
                    erl_internal,sets,ordsets,erl_lint,compile,
                    dynamic_compile,ale_utils,io_lib_pretty,io_lib_format,
                    io_lib,ale_codegen,dict,ale,ale_dynamic_sup,ale_sup,
                    ale_app,epp,ns_bootstrap,child_erlang,file_io_server,
                    orddict,erl_eval,file,c,kernel_config,user_io,user_sup,
                    supervisor_bridge,standard_error,code_server,unicode,
                    hipe_unified_loader,gb_sets,ets,binary,code,file_server,
                    net_kernel,global_group,erl_distribution,filename,
                    inet_gethost_native,os,inet_parse,inet,inet_udp,
                    inet_config,inet_db,global,gb_trees,rpc,supervisor,kernel,
                    application_master,sys,application,gen_server,erl_parse,
                    proplists,erl_scan,lists,application_controller,proc_lib,
                    gen,gen_event,error_logger,heart,error_handler,
                    erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {applications,
                   [{lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {os_mon,"CPO  CXC 138 46","2.2.14"},
                    {public_key,"Public key infrastructure","0.21"},
                    {asn1,"The Erlang ASN1 compiler version 2.0.4","2.0.4"},
                    {kernel,"ERTS  CXC 138 10","2.16.4"},
                    {ale,"Another Logger for Erlang","6.0.0-1693-community"},
                    {inets,"INETS  CXC 138 49","5.9.8"},
                    {ns_server,"Couchbase server","6.0.0-1693-community"},
                    {crypto,"CRYPTO version 2","3.2"},
                    {ssl,"Erlang/OTP SSL application","5.3.3"},
                    {sasl,"SASL  CXC 138 11","2.3.4"},
                    {stdlib,"ERTS  CXC 138 10","1.19.4"}]},
               {pre_loaded,
                   [erts_internal,erlang,erl_prim_loader,prim_zip,zlib,
                    prim_file,prim_inet,prim_eval,init,otp_ring0]},
               {process_count,105},
               {node,nonode@nohost},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,code_server,ale_stats_events,
                    ns_server_cluster_sup,lhttpc_manager,
                    application_controller,ale,'sink-ns_log',httpd_sup,
                    release_handler,kernel_safe_sup,standard_error,ale_sup,
                    overload,error_logger,'sink-disk_json_rpc',alarm_handler,
                    ale_dynamic_sup,'sink-disk_metakv',timer_server,
                    standard_error_sup,'sink-disk_access_int',
                    'sink-disk_access',crypto_server,'sink-disk_reports',
                    crypto_sup,sasl_safe_sup,'sink-disk_stats',tftp_sup,
                    'sink-disk_xdcr',inet_db,init,os_mon_sup,rex,
                    'sink-disk_debug',tls_connection_sup,user,ssl_sup,
                    kernel_sup,cpu_sup,'sink-disk_error',global_name_server,
                    memsup,disksup,'sink-disk_default',httpc_sup,
                    file_server_2,ssl_manager,local_tasks,global_group,
                    httpc_profile_sup,httpc_manager,httpc_handler_sup,ftp_sup,
                    sasl_sup,erl_prim_loader,inets_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,3}]
[ns_server:info,2019-03-13T10:24:42.493Z,nonode@nohost:ns_server_cluster_sup<0.137.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"d200fc0f68695d4aef1fad5c3c8cc55f8c033014\" upstream=\"refs/tags/0.9.7\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"a5567811193b0cc3571fe94e42fc1b8a6a80bc5b\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"10233dcde760b61f4ffac0479bc3a8cabff73beb\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"faa4390d57116ccbdcfc8f00e3affd3044a890cc\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"9e3465560240ffb242b50a47cb7f19251a12ee42\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"055db35bf221ccdc62363f1c4ad88eaac2b892ab\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"f551b6d4f32bb920a83dd28c705bddd5de0d03b2\" />",
 "  <project name=\"blevex\" path=\"godeps/src/github.com/blevesearch/blevex\" remote=\"blevesearch\" revision=\"4b158bb555a3297565afecf6fae675c74f1e47df\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"523d6077a2ec14038605cf8a1feeecaa29c44deb\" upstream=\"alice\">",
 "    <annotation name=\"RELEASE\" value=\"alice\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"1693\" />",
 "    <annotation name=\"VERSION\" value=\"6.0.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbaselabs/cbas\" revision=\"b1f12f65c27f72f582a291ef5a4b72c7a5bf1af0\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"0f3911da8789ba9436962fac63e6928c60f46e6c\" upstream=\"alice\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"78fd5f8ce545e6082271e74cd9f85aa8b8fbbe0d\" upstream=\"alice\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"0df84c7e3c6d95ff435c12a3c08c6f064db11e97\" />",
 "  <project name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"80d2ad8892d806f5103f602fec0d80adaa4b628f\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"a33ad7b7000a9d8d237ba273c47cc100401a0fb0\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"0a94f40b9080e0ecb11d3b7531a58c5e6a4a4465\" upstream=\"master\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbq-gui\" path=\"goproj/src/github.com/couchbase/cbq-gui\" remote=\"couchbase-priv\" revision=\"19fecfe58921c162a31c156781bd2a512711f14d\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"dbfa1c0d73f0e49f6f04e390f03de8f9a6cee769\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"dcae66272b24600ae0005fa06b511cfae8914d3d\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" />",
 "  <project name=\"couchbase-cli\" revision=\"d04a2983f3f014442d2ec1132bb505aa6c025dc3\" upstream=\"alice\" />",
 "  <project name=\"couchdb\" revision=\"45731d9f42d8046f8ba9ccb754657eb2996b5c4a\" upstream=\"alice\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"c545a8563778bfc40284caf9213c7925488e633a\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"3b4c35d79a35756c26ae547e0759b8ef08aa8438\" upstream=\"vulcan\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"f23ba3a5ee43012fcb4b92e1a2a405a92554f4f2\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"05067021a042a1b63e100a486afd7ebddab4c535\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"e84dd5be6c2b899e77bd3fc24a930cc2bcf9188d\" upstream=\"alice\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"kv\" name=\"forestdb\" revision=\"562366039e50730282548b02c1a30d73f97cba27\" upstream=\"vulcan\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"2a0e73f43451045f157640eec59ced72da18471f\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"9b3739952a0900be7628424082559d41dc3cd0d1\" upstream=\"alice\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"3b0453ce6faae42ab4d8cdb9ac1f93919c9d8d69\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" />",
 "  <project name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"47fc4e5e9153645da45af6a86a5bce95e63a0f9e\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d2f15a425a9c8e4d8447e5f5b89ce14845f7fa05\" upstream=\"vulcan\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb\" revision=\"699b13a51af5dd4f80ff3deedf41bba60debad32\" upstream=\"refs/tags/v1.3.7\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"710456e087a6d497e87f41d0a9d98d6a75672186\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"a0d26c2d6f5de912499d35a5aba573006e5e036f\" upstream=\"refs/tags/v7.1.7\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"aecdbe5a5a91f0688df7bdf260ca962178c06828\" upstream=\"vulcan\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"0da75df145308b9a4e6704d762ca9d9b77752efc\" upstream=\"vulcan\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"1e3589e665a728ec9a2c64b516fd26f52ac2663a\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"f98adca8eb365032cab838ef4d99453931afa112\" upstream=\"vulcan\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"a465a37b72784b21dde0290235a9066147fbb12f\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"28aa45915e50214577e4a7810a2a508c1d17934b\" upstream=\"alice\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"d0c17cc8a803812c2d2a304479cc3a0b200c9aba\" upstream=\"alice\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"081e8b16b991bf706eb77f8243935c6fba31b895\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"3681c2a912330352991ecdd642f257efe5b85518\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"956632ec1bc3e28276d00ee2f22c3202f06efb12\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"abd3b58b422dbc2e9463a589d0f3d93441726e23\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project groups=\"kv\" name=\"moxi\" revision=\"cd8da46b9b953800d430c8b0aa4667790727ed6f\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"62685c2d7ca23c807425dca88b11a3e2323dab41\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"f3bef3551997be504612a2d05a8b324b3bfdfe1b\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"43a2cac6976489bb79896f09695f2af2d9b53857\" upstream=\"alice\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"96501c57bb0fd61c85cba6f63101aed2bcf41d38\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"30badd9e911df0e6dd28e4eec2949fe144c0235c\" upstream=\"alice\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"2fbe5179a2673a9275cd0906daa4b1cab38a3eb5\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"55e45187ca8943aa8910e9ae2b59b41242d14386\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"655cdfa588ea190e901bc5590e65d5621688847c\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"df2438af8bb18ba35e599caa1a7afe2eab2c5137\" upstream=\"alice\" />",
 "  <project name=\"query-ui\" revision=\"15a6461f437fe810e411a6613ec7c143991cd1c6\" upstream=\"alice\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"fe09428be4c233d726797a1380f7438f4f71a31a\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"73353fe6dad8f3d67409feefb9b17f90f6de917b\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"4fba14c79f356ae48d6141c561bf9fd7ba33fabd\" upstream=\"refs/tags/v0.14.0\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"c30c3d4c250e68e81c57aa1e8ae91ffd21243cdb\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"9d4e42a20653790449273b3c85e67d6d8bae6e2e\" />",
 "  <project name=\"testrunner\" revision=\"d8f6c71dd26932f304c281645267c31146ef3e1c\" upstream=\"alice\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"601048ad6acbab6cedd582db09b8c4839ff25b15\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"b277d99e18b0ad625405c4cf1ec79af8c94710c7\" upstream=\"alice\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"0ceea4a37442f76199b9259840baf48d17af3c1a\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"5d12c10af88bec4e655ca358aaa3e6d60193a082\" upstream=\"alice\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2019-03-13T10:24:42.577Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.139.0>},
                       {name,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:42.584Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:86]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2019-03-13T10:24:42.585Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:read_address_config_from_path:86]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2019-03-13T10:24:42.586Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:init:163]ip config not found. Looks like we're brand new node
[error_logger:info,2019-03-13T10:24:42.587Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.143.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2019-03-13T10:24:42.588Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.142.0>},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:43.053Z,nonode@nohost:dist_manager<0.141.0>:dist_manager:bringup:215]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2019-03-13T10:24:43.065Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.145.0>},
                       {name,erl_epmd},
                       {mfargs,{erl_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.065Z,nonode@nohost:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.146.0>},
                       {name,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:43.067Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:configure_net_kernel:259]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2019-03-13T10:24:43.067Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.147.0>},
                       {name,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@127.0.0.1',longnames]]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.067Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.144.0>},
                       {name,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@127.0.0.1',longnames]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2019-03-13T10:24:43.071Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:save_node:147]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2019-03-13T10:24:43.079Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:bringup:229]Attempted to save node name to disk: ok
[ns_server:debug,2019-03-13T10:24:43.079Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:wait_for_node:236]Waiting for connection to node 'babysitter_of_ns_1@127.0.0.1' to be established
[error_logger:info,2019-03-13T10:24:43.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:43.104Z,ns_1@127.0.0.1:dist_manager<0.141.0>:dist_manager:wait_for_node:248]Observed node 'babysitter_of_ns_1@127.0.0.1' to come up
[error_logger:info,2019-03-13T10:24:43.104Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.141.0>},
                       {name,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.116Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.152.0>},
                       {name,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.118Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.153.0>},
                       {name,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:43.121Z,ns_1@127.0.0.1:ns_config_sup<0.154.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2019-03-13T10:24:43.121Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.155.0>},
                       {name,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.121Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.156.0>},
                       {name,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:43.211Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1095]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2019-03-13T10:24:43.219Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1109]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2019-03-13T10:24:43.221Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1114]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2019-03-13T10:24:43.233Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1117]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2019-03-13T10:24:43.244Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:load_config:1138]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   false]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@127.0.0.1',ns_log},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@127.0.0.1',port_servers},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}]},
 {{node,'ns_1@127.0.0.1',moxi},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
   {port,11211},
   {verbosity,[]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1539},
 {fts_memory_quota,512},
 {memory_quota,4118},
 {{node,'ns_1@127.0.0.1',memcached_config},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {maxconn,maxconn},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {maxconn,dedicated_port_maxconn},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {maxconn,maxconn},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,<<"required">>},
          {ipv6,<<"optional">>}]}]}},
     {ssl_cipher_list,{"~s",[ssl_cipher_list]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,{memcached_config_mgr,is_enabled,[[5,0]]}},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}}]}]},
 {{node,'ns_1@127.0.0.1',memcached},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
   {port,11210},
   {dedicated_port,11209},
   {ssl_port,undefined},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@127.0.0.1',memcached_defaults},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
   {maxconn,30000},
   {dedicated_port_maxconn,5000},
   {ssl_cipher_list,"HIGH"},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,false},
   {datatype_snappy,true}]},
 {memcached,[]},
 {{node,'ns_1@127.0.0.1',audit},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@127.0.0.1',isasl},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {read_only_user_creds,null},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912}">>},
 {{node,'ns_1@127.0.0.1',eventing_debug_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9140]},
 {{node,'ns_1@127.0.0.1',eventing_https_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',eventing_http_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   8096]},
 {{node,'ns_1@127.0.0.1',cbas_ssl_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',cbas_parent_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9122]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9121]},
 {{node,'ns_1@127.0.0.1',cbas_replication_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9120]},
 {{node,'ns_1@127.0.0.1',cbas_metadata_callback_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9119]},
 {{node,'ns_1@127.0.0.1',cbas_debug_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|-1]},
 {{node,'ns_1@127.0.0.1',cbas_messaging_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9118]},
 {{node,'ns_1@127.0.0.1',cbas_result_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9117]},
 {{node,'ns_1@127.0.0.1',cbas_data_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9116]},
 {{node,'ns_1@127.0.0.1',cbas_cluster_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9115]},
 {{node,'ns_1@127.0.0.1',cbas_console_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9114]},
 {{node,'ns_1@127.0.0.1',cbas_cc_client_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9113]},
 {{node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9112]},
 {{node,'ns_1@127.0.0.1',cbas_cc_http_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9111]},
 {{node,'ns_1@127.0.0.1',cbas_admin_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9110]},
 {{node,'ns_1@127.0.0.1',cbas_http_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   8095]},
 {{node,'ns_1@127.0.0.1',fts_ssl_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',fts_http_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   8094]},
 {{node,'ns_1@127.0.0.1',indexer_stmaint_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9105]},
 {{node,'ns_1@127.0.0.1',indexer_stcatchup_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9104]},
 {{node,'ns_1@127.0.0.1',indexer_stinit_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9103]},
 {{node,'ns_1@127.0.0.1',indexer_https_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',indexer_http_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9102]},
 {{node,'ns_1@127.0.0.1',indexer_scan_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9101]},
 {{node,'ns_1@127.0.0.1',indexer_admin_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9100]},
 {{node,'ns_1@127.0.0.1',xdcr_rest_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9998]},
 {{node,'ns_1@127.0.0.1',projector_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   9999]},
 {{node,'ns_1@127.0.0.1',ssl_query_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',query_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   8093]},
 {{node,'ns_1@127.0.0.1',ssl_capi_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',capi_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   8092]},
 {{node,'ns_1@127.0.0.1',ssl_rest_port},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   undefined]},
 {{node,'ns_1@127.0.0.1',rest},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
   {port,8091},
   {port_meta,global}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {rest,[{port,8091}]},
 {{node,'ns_1@127.0.0.1',membership},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]},
 {quorum_nodes,['ns_1@127.0.0.1']},
 {nodes_wanted,['ns_1@127.0.0.1']},
 {{node,'ns_1@127.0.0.1',compaction_daemon},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,10},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@127.0.0.1',ldap_enabled},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   true]},
 {{node,'ns_1@127.0.0.1',is_enterprise},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   false]},
 {{node,'ns_1@127.0.0.1',config_version},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   {5,5,3}]},
 {{node,'ns_1@127.0.0.1',uuid},
  [{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
   <<"809ca823974d9231aa4400d5407944e4">>]}]
[error_logger:info,2019-03-13T10:24:43.265Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.157.0>},
                       {name,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.269Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.163.0>},
                       {name,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.274Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.164.0>},
                       {name,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.275Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.154.0>},
                       {name,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:43.279Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.166.0>},
                       {name,vbucket_filter_changes_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               [vbucket_filter_changes_registry,
                                [{terminate_command,shutdown}]]}},
                       {restart_type,permanent},
                       {shutdown,100},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.284Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.167.0>},
                       {name,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:43.305Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.170.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:43.308Z,ns_1@127.0.0.1:menelaus_barrier<0.171.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2019-03-13T10:24:43.308Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.171.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.308Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.172.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.319Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.173.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.322Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.175.0>},
                       {name,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.337Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.174.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:43.339Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.178.0>},
                       {name,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.350Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.180.0>},
                       {name,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:43.354Z,ns_1@127.0.0.1:users_replicator<0.180.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-03-13T10:24:43.359Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_storage:anounce_startup:69]Announce my startup to <0.180.0>
[ns_server:debug,2019-03-13T10:24:43.359Z,ns_1@127.0.0.1:users_replicator<0.180.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <0.181.0>
[ns_server:debug,2019-03-13T10:24:43.365Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:open:212]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2019-03-13T10:24:43.365Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.181.0>},
                       {name,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.366Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.179.0>},
                       {name,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:43.398Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:init:44]Starting versioned cache compiled_roles_cache
[error_logger:info,2019-03-13T10:24:43.398Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.183.0>},
                       {name,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.398Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.177.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:43.400Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.186.0>},
                       {name,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:43.401Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.187.0>},
                       {name,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:43.457Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:convert_docs_to_55_in_dets:243]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2019-03-13T10:24:43.457Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,'$1','_','$2'},
                                      [],
                                      [{{'$1','$2'}}]}],
                                    100}
[ns_server:debug,2019-03-13T10:24:43.458Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:init_after_ack:204]Loading 0 items, 299 words took 0ms
[ns_server:debug,2019-03-13T10:24:43.465Z,ns_1@127.0.0.1:users_replicator<0.180.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2019-03-13T10:24:43.469Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.191.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:141]Waiting for ns_couchdb node to start
[error_logger:info,2019-03-13T10:24:43.469Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.190.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:43.469Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:43.471Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:43.471Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.194.0>,shutdown}}
[error_logger:info,2019-03-13T10:24:43.471Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:43.672Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:43.674Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:43.674Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.197.0>,shutdown}}
[error_logger:info,2019-03-13T10:24:43.674Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:43.875Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:43.877Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:43.877Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.200.0>,shutdown}}
[error_logger:info,2019-03-13T10:24:43.877Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:44.078Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:44.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.203.0>,shutdown}}
[error_logger:info,2019-03-13T10:24:44.080Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:44.080Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:44.281Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:44.283Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.206.0>,shutdown}}
[ns_server:debug,2019-03-13T10:24:44.283Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:44.284Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:44.485Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:44.487Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:44.487Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.209.0>,shutdown}}
[error_logger:info,2019-03-13T10:24:44.487Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:44.688Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:44.691Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:44.691Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.212.0>,shutdown}}
[error_logger:info,2019-03-13T10:24:44.691Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:44.893Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:44.895Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2019-03-13T10:24:44.895Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.215.0>,shutdown}}
[error_logger:info,2019-03-13T10:24:44.895Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,875,nodedown,'couchdb_ns_1@127.0.0.1'}}
[error_logger:info,2019-03-13T10:24:45.096Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@127.0.0.1'}}
[ns_server:debug,2019-03-13T10:24:45.146Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:45.347Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:45.548Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:45.751Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:45.952Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:46.158Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:46.360Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:46.561Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:46.763Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:46.965Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:47.167Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:47.373Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[ns_server:debug,2019-03-13T10:24:47.576Z,ns_1@127.0.0.1:<0.192.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:155]ns_couchdb is not ready: false
[error_logger:info,2019-03-13T10:24:48.486Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.233.0>},
                       {name,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:48.687Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: Apache CouchDB v4.5.1-108-g45731d9 (LogLevel=info) is starting.

[ns_server:info,2019-03-13T10:24:49.168Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2019-03-13T10:24:49.264Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.191.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.96617950>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:49.303Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:ns_storage_conf:setup_db_and_ix_paths:47]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2019-03-13T10:24:49.313Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.236.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.319Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.237.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:49.342Z,ns_1@127.0.0.1:ns_server_sup<0.235.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2019-03-13T10:24:49.343Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.238.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.350Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.239.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-03-13T10:24:49.361Z,ns_1@127.0.0.1:ns_log<0.240.0>:ns_log:read_logs:92]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first startup): {error,
                                                                                                 enoent}
[error_logger:info,2019-03-13T10:24:49.361Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.240.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.362Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.241.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:49.379Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-03-13T10:24:49.382Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2019-03-13T10:24:49.469Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: 192: Booted. Waiting for shutdown request

[ns_server:debug,2019-03-13T10:24:49.722Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2019-03-13T10:24:49.722Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.242.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-03-13T10:24:49.736Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:49.736Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-03-13T10:24:49.741Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-03-13T10:24:49.748Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-03-13T10:24:49.754Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2019-03-13T10:24:49.754Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.245.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.754Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.248.0>},
                       {name,ns_log_events},
                       {mfargs,{gen_event,start_link,[{local,ns_log_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-03-13T10:24:49.756Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:49.756Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-03-13T10:24:49.758Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.250.0>},
                       {name,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:49.758Z,ns_1@127.0.0.1:ns_node_disco<0.251.0>:ns_node_disco:init:130]Initting ns_node_disco with []
[ns_server:debug,2019-03-13T10:24:49.759Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[ns_server:debug,2019-03-13T10:24:49.760Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691889}}]}]
[user:info,2019-03-13T10:24:49.760Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_init:83]Initial otp cookie generated: {sanitized,
                                  <<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}
[ns_server:debug,2019-03-13T10:24:49.760Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
otp ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691889}}]},
 {cookie,{sanitized,<<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}}]
[ns_server:debug,2019-03-13T10:24:49.761Z,ns_1@127.0.0.1:<0.252.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}
[ns_server:debug,2019-03-13T10:24:49.797Z,ns_1@127.0.0.1:<0.252.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}
[error_logger:info,2019-03-13T10:24:49.799Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.251.0>},
                       {name,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.807Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.254.0>},
                       {name,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.811Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.255.0>},
                       {name,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:49.818Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:init:69]init pulling
[ns_server:debug,2019-03-13T10:24:49.818Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:init:71]init pushing
[error_logger:info,2019-03-13T10:24:49.818Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.256.0>},
                       {name,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:49.826Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:init:75]init reannouncing
[ns_server:debug,2019-03-13T10:24:49.828Z,ns_1@127.0.0.1:ns_config_events<0.155.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2019-03-13T10:24:49.828Z,ns_1@127.0.0.1:ns_config_events<0.155.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2019-03-13T10:24:49.829Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[ns_server:debug,2019-03-13T10:24:49.829Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              1169890317},
                                                                             false,
                                                                             []}
[ns_server:debug,2019-03-13T10:24:49.830Z,ns_1@127.0.0.1:ns_cookie_manager<0.152.0>:ns_cookie_manager:do_cookie_sync:106]ns_cookie_manager do_cookie_sync
[ns_server:debug,2019-03-13T10:24:49.830Z,ns_1@127.0.0.1:<0.265.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}
[ns_server:debug,2019-03-13T10:24:49.830Z,ns_1@127.0.0.1:<0.265.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}
[ns_server:debug,2019-03-13T10:24:49.831Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
otp ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691889}}]},
 {cookie,{sanitized,<<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}}]
[ns_server:debug,2019-03-13T10:24:49.831Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2019-03-13T10:24:49.831Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2019-03-13T10:24:49.832Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2019-03-13T10:24:49.832Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2019-03-13T10:24:49.832Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:24:49.832Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[],{configs,[]}]
[ns_server:debug,2019-03-13T10:24:49.832Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cbas_memory_quota ->
1539
[ns_server:debug,2019-03-13T10:24:49.833Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2019-03-13T10:24:49.833Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2019-03-13T10:24:49.833Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-03-13T10:24:49.833Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
fts_memory_quota ->
512
[ns_server:debug,2019-03-13T10:24:49.834Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2019-03-13T10:24:49.834Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2019-03-13T10:24:49.834Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
max_bucket_count ->
10
[ns_server:debug,2019-03-13T10:24:49.834Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memcached ->
[]
[ns_server:debug,2019-03-13T10:24:49.834Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memory_quota ->
4118
[ns_server:debug,2019-03-13T10:24:49.834Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
nodes_wanted ->
['ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
quorum_nodes ->
['ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
read_only_user_creds ->
null
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
remote_clusters ->
[]
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest ->
[{port,8091}]
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest_creds ->
null
[ns_server:debug,2019-03-13T10:24:49.835Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
secure_headers ->
[]
[ns_server:debug,2019-03-13T10:24:49.836Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2019-03-13T10:24:49.836Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2019-03-13T10:24:49.836Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2019-03-13T10:24:49.836Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2019-03-13T10:24:49.836Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912}">>
[ns_server:debug,2019-03-13T10:24:49.836Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2019-03-13T10:24:49.837Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2019-03-13T10:24:49.837Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}]
[ns_server:debug,2019-03-13T10:24:49.837Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|8092]
[ns_server:debug,2019-03-13T10:24:49.837Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9110]
[ns_server:debug,2019-03-13T10:24:49.838Z,ns_1@127.0.0.1:<0.266.0>:ns_node_disco:do_nodes_wanted_updated_fun:216]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}
[ns_server:debug,2019-03-13T10:24:49.838Z,ns_1@127.0.0.1:<0.266.0>:ns_node_disco:do_nodes_wanted_updated_fun:222]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"gFsn1Jf3JAC2PlnDl6EMNfJ+5BbPfvAEVbK0NkqbyGc=">>}
[ns_server:debug,2019-03-13T10:24:49.838Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9113]
[ns_server:debug,2019-03-13T10:24:49.838Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9112]
[ns_server:debug,2019-03-13T10:24:49.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9111]
[ns_server:debug,2019-03-13T10:24:49.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9115]
[ns_server:debug,2019-03-13T10:24:49.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9114]
[ns_server:debug,2019-03-13T10:24:49.840Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9116]
[ns_server:debug,2019-03-13T10:24:49.840Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|-1]
[ns_server:debug,2019-03-13T10:24:49.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|8095]
[ns_server:debug,2019-03-13T10:24:49.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9118]
[ns_server:debug,2019-03-13T10:24:49.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9119]
[ns_server:debug,2019-03-13T10:24:49.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9121]
[ns_server:debug,2019-03-13T10:24:49.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9122]
[ns_server:debug,2019-03-13T10:24:49.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9120]
[ns_server:debug,2019-03-13T10:24:49.842Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9117]
[ns_server:debug,2019-03-13T10:24:49.842Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 undefined]
[ns_server:debug,2019-03-13T10:24:49.842Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2019-03-13T10:24:49.843Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 {5,5,3}]
[ns_server:debug,2019-03-13T10:24:49.843Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9140]
[ns_server:debug,2019-03-13T10:24:49.843Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|8096]
[ns_server:debug,2019-03-13T10:24:49.843Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 undefined]
[ns_server:debug,2019-03-13T10:24:49.843Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|8094]
[ns_server:debug,2019-03-13T10:24:49.843Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 undefined]
[ns_server:debug,2019-03-13T10:24:49.844Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9100]
[ns_server:debug,2019-03-13T10:24:49.844Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9102]
[ns_server:debug,2019-03-13T10:24:49.851Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 undefined]
[ns_server:debug,2019-03-13T10:24:49.851Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9101]
[ns_server:debug,2019-03-13T10:24:49.851Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9104]
[ns_server:debug,2019-03-13T10:24:49.852Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9103]
[ns_server:debug,2019-03-13T10:24:49.852Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9105]
[ns_server:debug,2019-03-13T10:24:49.852Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|false]
[ns_server:debug,2019-03-13T10:24:49.852Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2019-03-13T10:24:49.852Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ldap_enabled} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|true]
[ns_server:debug,2019-03-13T10:24:49.853Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',membership} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 active]
[ns_server:debug,2019-03-13T10:24:49.854Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
 {port,11210},
 {dedicated_port,11209},
 {ssl_port,undefined},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2019-03-13T10:24:49.855Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {maxconn,maxconn},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {maxconn,dedicated_port_maxconn},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {maxconn,maxconn},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,<<"required">>},
        {ipv6,<<"optional">>}]}]}},
   {ssl_cipher_list,{"~s",[ssl_cipher_list]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,{memcached_config_mgr,is_enabled,[[5,0]]}},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}}]}]
[ns_server:debug,2019-03-13T10:24:49.856Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
 {maxconn,30000},
 {dedicated_port_maxconn,5000},
 {ssl_cipher_list,"HIGH"},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,false},
 {datatype_snappy,true}]
[ns_server:debug,2019-03-13T10:24:49.856Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',moxi} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
 {port,11211},
 {verbosity,[]}]
[ns_server:debug,2019-03-13T10:24:49.856Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2019-03-13T10:24:49.856Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}]
[ns_server:debug,2019-03-13T10:24:49.857Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9999]
[ns_server:debug,2019-03-13T10:24:49.857Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|8093]
[ns_server:debug,2019-03-13T10:24:49.857Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2019-03-13T10:24:49.857Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 undefined]
[ns_server:debug,2019-03-13T10:24:49.857Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 undefined]
[ns_server:debug,2019-03-13T10:24:49.857Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 undefined]
[ns_server:debug,2019-03-13T10:24:49.857Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|
 <<"809ca823974d9231aa4400d5407944e4">>]
[ns_server:debug,2019-03-13T10:24:49.858Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|9998]
[ns_server:debug,2019-03-13T10:24:49.858Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691883}}]}|false]
[ns_server:debug,2019-03-13T10:24:49.858Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691889}}]}]
[error_logger:info,2019-03-13T10:24:49.875Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.257.0>},
                       {name,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.875Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.249.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:49.875Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,
                               read_only_user_creds,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',indexer_admin_port},
                               {node,'ns_1@127.0.0.1',indexer_http_port},
                               {node,'ns_1@127.0.0.1',indexer_https_port},
                               {node,'ns_1@127.0.0.1',indexer_scan_port},
                               {node,'ns_1@127.0.0.1',indexer_stcatchup_port},
                               {node,'ns_1@127.0.0.1',indexer_stinit_port}]..)
[error_logger:info,2019-03-13T10:24:49.894Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.273.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.899Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.275.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.900Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.278.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.900Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.279.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:49.910Z,ns_1@127.0.0.1:ns_log_events<0.248.0>:ns_mail_log:init:44]ns_mail_log started up
[error_logger:info,2019-03-13T10:24:49.910Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_mail_sup}
             started: [{pid,<0.281.0>},
                       {name,ns_mail_log},
                       {mfargs,{ns_mail_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.910Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.280.0>},
                       {name,ns_mail_sup},
                       {mfargs,{ns_mail_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:49.911Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.282.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.915Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.283.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.928Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.285.0>},
                       {name,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.928Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.287.0>},
                       {name,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.929Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.284.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:49.935Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.294.0>},
                       {name,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:49.940Z,ns_1@127.0.0.1:ns_heart<0.285.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2019-03-13T10:24:49.941Z,ns_1@127.0.0.1:ns_heart<0.285.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:debug,2019-03-13T10:24:49.972Z,ns_1@127.0.0.1:<0.288.0>:restartable:start_child:98]Started child process <0.293.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2019-03-13T10:24:49.972Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.295.0>},
                       {name,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:49.973Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.288.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:49.973Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.298.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.002Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.300.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.003Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.301.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.003Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.302.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.003Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.303.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.029Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.043Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.307.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.043Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.309.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:50.075Z,ns_1@127.0.0.1:ns_couchdb_port<0.190.0>:ns_port_server:log:223]ns_couchdb<0.190.0>: working as port

[error_logger:info,2019-03-13T10:24:50.082Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.312.0>},
                       {name,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.083Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.314.0>},
                       {name,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.112Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.315.0>},
                       {name,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.120Z,ns_1@127.0.0.1:ns_heart<0.285.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2019-03-13T10:24:50.126Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.316.0>},
                       {name,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.131Z,ns_1@127.0.0.1:ns_heart<0.285.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:45]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2019-03-13T10:24:50.137Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.318.0>},
                       {name,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.137Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.319.0>},
                       {name,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:50.146Z,ns_1@127.0.0.1:menelaus_sup<0.310.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts

[ns_server:info,2019-03-13T10:24:50.150Z,ns_1@127.0.0.1:menelaus_sup<0.310.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql

[ns_server:debug,2019-03-13T10:24:50.168Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.287.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,239}]}]}}

[ns_server:debug,2019-03-13T10:24:50.169Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.287.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,116}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2019-03-13T10:24:50.171Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.287.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-03-13T10:24:50.171Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.287.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:45]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2019-03-13T10:24:50.201Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.335.0>},
                       {name,menelaus_web},
                       {mfargs,{menelaus_web,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.217Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.355.0>},
                       {name,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.241Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.356.0>},
                       {name,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.263Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.357.0>},
                       {name,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.286Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.358.0>},
                       {name,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2019-03-13T10:24:50.287Z,ns_1@127.0.0.1:ns_server_sup<0.235.0>:menelaus_sup:start_link:46]Couchbase Server has started on web port 8091 on node 'ns_1@127.0.0.1'. Version: "6.0.0-1693-community".
[error_logger:info,2019-03-13T10:24:50.287Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.310.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.287Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.364.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.295Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.368.0>},
                       {name,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.295Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.369.0>},
                       {name,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.31986353>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.295Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.367.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:50.303Z,ns_1@127.0.0.1:ns_ports_setup<0.364.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2019-03-13T10:24:50.326Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.371.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.326Z,ns_1@127.0.0.1:ns_audit_cfg<0.372.0>:ns_audit_cfg:write_audit_json:265]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json" : [{descriptors_path,
                                                                                <<"/opt/couchbase/etc/security">>},
                                                                               {version,
                                                                                1},
                                                                               {auditd_enabled,
                                                                                false},
                                                                               {disabled,
                                                                                []},
                                                                               {log_path,
                                                                                <<"/opt/couchbase/var/lib/couchbase/logs">>},
                                                                               {rotate_interval,
                                                                                86400},
                                                                               {rotate_size,
                                                                                20971520},
                                                                               {sync,
                                                                                []}]
[ns_server:debug,2019-03-13T10:24:50.336Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2019-03-13T10:24:50.338Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:50.338Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-03-13T10:24:50.352Z,ns_1@127.0.0.1:ns_audit_cfg<0.372.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2019-03-13T10:24:50.353Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.372.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-03-13T10:24:50.357Z,ns_1@127.0.0.1:<0.375.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2019-03-13T10:24:50.370Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.376.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.389Z,ns_1@127.0.0.1:memcached_config_mgr<0.377.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-03-13T10:24:50.389Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.377.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:50.403Z,ns_1@127.0.0.1:<0.378.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2019-03-13T10:24:50.404Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.378.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.407Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.379.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.412Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.381.0>},
                       {name,ns_bucket_worker},
                       {mfargs,{work_queue,start_link,[ns_bucket_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.416Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.383.0>},
                       {name,buckets_observing_subscription},
                       {mfargs,{ns_bucket_sup,subscribe_on_config_events,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.416Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.382.0>},
                       {name,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.417Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.380.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.432Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.384.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.437Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.388.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.445Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.390.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.451Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.391.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.452Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.393.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.458Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.394.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.460Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.396.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.467Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.397.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.481Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.399.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.481Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.401.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.487Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.402.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.492Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.404.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.494Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.404.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-03-13T10:24:50.496Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.404.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2019-03-13T10:24:50.500Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.408.0>},
                       {name,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.503Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.410.0>},
                       {name,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.521Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.411.0>},
                       {name,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.535Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.414.0>},
                       {name,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.417.0>},
                       {name,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.547Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.409.0>},
                       {name,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.548Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.420.0>},
                       {name,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.41280346>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.548Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.407.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.560Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.422.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.578Z,ns_1@127.0.0.1:<0.425.0>:new_concurrency_throttle:init:113]init concurrent throttle process, pid: <0.425.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2019-03-13T10:24:50.582Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2019-03-13T10:24:50.582Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2019-03-13T10:24:50.583Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.423.0>},
                       {name,compaction_new_daemon},
                       {mfargs,{compaction_new_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.584Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2019-03-13T10:24:50.585Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:24:50.588Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1308]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2019-03-13T10:24:50.589Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2019-03-13T10:24:50.591Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.427.0>},
                       {name,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.591Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.426.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.596Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.428.0>},
                       {name,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.616Z,ns_1@127.0.0.1:ns_ports_setup<0.364.0>:ns_ports_setup:set_children:90]Monitor ns_child_ports_sup <12395.72.0>
[ns_server:debug,2019-03-13T10:24:50.616Z,ns_1@127.0.0.1:memcached_config_mgr<0.377.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:info,2019-03-13T10:24:50.636Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.433.0>},
                       {name,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.643Z,ns_1@127.0.0.1:memcached_config_mgr<0.377.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:info,2019-03-13T10:24:50.650Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.436.0>},
                       {name,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.650Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.431.0>},
                       {name,leader_leases_sup},
                       {mfargs,
                           {leader_services_sup,start_link,
                               [leader_leases_sup]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.651Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.438.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.661Z,ns_1@127.0.0.1:memcached_config_mgr<0.377.0>:memcached_config_mgr:init:79]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[error_logger:info,2019-03-13T10:24:50.664Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.439.0>},
                       {name,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.665Z,ns_1@127.0.0.1:memcached_config_mgr<0.377.0>:memcached_config_mgr:init:82]activated memcached port server
[ns_server:debug,2019-03-13T10:24:50.684Z,ns_1@127.0.0.1:leader_registry_sup<0.437.0>:mb_master:check_master_takeover_needed:133]Sending master node question to the following nodes: []
[ns_server:debug,2019-03-13T10:24:50.684Z,ns_1@127.0.0.1:leader_registry_sup<0.437.0>:mb_master:check_master_takeover_needed:135]Got replies: []
[ns_server:debug,2019-03-13T10:24:50.684Z,ns_1@127.0.0.1:leader_registry_sup<0.437.0>:mb_master:check_master_takeover_needed:141]Was unable to discover master, not going to force mastership takeover
[user:info,2019-03-13T10:24:50.684Z,ns_1@127.0.0.1:mb_master<0.442.0>:mb_master:init:86]I'm the only node, so I'm the master.
[ns_server:debug,2019-03-13T10:24:50.685Z,ns_1@127.0.0.1:leader_registry<0.439.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[ns_server:debug,2019-03-13T10:24:50.710Z,ns_1@127.0.0.1:leader_lease_acquirer<0.445.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2019-03-13T10:24:50.710Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.445.0>},
                       {name,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.715Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.447.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2019-03-13T10:24:50.715Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.447.0>},
                       {name,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2019-03-13T10:24:50.739Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:50.739Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:info,2019-03-13T10:24:50.747Z,ns_1@127.0.0.1:mb_master_sup<0.444.0>:misc:start_singleton:756]start_singleton(gen_server, ns_tick, [], []): started as <0.449.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-03-13T10:24:50.747Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.449.0>},
                       {name,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[ns_server:warn,2019-03-13T10:24:50.758Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:50.758Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2019-03-13T10:24:50.783Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.452.0>},
                       {name,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:50.791Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.451.0>:misc:start_singleton:756]start_singleton(gen_server, auto_reprovision, [], []): started as <0.453.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-03-13T10:24:50.791Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.453.0>},
                       {name,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.795Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,0]}]

[ns_server:info,2019-03-13T10:24:50.796Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [4,1]
[ns_server:debug,2019-03-13T10:24:50.796Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,1]},
 {set,{service_map,n1ql},[]},
 {set,{service_map,index},[]}]

[ns_server:info,2019-03-13T10:24:50.797Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [4,5]
[ns_server:debug,2019-03-13T10:24:50.797Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,5]},
 {set,{service_map,fts},[]},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":5,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>}]

[ns_server:info,2019-03-13T10:24:50.798Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [4,6]
[ns_server:debug,2019-03-13T10:24:50.798Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[4,6]}]

[ns_server:info,2019-03-13T10:24:50.798Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,0]
[ns_server:debug,2019-03-13T10:24:50.798Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]},
 {delete,roles_definitions},
 {delete,users_upgrade},
 {delete,read_only_user_creds},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2019-03-13T10:24:50.798Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2019-03-13T10:24:50.799Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2019-03-13T10:24:50.815Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2019-03-13T10:24:50.816Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@127.0.0.1']},
 {set,scramsha_fallback_salt,<<158,7,179,16,73,179,77,129,150,58,218,116>>}]

[ns_server:info,2019-03-13T10:24:50.816Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2019-03-13T10:24:50.824Z,ns_1@127.0.0.1:ns_config<0.157.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]},
 {set,audit_decriptors,
      [{20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]}]}]

[ns_server:debug,2019-03-13T10:24:50.828Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,quorum_nodes,
                               read_only_user_creds,roles_definitions,
                               scramsha_fallback_salt,users_upgrade,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {rbac_upgrade,[5,5]},
                               {service_map,fts},
                               {service_map,index},
                               {service_map,n1ql}]..)
[ns_server:debug,2019-03-13T10:24:50.830Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.447.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2019-03-13T10:24:50.830Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,1169890317},
                                                                false,[]} to {[6,
                                                                               0],
                                                                              {0,
                                                                               1169890317},
                                                                              false,
                                                                              []}
[ns_server:debug,2019-03-13T10:24:50.831Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.447.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2019-03-13T10:24:50.831Z,ns_1@127.0.0.1:leader_lease_acquirer<0.445.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2019-03-13T10:24:50.831Z,ns_1@127.0.0.1:menelaus_ui_auth<0.312.0>:token_server:handle_cast:202]Purge tokens []
[ns_server:warn,2019-03-13T10:24:50.832Z,ns_1@127.0.0.1:<0.458.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:50.832Z,ns_1@127.0.0.1:<0.435.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.377.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:50.833Z,ns_1@127.0.0.1:<0.434.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.377.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:50.834Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.447.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2019-03-13T10:24:50.834Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:handle_call:116]Got full synchronization request from 'ns_1@127.0.0.1'
[ns_server:debug,2019-03-13T10:24:50.835Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:handle_call:122]Fully synchronized config in 374 us
[user:warn,2019-03-13T10:24:50.836Z,ns_1@127.0.0.1:<0.454.0>:ns_orchestrator:consider_switching_compat_mode_dont_exit:925]Changed cluster compat mode from undefined to [6,0]
[ns_server:info,2019-03-13T10:24:50.837Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.451.0>:misc:start_singleton:756]start_singleton(gen_fsm, ns_orchestrator, [], []): started as <0.454.0> on 'ns_1@127.0.0.1'

[ns_server:debug,2019-03-13T10:24:50.838Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit_decriptors ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]}]
[ns_server:debug,2019-03-13T10:24:50.838Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 <<158,7,179,16,73,179,77,129,150,58,218,116>>]
[ns_server:debug,2019-03-13T10:24:50.838Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{rbac_upgrade,[5,5]} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:24:50.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2019-03-13T10:24:50.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>]
[ns_server:debug,2019-03-13T10:24:50.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
client_cert_auth ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2019-03-13T10:24:50.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
users_upgrade ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:24:50.839Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
roles_definitions ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:24:50.840Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,fts} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}]
[ns_server:debug,2019-03-13T10:24:50.840Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,index} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}]
[ns_server:debug,2019-03-13T10:24:50.840Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,n1ql} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}]
[ns_server:debug,2019-03-13T10:24:50.840Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
cluster_compat_version ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{8,63719691890}}]},6,0]
[ns_server:debug,2019-03-13T10:24:50.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
audit ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2019-03-13T10:24:50.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]}]
[ns_server:debug,2019-03-13T10:24:50.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"809ca823974d9231aa4400d5407944e4">>,{3,63719691890}}],{configs,[]}]
[ns_server:debug,2019-03-13T10:24:50.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
quorum_nodes ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:24:50.841Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
read_only_user_creds ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:24:50.842Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2019-03-13T10:24:50.842Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691890}}]}]
[ns_server:debug,2019-03-13T10:24:50.860Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-03-13T10:24:50.865Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.245.0>
[ns_server:debug,2019-03-13T10:24:50.865Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:24:50.868Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.245.0>
[ns_server:debug,2019-03-13T10:24:50.880Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:warn,2019-03-13T10:24:50.886Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:50.887Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:error,2019-03-13T10:24:50.915Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.457.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-03-13T10:24:50.915Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.377.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:50.916Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.458.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-03-13T10:24:50.916Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.377.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: [do_check,do_check]
    links: [<0.235.0>,<0.435.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 23434
  neighbours:

[error_logger:info,2019-03-13T10:24:50.917Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.454.0>},
                       {name,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.917Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.451.0>},
                       {name,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:50.930Z,ns_1@127.0.0.1:<0.481.0>:auto_failover:init:211]init auto_failover.
[user:info,2019-03-13T10:24:50.931Z,ns_1@127.0.0.1:<0.481.0>:auto_failover:handle_call:242]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2019-03-13T10:24:50.945Z,ns_1@127.0.0.1:leader_lease_agent<0.436.0>:leader_lease_agent:do_handle_acquire_lease:147]Granting lease to {lease_holder,<<"4b42a99ee292374e4b7dd44ee5d8e5d5">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:debug,2019-03-13T10:24:50.957Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{3,63719691890}}]}]
[ns_server:debug,2019-03-13T10:24:50.957Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:50.957Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691890}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]}]
[ns_server:info,2019-03-13T10:24:50.957Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.450.0>:misc:start_singleton:756]start_singleton(gen_server, auto_failover, [], []): started as <0.481.0> on 'ns_1@127.0.0.1'

[error_logger:info,2019-03-13T10:24:50.958Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.481.0>},
                       {name,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:50.958Z,ns_1@127.0.0.1:<0.429.0>:restartable:start_child:98]Started child process <0.430.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2019-03-13T10:24:50.959Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.450.0>},
                       {name,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.960Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.442.0>},
                       {name,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.961Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.437.0>},
                       {name,leader_registry_sup},
                       {mfargs,
                           {leader_services_sup,start_link,
                               [leader_registry_sup]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.962Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.429.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:50.962Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.491.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.963Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.492.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:50.974Z,ns_1@127.0.0.1:<0.480.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"4b42a99ee292374e4b7dd44ee5d8e5d5">>)
[error_logger:info,2019-03-13T10:24:50.975Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.493.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:50.995Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.494.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:51.023Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.497.0>},
                       {name,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:51.024Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.499.0>},
                       {name,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:51.024Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.500.0>},
                       {name,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.81875396>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:51.042Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.508.0>},
                       {name,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[ns_server:debug,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:memcached_config_mgr<0.519.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:info,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.517.0>},
                       {name,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:menelaus_barrier<0.171.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.169.0>
[ns_server:debug,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:memcached_config_mgr<0.519.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:ns_server_nodes_sup<0.169.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[error_logger:info,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.496.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:<0.168.0>:restartable:start_child:98]Started child process <0.169.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:error,2019-03-13T10:24:51.051Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.377.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[ns_server:debug,2019-03-13T10:24:51.052Z,ns_1@127.0.0.1:<0.2.0>:child_erlang:child_loop:130]151: Entered child_loop
[error_logger:error,2019-03-13T10:24:51.052Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.377.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.052Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.235.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:51.052Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.519.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:51.052Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.168.0>},
                       {name,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2019-03-13T10:24:51.053Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@127.0.0.1'

[ns_server:debug,2019-03-13T10:24:51.054Z,ns_1@127.0.0.1:memcached_config_mgr<0.519.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-03-13T10:24:51.058Z,ns_1@127.0.0.1:memcached_config_mgr<0.519.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.059Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:debug,2019-03-13T10:24:51.061Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2019-03-13T10:24:51.071Z,ns_1@127.0.0.1:memcached_config_mgr<0.519.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.074Z,ns_1@127.0.0.1:<0.523.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.075Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.523.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.075Z,ns_1@127.0.0.1:<0.520.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.519.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.075Z,ns_1@127.0.0.1:<0.521.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.519.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.075Z,ns_1@127.0.0.1:memcached_config_mgr<0.524.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.075Z,ns_1@127.0.0.1:memcached_config_mgr<0.524.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.076Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.522.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.076Z,ns_1@127.0.0.1:memcached_config_mgr<0.524.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.076Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.519.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.078Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.519.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.521.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 16401
  neighbours:

[error_logger:error,2019-03-13T10:24:51.078Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.519.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.519.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.079Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.524.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.080Z,ns_1@127.0.0.1:memcached_config_mgr<0.524.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.084Z,ns_1@127.0.0.1:memcached_config_mgr<0.524.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.087Z,ns_1@127.0.0.1:<0.528.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.087Z,ns_1@127.0.0.1:<0.526.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.524.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.087Z,ns_1@127.0.0.1:<0.525.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.524.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.088Z,ns_1@127.0.0.1:memcached_config_mgr<0.529.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.088Z,ns_1@127.0.0.1:memcached_config_mgr<0.529.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.088Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.527.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[error_logger:error,2019-03-13T10:24:51.088Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.528.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.089Z,ns_1@127.0.0.1:memcached_config_mgr<0.529.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.089Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.524.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.091Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.524.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.526.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 16296
  neighbours:

[error_logger:error,2019-03-13T10:24:51.091Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.524.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.091Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.524.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.092Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.529.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.098Z,ns_1@127.0.0.1:memcached_config_mgr<0.529.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.102Z,ns_1@127.0.0.1:memcached_config_mgr<0.529.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.106Z,ns_1@127.0.0.1:<0.533.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.107Z,ns_1@127.0.0.1:memcached_config_mgr<0.534.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.107Z,ns_1@127.0.0.1:<0.531.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.529.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.107Z,ns_1@127.0.0.1:<0.530.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.529.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.107Z,ns_1@127.0.0.1:memcached_config_mgr<0.534.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-03-13T10:24:51.109Z,ns_1@127.0.0.1:memcached_config_mgr<0.534.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.109Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.532.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[error_logger:error,2019-03-13T10:24:51.109Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.529.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.110Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.529.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.531.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16390
  neighbours:

[error_logger:error,2019-03-13T10:24:51.110Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.529.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.111Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.529.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.111Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.534.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:error,2019-03-13T10:24:51.111Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.533.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.115Z,ns_1@127.0.0.1:memcached_config_mgr<0.534.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.121Z,ns_1@127.0.0.1:memcached_config_mgr<0.534.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.130Z,ns_1@127.0.0.1:<0.538.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.130Z,ns_1@127.0.0.1:<0.536.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.534.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.130Z,ns_1@127.0.0.1:<0.535.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.534.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[error_logger:error,2019-03-13T10:24:51.131Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.537.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.131Z,ns_1@127.0.0.1:memcached_config_mgr<0.540.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.131Z,ns_1@127.0.0.1:memcached_config_mgr<0.540.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.131Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.534.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.132Z,ns_1@127.0.0.1:memcached_config_mgr<0.540.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.133Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.534.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.536.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16490
  neighbours:

[error_logger:error,2019-03-13T10:24:51.133Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.534.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.133Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.534.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2019-03-13T10:24:51.133Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.538.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:info,2019-03-13T10:24:51.134Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.540.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.138Z,ns_1@127.0.0.1:memcached_config_mgr<0.540.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.144Z,ns_1@127.0.0.1:memcached_config_mgr<0.540.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:debug,2019-03-13T10:24:51.147Z,ns_1@127.0.0.1:json_rpc_connection-saslauthd-saslauthd-port<0.546.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.546.0>
[ns_server:warn,2019-03-13T10:24:51.149Z,ns_1@127.0.0.1:<0.545.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.149Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.545.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.149Z,ns_1@127.0.0.1:<0.543.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.540.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.150Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.150Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.150Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.544.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.149Z,ns_1@127.0.0.1:<0.541.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.540.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[error_logger:error,2019-03-13T10:24:51.150Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.540.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.151Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.540.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.543.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16357
  neighbours:

[error_logger:error,2019-03-13T10:24:51.152Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.540.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.152Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.540.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.152Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.548.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.152Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-03-13T10:24:51.160Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691891}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2019-03-13T10:24:51.160Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.160Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
memory_quota ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691891}}]}|512]
[ns_server:debug,2019-03-13T10:24:51.160Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{4,63719691891}}]}]
[ns_server:debug,2019-03-13T10:24:51.160Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([memory_quota,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2019-03-13T10:24:51.160Z,ns_1@127.0.0.1:ns_audit<0.376.0>:ns_audit:handle_call:110]Audit cluster_settings: [{cluster_name,<<>>},
                         {quotas,{[{kv,512},
                                   {index,256},
                                   {fts,512},
                                   {cbas,1539},
                                   {eventing,256}]}},
                         {real_userid,{[{domain,builtin},
                                        {user,<<"<ud>admin</ud>">>}]}},
                         {remote,{[{ip,<<"172.20.0.2">>},{port,50776}]}},
                         {timestamp,<<"2019-03-13T10:24:51.160Z">>}]
[ns_server:warn,2019-03-13T10:24:51.162Z,ns_1@127.0.0.1:<0.554.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-03-13T10:24:51.169Z,ns_1@127.0.0.1:memcached_config_mgr<0.548.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.181Z,ns_1@127.0.0.1:<0.564.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.182Z,ns_1@127.0.0.1:<0.549.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.548.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[error_logger:error,2019-03-13T10:24:51.182Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.563.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.183Z,ns_1@127.0.0.1:memcached_config_mgr<0.565.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-03-13T10:24:51.183Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.548.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.183Z,ns_1@127.0.0.1:<0.561.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.548.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-03-13T10:24:51.184Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.548.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.561.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16469
  neighbours:

[error_logger:error,2019-03-13T10:24:51.185Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.564.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-03-13T10:24:51.185Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.548.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.185Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.548.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.186Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.565.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.191Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{3,63719691891}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\""...>>]
[ns_server:debug,2019-03-13T10:24:51.192Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2019-03-13T10:24:51.192Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{5,63719691891}}]}]
[ns_server:debug,2019-03-13T10:24:51.211Z,ns_1@127.0.0.1:memcached_config_mgr<0.565.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-03-13T10:24:51.214Z,ns_1@127.0.0.1:memcached_config_mgr<0.565.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-03-13T10:24:51.218Z,ns_1@127.0.0.1:memcached_config_mgr<0.565.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.233Z,ns_1@127.0.0.1:memcached_config_mgr<0.565.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.236Z,ns_1@127.0.0.1:<0.576.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.237Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.576.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.237Z,ns_1@127.0.0.1:<0.574.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.565.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.237Z,ns_1@127.0.0.1:<0.573.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.565.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.237Z,ns_1@127.0.0.1:memcached_config_mgr<0.577.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.238Z,ns_1@127.0.0.1:memcached_config_mgr<0.577.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.238Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.575.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.239Z,ns_1@127.0.0.1:memcached_config_mgr<0.577.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.238Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.565.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.241Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.565.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.574.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16407
  neighbours:

[error_logger:error,2019-03-13T10:24:51.241Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.565.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.241Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.565.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.241Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.577.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.245Z,ns_1@127.0.0.1:memcached_config_mgr<0.577.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.249Z,ns_1@127.0.0.1:memcached_config_mgr<0.577.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.254Z,ns_1@127.0.0.1:<0.581.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.255Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.581.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.255Z,ns_1@127.0.0.1:<0.578.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.577.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[error_logger:error,2019-03-13T10:24:51.255Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.580.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.256Z,ns_1@127.0.0.1:<0.579.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.577.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.256Z,ns_1@127.0.0.1:memcached_config_mgr<0.582.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-03-13T10:24:51.256Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.577.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.256Z,ns_1@127.0.0.1:memcached_config_mgr<0.582.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.256Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.577.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[ns_server:debug,2019-03-13T10:24:51.258Z,ns_1@127.0.0.1:memcached_config_mgr<0.582.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.258Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.577.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.579.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16446
  neighbours:

[error_logger:error,2019-03-13T10:24:51.258Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.577.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.258Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.582.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.260Z,ns_1@127.0.0.1:memcached_config_mgr<0.582.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.263Z,ns_1@127.0.0.1:memcached_config_mgr<0.582.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.265Z,ns_1@127.0.0.1:<0.586.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.265Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.586.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.265Z,ns_1@127.0.0.1:<0.584.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.582.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.266Z,ns_1@127.0.0.1:memcached_config_mgr<0.587.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.265Z,ns_1@127.0.0.1:<0.583.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.582.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.266Z,ns_1@127.0.0.1:memcached_config_mgr<0.587.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-03-13T10:24:51.267Z,ns_1@127.0.0.1:memcached_config_mgr<0.587.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.267Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.585.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[error_logger:error,2019-03-13T10:24:51.267Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.582.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.268Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.582.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.584.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16474
  neighbours:

[error_logger:error,2019-03-13T10:24:51.268Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.582.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.269Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.582.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.270Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.587.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.271Z,ns_1@127.0.0.1:memcached_config_mgr<0.587.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.276Z,ns_1@127.0.0.1:memcached_config_mgr<0.587.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.278Z,ns_1@127.0.0.1:<0.591.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.278Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.591.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-03-13T10:24:51.279Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.590.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.279Z,ns_1@127.0.0.1:<0.588.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.587.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.279Z,ns_1@127.0.0.1:<0.589.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.587.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-03-13T10:24:51.280Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.587.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.280Z,ns_1@127.0.0.1:memcached_config_mgr<0.592.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.280Z,ns_1@127.0.0.1:memcached_config_mgr<0.592.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.281Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.587.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.589.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16403
  neighbours:

[error_logger:error,2019-03-13T10:24:51.281Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.587.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.281Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.587.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.282Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.592.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.283Z,ns_1@127.0.0.1:memcached_config_mgr<0.592.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-03-13T10:24:51.302Z,ns_1@127.0.0.1:memcached_config_mgr<0.592.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.308Z,ns_1@127.0.0.1:memcached_config_mgr<0.592.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.311Z,ns_1@127.0.0.1:<0.596.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.312Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.596.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.312Z,ns_1@127.0.0.1:<0.593.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.592.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.312Z,ns_1@127.0.0.1:<0.594.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.592.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.313Z,ns_1@127.0.0.1:memcached_config_mgr<0.597.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-03-13T10:24:51.313Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.595.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.313Z,ns_1@127.0.0.1:memcached_config_mgr<0.597.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.313Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.592.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.314Z,ns_1@127.0.0.1:memcached_config_mgr<0.597.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.314Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.592.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.594.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16475
  neighbours:

[error_logger:error,2019-03-13T10:24:51.315Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.592.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.315Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.592.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.315Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.597.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.316Z,ns_1@127.0.0.1:memcached_config_mgr<0.597.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.320Z,ns_1@127.0.0.1:memcached_config_mgr<0.597.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.322Z,ns_1@127.0.0.1:<0.601.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.322Z,ns_1@127.0.0.1:<0.598.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.597.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.322Z,ns_1@127.0.0.1:<0.599.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.597.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.323Z,ns_1@127.0.0.1:memcached_config_mgr<0.602.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.323Z,ns_1@127.0.0.1:memcached_config_mgr<0.602.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.323Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.600.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.324Z,ns_1@127.0.0.1:memcached_config_mgr<0.602.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.324Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.597.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.324Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.601.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-03-13T10:24:51.325Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.597.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.599.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 16435
  neighbours:

[error_logger:error,2019-03-13T10:24:51.325Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.597.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.325Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.597.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.326Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.602.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.328Z,ns_1@127.0.0.1:memcached_config_mgr<0.602.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.332Z,ns_1@127.0.0.1:memcached_config_mgr<0.602.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.334Z,ns_1@127.0.0.1:<0.606.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.335Z,ns_1@127.0.0.1:<0.603.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.602.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.335Z,ns_1@127.0.0.1:<0.604.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.602.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.336Z,ns_1@127.0.0.1:memcached_config_mgr<0.607.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-03-13T10:24:51.336Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.605.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.336Z,ns_1@127.0.0.1:memcached_config_mgr<0.607.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.336Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.606.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-03-13T10:24:51.336Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.602.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.336Z,ns_1@127.0.0.1:memcached_config_mgr<0.607.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.337Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.602.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.604.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 987
    stack_size: 27
    reductions: 16439
  neighbours:

[error_logger:error,2019-03-13T10:24:51.337Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.602.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.338Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.602.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.338Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.607.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.340Z,ns_1@127.0.0.1:memcached_config_mgr<0.607.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:warn,2019-03-13T10:24:51.341Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.341Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-03-13T10:24:51.347Z,ns_1@127.0.0.1:memcached_config_mgr<0.607.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.349Z,ns_1@127.0.0.1:<0.611.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.349Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.611.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.350Z,ns_1@127.0.0.1:<0.608.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.607.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.350Z,ns_1@127.0.0.1:<0.609.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.607.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.350Z,ns_1@127.0.0.1:memcached_config_mgr<0.612.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[error_logger:error,2019-03-13T10:24:51.350Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.610.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.350Z,ns_1@127.0.0.1:memcached_config_mgr<0.612.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-03-13T10:24:51.351Z,ns_1@127.0.0.1:memcached_config_mgr<0.612.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.350Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.607.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.354Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.607.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.609.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 16431
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.354Z,ns_1@127.0.0.1:memcached_config_mgr<0.612.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[error_logger:error,2019-03-13T10:24:51.354Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.607.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.355Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.607.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.356Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.612.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.361Z,ns_1@127.0.0.1:memcached_config_mgr<0.612.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.363Z,ns_1@127.0.0.1:<0.616.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.363Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.616.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:warn,2019-03-13T10:24:51.363Z,ns_1@127.0.0.1:<0.375.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2019-03-13T10:24:51.364Z,ns_1@127.0.0.1:<0.614.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.612.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.364Z,ns_1@127.0.0.1:<0.613.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.612.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.364Z,ns_1@127.0.0.1:memcached_config_mgr<0.617.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.364Z,ns_1@127.0.0.1:memcached_config_mgr<0.617.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.364Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.615.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.365Z,ns_1@127.0.0.1:memcached_config_mgr<0.617.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.365Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.612.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.366Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.612.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.614.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16562
  neighbours:

[error_logger:error,2019-03-13T10:24:51.366Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.612.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.366Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.612.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.367Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.617.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.369Z,ns_1@127.0.0.1:memcached_config_mgr<0.617.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.374Z,ns_1@127.0.0.1:memcached_config_mgr<0.617.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.376Z,ns_1@127.0.0.1:<0.621.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.376Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.621.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.376Z,ns_1@127.0.0.1:<0.619.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.617.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.377Z,ns_1@127.0.0.1:memcached_config_mgr<0.622.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.377Z,ns_1@127.0.0.1:memcached_config_mgr<0.622.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.378Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.620.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.379Z,ns_1@127.0.0.1:memcached_config_mgr<0.622.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.378Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.617.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.380Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.617.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.619.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 2586
    stack_size: 27
    reductions: 16471
  neighbours:

[error_logger:error,2019-03-13T10:24:51.380Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.617.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.380Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.617.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-03-13T10:24:51.381Z,ns_1@127.0.0.1:memcached_config_mgr<0.622.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[error_logger:info,2019-03-13T10:24:51.381Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.622.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.382Z,ns_1@127.0.0.1:<0.618.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.617.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.392Z,ns_1@127.0.0.1:memcached_config_mgr<0.622.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.395Z,ns_1@127.0.0.1:<0.626.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.395Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.626.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.396Z,ns_1@127.0.0.1:<0.623.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.622.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.396Z,ns_1@127.0.0.1:<0.624.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.622.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-03-13T10:24:51.397Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.625.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 138
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.397Z,ns_1@127.0.0.1:memcached_config_mgr<0.627.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.397Z,ns_1@127.0.0.1:memcached_config_mgr<0.627.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.397Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.622.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.398Z,ns_1@127.0.0.1:memcached_config_mgr<0.627.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.398Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.622.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.624.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 16423
  neighbours:

[error_logger:error,2019-03-13T10:24:51.398Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.622.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.398Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.622.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.399Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.627.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.401Z,ns_1@127.0.0.1:memcached_config_mgr<0.627.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.408Z,ns_1@127.0.0.1:memcached_config_mgr<0.627.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.410Z,ns_1@127.0.0.1:<0.631.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.410Z,ns_1@127.0.0.1:<0.629.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.627.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.411Z,ns_1@127.0.0.1:<0.628.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.627.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[error_logger:error,2019-03-13T10:24:51.411Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.630.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 143
  neighbours:

[ns_server:debug,2019-03-13T10:24:51.411Z,ns_1@127.0.0.1:memcached_config_mgr<0.632.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.411Z,ns_1@127.0.0.1:memcached_config_mgr<0.632.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:error,2019-03-13T10:24:51.411Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.627.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.413Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.627.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.629.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16558
  neighbours:

[error_logger:error,2019-03-13T10:24:51.413Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.627.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[ns_server:debug,2019-03-13T10:24:51.413Z,ns_1@127.0.0.1:memcached_config_mgr<0.632.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:error,2019-03-13T10:24:51.413Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.627.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:error,2019-03-13T10:24:51.413Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.631.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:info,2019-03-13T10:24:51.413Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.632.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.422Z,ns_1@127.0.0.1:memcached_config_mgr<0.632.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.428Z,ns_1@127.0.0.1:memcached_config_mgr<0.632.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.430Z,ns_1@127.0.0.1:<0.636.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.430Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.636.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[error_logger:error,2019-03-13T10:24:51.431Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.635.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-03-13T10:24:51.432Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.632.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[ns_server:debug,2019-03-13T10:24:51.433Z,ns_1@127.0.0.1:<0.634.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.632.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[ns_server:debug,2019-03-13T10:24:51.433Z,ns_1@127.0.0.1:memcached_config_mgr<0.637.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:51.433Z,ns_1@127.0.0.1:<0.633.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.632.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[error_logger:error,2019-03-13T10:24:51.434Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.632.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.634.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16580
  neighbours:

[error_logger:error,2019-03-13T10:24:51.434Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.632.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.435Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.632.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[error_logger:info,2019-03-13T10:24:51.435Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.637.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:51.436Z,ns_1@127.0.0.1:memcached_config_mgr<0.637.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[ns_server:debug,2019-03-13T10:24:51.436Z,ns_1@127.0.0.1:memcached_config_mgr<0.637.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[ns_server:debug,2019-03-13T10:24:51.441Z,ns_1@127.0.0.1:memcached_config_mgr<0.637.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:51.448Z,ns_1@127.0.0.1:memcached_config_mgr<0.637.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:warn,2019-03-13T10:24:51.451Z,ns_1@127.0.0.1:<0.641.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2019-03-13T10:24:51.452Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Error in process <0.641.0> on node 'ns_1@127.0.0.1' with exit value: {{badmatch,{error,couldnt_connect_to_memcached}},[{ns_memcached,'-config_validate/1-fun-0-',1,[{file,"src/ns_memcached.erl"},{line,1474}]},{async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}


[ns_server:debug,2019-03-13T10:24:51.452Z,ns_1@127.0.0.1:<0.638.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.637.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1474}]},
                                               {async,'-async_init/4-fun-2-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,208}]}]}. Exiting
[ns_server:debug,2019-03-13T10:24:51.452Z,ns_1@127.0.0.1:<0.639.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {ns_config_events,<0.637.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1474}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-2-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    208}]}]}
[error_logger:error,2019-03-13T10:24:51.453Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: async:-start/2-fun-0-/0
    pid: <0.640.0>
    registered_name: []
    exception exit: {raised,
                        {error,
                            {badmatch,{error,couldnt_connect_to_memcached}},
                            [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                             {async,'-async_init/4-fun-2-',3,
                                 [{file,"src/async.erl"},{line,208}]}]}}
      in function  async:handle_get_result/2 (src/async.erl, line 339)
    ancestors: [memcached_config_mgr,ns_server_sup,ns_server_nodes_sup,
                  <0.168.0>,ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: []
    dictionary: [{'$async_role',controller}]
    trap_exit: true
    status: running
    heap_size: 610
    stack_size: 27
    reductions: 149
  neighbours:

[error_logger:error,2019-03-13T10:24:51.453Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]** Generic server <0.637.0> terminating 
** Last message in was do_check
** When Server state == {state,<12395.79.0>,
                               <<"{\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"maxconn\": 30000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"maxconn\": 5000,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": \"HIGH\",\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"tracing_enabled\": false,\n  \"verbosity\": 0,\n  \"xattr_enabled\": false\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1474}]},
     {async,'-async_init/4-fun-2-',3,[{file,"src/async.erl"},{line,208}]}]}

[error_logger:error,2019-03-13T10:24:51.454Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.637.0>
    registered_name: []
    exception exit: {{badmatch,{error,couldnt_connect_to_memcached}},
                     [{ns_memcached,'-config_validate/1-fun-0-',1,
                                    [{file,"src/ns_memcached.erl"},
                                     {line,1474}]},
                      {async,'-async_init/4-fun-2-',3,
                             [{file,"src/async.erl"},{line,208}]}]}
      in function  gen_server:init_it/6 (gen_server.erl, line 328)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.168.0>,
                  ns_server_cluster_sup,<0.89.0>]
    messages: []
    links: [<0.235.0>,<0.639.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 27
    reductions: 16403
  neighbours:

[error_logger:error,2019-03-13T10:24:51.454Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]Supervisor received unexpected message: {ack,<0.637.0>,
                                         {error,
                                          {{badmatch,
                                            {error,
                                             couldnt_connect_to_memcached}},
                                           [{ns_memcached,
                                             '-config_validate/1-fun-0-',1,
                                             [{file,"src/ns_memcached.erl"},
                                              {line,1474}]},
                                            {async,'-async_init/4-fun-2-',3,
                                             [{file,"src/async.erl"},
                                              {line,208}]}]}}}

[error_logger:error,2019-03-13T10:24:51.455Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1474}]},
                   {async,'-async_init/4-fun-2-',3,
                          [{file,"src/async.erl"},{line,208}]}]}
     Offender:   [{pid,<0.637.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2019-03-13T10:24:51.614Z,ns_1@127.0.0.1:json_rpc_connection-goxdcr-cbauth<0.643.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.643.0>
[ns_server:debug,2019-03-13T10:24:51.614Z,ns_1@127.0.0.1:menelaus_cbauth<0.358.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"goxdcr-cbauth",<0.643.0>} started
[ns_server:debug,2019-03-13T10:24:51.635Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:warn,2019-03-13T10:24:51.743Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.743Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-03-13T10:24:51.752Z,ns_1@127.0.0.1:<0.650.0>:ns_memcached:connect:1230]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:warn,2019-03-13T10:24:51.774Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.774Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2019-03-13T10:24:51.892Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:ns_memcached:connect:1227]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2019-03-13T10:24:51.893Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2019-03-13T10:24:51.933Z,ns_1@127.0.0.1:<0.481.0>:auto_failover_logic:log_master_activity:170]Transitioned node {'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>} state new -> up
[ns_server:debug,2019-03-13T10:24:52.181Z,ns_1@127.0.0.1:ns_audit<0.376.0>:ns_audit:handle_call:110]Audit modify_index_storage_mode: [{storageMode,<<"forestdb">>},
                                  {real_userid,
                                      {[{domain,builtin},
                                        {user,<<"<ud>admin</ud>">>}]}},
                                  {remote,
                                      {[{ip,<<"172.20.0.2">>},{port,50778}]}},
                                  {timestamp,<<"2019-03-13T10:24:51.192Z">>}]
[ns_server:debug,2019-03-13T10:24:52.201Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{6,63719691892}}]}]
[ns_server:debug,2019-03-13T10:24:52.202Z,ns_1@127.0.0.1:ns_audit<0.376.0>:ns_audit:handle_call:110]Audit setup_node_services: [{services,[index,kv,n1ql]},
                            {node,'ns_1@127.0.0.1'},
                            {real_userid,
                                {[{domain,builtin},
                                  {user,<<"<ud>admin</ud>">>}]}},
                            {remote,{[{ip,<<"172.20.0.2">>},{port,50788}]}},
                            {timestamp,<<"2019-03-13T10:24:52.201Z">>}]
[ns_server:debug,2019-03-13T10:24:52.202Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{node,'ns_1@127.0.0.1',services} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691892}}]},
 index,kv,n1ql]
[ns_server:debug,2019-03-13T10:24:52.202Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {node,'ns_1@127.0.0.1',services}]..)
[ns_server:debug,2019-03-13T10:24:52.220Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{7,63719691892}}]}]
[ns_server:debug,2019-03-13T10:24:52.220Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:52.220Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
settings ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691892}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2019-03-13T10:24:52.255Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([rest,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:52.260Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{8,63719691892}}]}]
[ns_server:debug,2019-03-13T10:24:52.260Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest ->
[{port,8091}]
[ns_server:debug,2019-03-13T10:24:52.298Z,ns_1@127.0.0.1:menelaus_ui_auth<0.312.0>:token_server:handle_cast:202]Purge tokens []
[ns_server:debug,2019-03-13T10:24:52.298Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[6,0],
                                                                {0,1169890317},
                                                                false,[]} to {[6,
                                                                               0],
                                                                              {0,
                                                                               1169890317},
                                                                              true,
                                                                              []}
[ns_server:debug,2019-03-13T10:24:52.299Z,ns_1@127.0.0.1:ns_audit<0.376.0>:ns_audit:handle_call:110]Audit password_change: [{identity,{[{domain,builtin},
                                    {user,<<"<ud>admin</ud>">>}]}},
                        {real_userid,{[{domain,builtin},
                                       {user,<<"<ud>admin</ud>">>}]}},
                        {remote,{[{ip,<<"172.20.0.2">>},{port,50792}]}},
                        {timestamp,<<"2019-03-13T10:24:52.298Z">>}]
[ns_server:debug,2019-03-13T10:24:52.299Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{9,63719691892}}]}]
[ns_server:debug,2019-03-13T10:24:52.299Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
rest_creds ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691892}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"NO/WMqzCzNp2Ejr7V/l+cNo0kfr3vqcd9r6AQCqL+9zkwqeXtB/RYFvPMOpo22FmiLp1HLzhSsg8PhP9fdZW3A==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"KX+hlmBJXlb9G3pM5UNFlKEDNt88Aa0B5IKBhjI5H4M=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"Qs+LYINJQjW3uUjQyAN4PK4LbeU=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2019-03-13T10:24:52.300Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{10,63719691892}}]}]
[ns_server:debug,2019-03-13T10:24:52.300Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
uuid ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691892}}]}|
 <<"3059e1689c9b9ac826ead86522411f20">>]
[ns_server:debug,2019-03-13T10:24:52.302Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([rest_creds,uuid,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:52.306Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:24:52.308Z,ns_1@127.0.0.1:ns_ports_setup<0.364.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,projector,indexer,query,saslauthd_port,goxdcr]
[ns_server:debug,2019-03-13T10:24:52.312Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-03-13T10:24:52.318Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:info,2019-03-13T10:24:52.322Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.703.0>},
                       {name,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:52.327Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.245.0>
[ns_server:debug,2019-03-13T10:24:52.327Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:24:52.329Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.245.0>
[ns_server:debug,2019-03-13T10:24:52.334Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2019-03-13T10:24:52.346Z,ns_1@127.0.0.1:menelaus_cbauth<0.358.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"goxdcr-cbauth",<0.643.0>} needs_update
[error_logger:info,2019-03-13T10:24:52.346Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_children_sup}
             started: [{pid,<0.706.0>},
                       {name,{service_agent,index}},
                       {mfargs,{service_agent,start_link,[index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:52.365Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2019-03-13T10:24:52.374Z,ns_1@127.0.0.1:service_stats_collector-index<0.710.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[error_logger:info,2019-03-13T10:24:52.375Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.710.0>},
                       {name,{service_index,service_stats_collector}},
                       {mfargs,
                           {service_stats_collector,start_link,
                               [service_index]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:52.384Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.713.0>},
                       {name,{service_index,stats_archiver,"@index"}},
                       {mfargs,{stats_archiver,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:52.386Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.715.0>},
                       {name,{service_index,stats_reader,"@index"}},
                       {mfargs,{stats_reader,start_link,["@index"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:52.387Z,ns_1@127.0.0.1:ns_audit_cfg<0.372.0>:ns_audit_cfg:write_audit_json:265]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json" : [{descriptors_path,
                                                                                <<"/opt/couchbase/etc/security">>},
                                                                               {version,
                                                                                2},
                                                                               {uuid,
                                                                                <<"18411111">>},
                                                                               {event_states,
                                                                                {[]}},
                                                                               {filtering_enabled,
                                                                                true},
                                                                               {disabled_userids,
                                                                                []},
                                                                               {auditd_enabled,
                                                                                false},
                                                                               {log_path,
                                                                                <<"/opt/couchbase/var/lib/couchbase/logs">>},
                                                                               {rotate_interval,
                                                                                86400},
                                                                               {rotate_size,
                                                                                20971520},
                                                                               {sync,
                                                                                []}]
[error_logger:info,2019-03-13T10:24:52.391Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.716.0>},
                       {name,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:52.410Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.718.0>},
                       {name,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:52.414Z,ns_1@127.0.0.1:ns_audit_cfg<0.372.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2019-03-13T10:24:52.437Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.242.0>
[ns_server:debug,2019-03-13T10:24:52.437Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:24:52.439Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.242.0>
[ns_server:debug,2019-03-13T10:24:52.456Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2019-03-13T10:24:52.466Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2019-03-13T10:24:52.686Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:error,2019-03-13T10:24:52.750Z,ns_1@127.0.0.1:query_stats_collector<0.397.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[stats:warn,2019-03-13T10:24:52.755Z,ns_1@127.0.0.1:<0.402.0>:base_stats_collector:latest_tick:69](Collector: global_stats_collector) Dropped 1 ticks
[ns_server:debug,2019-03-13T10:24:53.243Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:debug,2019-03-13T10:24:53.244Z,ns_1@127.0.0.1:json_rpc_connection-projector-cbauth<0.746.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.746.0>
[ns_server:debug,2019-03-13T10:24:53.245Z,ns_1@127.0.0.1:menelaus_cbauth<0.358.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"projector-cbauth",<0.746.0>} started
[ns_server:debug,2019-03-13T10:24:53.270Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2019-03-13T10:24:53.380Z,ns_1@127.0.0.1:service_stats_collector-index<0.710.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[user:info,2019-03-13T10:24:53.396Z,ns_1@127.0.0.1:<0.749.0>:menelaus_web_alerts_srv:global_alert:116]Approaching full disk warning. Usage of disk "/opt/couchbase/var" on node "127.0.0.1" is around 92%.
[ns_server:debug,2019-03-13T10:24:53.496Z,ns_1@127.0.0.1:json_rpc_connection-index-cbauth<0.753.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-cbauth", handling process <0.753.0>
[ns_server:debug,2019-03-13T10:24:53.496Z,ns_1@127.0.0.1:menelaus_cbauth<0.358.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"index-cbauth",<0.753.0>} started
[ns_server:debug,2019-03-13T10:24:53.562Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@index-cbauth",admin}
[ns_server:error,2019-03-13T10:24:53.750Z,ns_1@127.0.0.1:query_stats_collector<0.397.0>:rest_utils:get_json_local:63]Request to (n1ql) /admin/stats failed: {error,
                                        {econnrefused,
                                         [{lhttpc_client,send_request,1,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,220}]},
                                          {lhttpc_client,execute,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,169}]},
                                          {lhttpc_client,request,9,
                                           [{file,
                                             "/home/couchbase/jenkins/workspace/couchbase-server-unix/couchdb/src/lhttpc/lhttpc_client.erl"},
                                            {line,92}]}]}}
[ns_server:debug,2019-03-13T10:24:53.842Z,ns_1@127.0.0.1:json_rpc_connection-index-service_api<0.772.0>:json_rpc_connection:init:73]Observed revrpc connection: label "index-service_api", handling process <0.772.0>
[ns_server:debug,2019-03-13T10:24:53.843Z,ns_1@127.0.0.1:service_agent-index<0.706.0>:service_agent:do_handle_connection:324]Observed new json rpc connection for index: <0.772.0>
[ns_server:debug,2019-03-13T10:24:53.843Z,ns_1@127.0.0.1:<0.709.0>:ns_pubsub:do_subscribe_link:145]Parent process of subscription {json_rpc_events,<0.707.0>} exited with reason normal
[ns_server:debug,2019-03-13T10:24:53.869Z,ns_1@127.0.0.1:json_rpc_connection-cbq-engine-cbauth<0.782.0>:json_rpc_connection:init:73]Observed revrpc connection: label "cbq-engine-cbauth", handling process <0.782.0>
[ns_server:debug,2019-03-13T10:24:53.869Z,ns_1@127.0.0.1:menelaus_cbauth<0.358.0>:menelaus_cbauth:handle_cast:102]Observed json rpc process {"cbq-engine-cbauth",<0.782.0>} started
[ns_server:debug,2019-03-13T10:24:53.925Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2019-03-13T10:24:53.970Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{11,63719691893}}]}]
[ns_server:debug,2019-03-13T10:24:53.971Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:54.383Z,ns_1@127.0.0.1:service_stats_collector-index<0.710.0>:service_stats_collector:check_status:346]Checking if service service_index is started...
[ns_server:debug,2019-03-13T10:24:54.385Z,ns_1@127.0.0.1:service_stats_collector-index<0.710.0>:service_stats_collector:check_status:350]Service service_index is started
[ns_server:debug,2019-03-13T10:24:54.932Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.287.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-03-13T10:24:55.292Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2019-03-13T10:24:55.307Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{12,63719691895}}]}]
[ns_server:debug,2019-03-13T10:24:55.308Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:55.308Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"809ca823974d9231aa4400d5407944e4">>,{4,63719691895}}],
 {configs,[[{map,[]},
            {fastForwardMap,[]},
            {repl_type,dcp},
            {uuid,<<"74fdafa75a3427824a57ff13d8c62f4b">>},
            {auth_type,sasl},
            {replica_index,true},
            {ram_quota,268435456},
            {flush_enabled,false},
            {num_threads,3},
            {eviction_policy,value_only},
            {conflict_resolution_type,seqno},
            {storage_mode,couchstore},
            {max_ttl,0},
            {compression_mode,off},
            {type,membase},
            {num_vbuckets,1024},
            {num_replicas,1},
            {replication_topology,star},
            {servers,[]},
            {sasl_password,"*****"}]]}]
[ns_server:debug,2019-03-13T10:24:55.311Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[6,0],
                                                                {0,1169890317},
                                                                true,[]} to {[6,
                                                                              0],
                                                                             {0,
                                                                              1169890317},
                                                                             true,
                                                                             [{"main-bucket",
                                                                               <<"74fdafa75a3427824a57ff13d8c62f4b">>}]}
[ns_server:debug,2019-03-13T10:24:55.311Z,ns_1@127.0.0.1:ns_audit<0.376.0>:ns_audit:handle_call:110]Audit create_bucket: [{props,{[{compression_mode,off},
                               {max_ttl,0},
                               {storage_mode,couchstore},
                               {conflict_resolution_type,seqno},
                               {eviction_policy,value_only},
                               {num_threads,3},
                               {flush_enabled,false},
                               {ram_quota,268435456},
                               {replica_index,true}]}},
                      {type,membase},
                      {bucket_name,<<"main-bucket">>},
                      {real_userid,{[{domain,builtin},
                                     {user,<<"<ud>admin</ud>">>}]}},
                      {remote,{[{ip,<<"172.20.0.2">>},{port,50852}]}},
                      {timestamp,<<"2019-03-13T10:24:55.307Z">>}]
[menelaus:info,2019-03-13T10:24:55.312Z,ns_1@127.0.0.1:<0.766.0>:menelaus_web_buckets:do_bucket_create:662]Created bucket "main-bucket" of type: couchbase
[{replica_index,true},
 {ram_quota,268435456},
 {flush_enabled,false},
 {num_threads,3},
 {eviction_policy,value_only},
 {conflict_resolution_type,seqno},
 {storage_mode,couchstore},
 {max_ttl,0},
 {compression_mode,off}]
[ns_server:debug,2019-03-13T10:24:55.325Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-03-13T10:24:55.326Z,ns_1@127.0.0.1:<0.870.0>:ns_janitor:update_servers:71]janitor decided to update servers list for bucket "main-bucket" to ['ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:24:55.328Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-03-13T10:24:55.328Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{13,63719691895}}]}]
[ns_server:debug,2019-03-13T10:24:55.329Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:55.329Z,ns_1@127.0.0.1:ns_bucket_worker<0.381.0>:ns_bucket_sup:update_children:108]Starting new child: {{single_bucket_kv_sup,"main-bucket"},
                     {single_bucket_kv_sup,start_link,["main-bucket"]},
                     permanent,infinity,supervisor,
                     [single_bucket_kv_sup]}

[ns_server:debug,2019-03-13T10:24:55.330Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"809ca823974d9231aa4400d5407944e4">>,{5,63719691895}}],
 {configs,[{"main-bucket",
            [{map,[]},
             {fastForwardMap,[]},
             {repl_type,dcp},
             {uuid,<<"74fdafa75a3427824a57ff13d8c62f4b">>},
             {auth_type,sasl},
             {replica_index,true},
             {ram_quota,268435456},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,off},
             {type,membase},
             {num_vbuckets,1024},
             {num_replicas,1},
             {replication_topology,star},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[error_logger:info,2019-03-13T10:24:55.332Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.856.0>},
                       {name,{service_index,stats_archiver,"main-bucket"}},
                       {mfargs,
                           {stats_archiver,start_link,["@index-main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.333Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_stats_children_sup}
             started: [{pid,<0.883.0>},
                       {name,{service_index,stats_reader,"main-bucket"}},
                       {mfargs,
                           {stats_reader,start_link,["@index-main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:55.341Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.245.0>
[ns_server:debug,2019-03-13T10:24:55.341Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:24:55.342Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.245.0>
[ns_server:debug,2019-03-13T10:24:55.349Z,ns_1@127.0.0.1:single_bucket_kv_sup-main-bucket<0.888.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:76]Syncing config to couchdb node
[ns_server:debug,2019-03-13T10:24:55.354Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2019-03-13T10:24:55.355Z,ns_1@127.0.0.1:single_bucket_kv_sup-main-bucket<0.888.0>:single_bucket_kv_sup:sync_config_to_couchdb_node:81]Synced config to couchdb node successfully
[ns_server:debug,2019-03-13T10:24:55.362Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2019-03-13T10:24:55.369Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2019-03-13T10:24:55.373Z,ns_1@127.0.0.1:<0.898.0>:janitor_agent:query_vbucket_states_loop:96]Exception from query_vbucket_states of "main-bucket":'ns_1@127.0.0.1'
{'EXIT',{noproc,{gen_server,call,
                            [{'janitor_agent-main-bucket','ns_1@127.0.0.1'},
                             query_vbucket_states,infinity]}}}
[ns_server:debug,2019-03-13T10:24:55.373Z,ns_1@127.0.0.1:<0.898.0>:janitor_agent:query_vbucket_states_loop_next_step:107]Waiting for "main-bucket" on 'ns_1@127.0.0.1'
[ns_server:debug,2019-03-13T10:24:55.402Z,ns_1@127.0.0.1:capi_doc_replicator-main-bucket<0.900.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[ns_server:debug,2019-03-13T10:24:55.402Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-main-bucket<0.901.0>:replicated_storage:wait_for_startup:55]Start waiting for startup
[error_logger:info,2019-03-13T10:24:55.403Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.899.0>,docs_kv_sup}
             started: [{pid,<0.900.0>},
                       {name,doc_replicator},
                       {mfargs,
                           {capi_ddoc_manager,start_replicator,
                               ["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.403Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.899.0>,docs_kv_sup}
             started: [{pid,<0.901.0>},
                       {name,doc_replication_srv},
                       {mfargs,
                           {doc_replication_srv,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:55.414Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.242.0>
[ns_server:debug,2019-03-13T10:24:55.414Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:24:55.415Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.242.0>
[ns_server:warn,2019-03-13T10:24:55.416Z,ns_1@127.0.0.1:kv_monitor<0.718.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["main-bucket"]
[ns_server:debug,2019-03-13T10:24:55.421Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[stats:error,2019-03-13T10:24:55.423Z,ns_1@127.0.0.1:<0.349.0>:stats_reader:log_bad_responses:233]Some nodes didn't respond: ['ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:24:55.430Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[error_logger:info,2019-03-13T10:24:55.444Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-main-bucket'}
             started: [{pid,<12396.254.0>},
                       {name,capi_ddoc_manager_events},
                       {mfargs,
                           {capi_ddoc_manager,start_link_event_manager,
                               ["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:55.449Z,ns_1@127.0.0.1:capi_doc_replicator-main-bucket<0.900.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.255.0>
[ns_server:debug,2019-03-13T10:24:55.449Z,ns_1@127.0.0.1:capi_ddoc_replication_srv-main-bucket<0.901.0>:replicated_storage:wait_for_startup:58]Received replicated storage registration from <12396.255.0>
[error_logger:info,2019-03-13T10:24:55.450Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'capi_ddoc_manager_sup-main-bucket'}
             started: [{pid,<12396.255.0>},
                       {name,capi_ddoc_manager},
                       {mfargs,
                           {capi_ddoc_manager,start_link,
                               ["main-bucket",<0.900.0>,<0.901.0>]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.450Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.899.0>,docs_kv_sup}
             started: [{pid,<12396.253.0>},
                       {name,capi_ddoc_manager_sup},
                       {mfargs,
                           {capi_ddoc_manager_sup,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:55.454Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:init:46]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2019-03-13T10:24:55.454Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:init:48]ns_ports_setup seems to be ready
[error_logger:info,2019-03-13T10:24:55.454Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.908.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:55.456Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:find_port_pid_loop:124]Found memcached port <12395.79.0>
[error_logger:info,2019-03-13T10:24:55.472Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.899.0>,docs_kv_sup}
             started: [{pid,<12396.257.0>},
                       {name,capi_set_view_manager},
                       {mfargs,
                           {capi_set_view_manager,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2019-03-13T10:24:55.473Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:55.479Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:init:85]found memcached port to be already active
[ns_server:debug,2019-03-13T10:24:55.495Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:apply_changed_memcached_config:163]New memcached config is hot-reloadable.
[ns_server:debug,2019-03-13T10:24:55.497Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:do_read_current_memcached_config:256]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2019-03-13T10:24:55.503Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.404.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2019-03-13T10:24:55.509Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.899.0>,docs_kv_sup}
             started: [{pid,<12396.260.0>},
                       {name,couch_stats_reader},
                       {mfargs,
                           {couch_stats_reader,start_link_remote,
                               ['couchdb_ns_1@127.0.0.1',"main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.511Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.899.0>},
                       {name,{docs_kv_sup,"main-bucket"}},
                       {mfargs,{docs_kv_sup,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:55.511Z,ns_1@127.0.0.1:goxdcr_status_keeper<0.404.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2019-03-13T10:24:55.512Z,ns_1@127.0.0.1:ns_memcached-main-bucket<0.916.0>:ns_memcached:init:158]Starting ns_memcached
[ns_server:debug,2019-03-13T10:24:55.512Z,ns_1@127.0.0.1:<0.917.0>:ns_memcached:run_connect_phase:181]Started 'connecting' phase of ns_memcached-main-bucket. Parent is <0.916.0>
[error_logger:info,2019-03-13T10:24:55.512Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.915.0>,ns_memcached_sup}
             started: [{pid,<0.916.0>},
                       {name,{ns_memcached,"main-bucket"}},
                       {mfargs,{ns_memcached,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.538Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.915.0>,ns_memcached_sup}
             started: [{pid,<0.919.0>},
                       {name,{terse_bucket_info_uploader,"main-bucket"}},
                       {mfargs,
                           {terse_bucket_info_uploader,start_link,
                               ["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.538Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.915.0>},
                       {name,{ns_memcached_sup,"main-bucket"}},
                       {mfargs,{ns_memcached_sup,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[user:info,2019-03-13T10:24:55.571Z,ns_1@127.0.0.1:memcached_config_mgr<0.908.0>:memcached_config_mgr:hot_reload_config:223]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>,
                                                                      <<"xattr_enabled">>]
[error_logger:info,2019-03-13T10:24:55.578Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.923.0>},
                       {name,{dcp_sup,"main-bucket"}},
                       {mfargs,{dcp_sup,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:55.584Z,ns_1@127.0.0.1:capi_doc_replicator-main-bucket<0.900.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2019-03-13T10:24:55.587Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.924.0>},
                       {name,{dcp_replication_manager,"main-bucket"}},
                       {mfargs,
                           {dcp_replication_manager,start_link,
                               ["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.604Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.925.0>},
                       {name,{replication_manager,"main-bucket"}},
                       {mfargs,
                           {replication_manager,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.611Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-main-bucket'}
             started: [{pid,<0.927.0>},
                       {name,rebalance_subprocesses_registry},
                       {mfargs,
                           {ns_process_registry,start_link,
                               ['rebalance_subprocesses_registry-main-bucket',
                                [{terminate_command,kill}]]}},
                       {restart_type,permanent},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:55.615Z,ns_1@127.0.0.1:janitor_agent-main-bucket<0.928.0>:janitor_agent:read_flush_counter:918]Loading flushseq failed: {error,enoent}. Assuming it's equal to global config.
[ns_server:info,2019-03-13T10:24:55.616Z,ns_1@127.0.0.1:janitor_agent-main-bucket<0.928.0>:janitor_agent:read_flush_counter_from_config:925]Initialized flushseq 0 from bucket config
[error_logger:info,2019-03-13T10:24:55.618Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'janitor_agent_sup-main-bucket'}
             started: [{pid,<0.928.0>},
                       {name,janitor_agent},
                       {mfargs,{janitor_agent,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.619Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.926.0>},
                       {name,{janitor_agent_sup,"main-bucket"}},
                       {mfargs,{janitor_agent_sup,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.630Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.929.0>},
                       {name,{stats_collector,"main-bucket"}},
                       {mfargs,{stats_collector,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.644Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.932.0>},
                       {name,{stats_archiver,"main-bucket"}},
                       {mfargs,{stats_archiver,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.645Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.934.0>},
                       {name,{stats_reader,"main-bucket"}},
                       {mfargs,{stats_reader,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2019-03-13T10:24:55.650Z,ns_1@127.0.0.1:ns_memcached-main-bucket<0.916.0>:ns_memcached:ensure_bucket:1264]Created bucket "main-bucket" with config string "max_size=268435456;dbname=/opt/couchbase/var/lib/couchbase/data/main-bucket;backend=couchdb;couch_bucket=main-bucket;max_vbuckets=1024;alog_path=/opt/couchbase/var/lib/couchbase/data/main-bucket/access.log;data_traffic_enabled=false;max_num_workers=3;uuid=74fdafa75a3427824a57ff13d8c62f4b;conflict_resolution_type=seqno;bucket_type=persistent;item_eviction_policy=value_only;max_ttl=0;ht_locks=47;compression_mode=off;failpartialwarmup=false"
[ns_server:info,2019-03-13T10:24:55.651Z,ns_1@127.0.0.1:ns_memcached-main-bucket<0.916.0>:ns_memcached:handle_cast:647]Main ns_memcached connection established: {ok,#Port<0.7140>}
[user:info,2019-03-13T10:24:55.657Z,ns_1@127.0.0.1:ns_memcached-main-bucket<0.916.0>:ns_memcached:handle_cast:676]Bucket "main-bucket" loaded on node 'ns_1@127.0.0.1' in 0 seconds.
[error_logger:info,2019-03-13T10:24:55.663Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.942.0>},
                       {name,{goxdcr_stats_collector,"main-bucket"}},
                       {mfargs,
                           {goxdcr_stats_collector,start_link,
                               ["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.670Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.944.0>},
                       {name,{goxdcr_stats_archiver,"main-bucket"}},
                       {mfargs,
                           {stats_archiver,start_link,["@xdcr-main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.671Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.946.0>},
                       {name,{goxdcr_stats_reader,"main-bucket"}},
                       {mfargs,
                           {stats_reader,start_link,["@xdcr-main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.672Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,'single_bucket_kv_sup-main-bucket'}
             started: [{pid,<0.947.0>},
                       {name,{failover_safeness_level,"main-bucket"}},
                       {mfargs,
                           {failover_safeness_level,start_link,
                               ["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2019-03-13T10:24:55.673Z,ns_1@127.0.0.1:error_logger<0.6.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_sup}
             started: [{pid,<0.888.0>},
                       {name,{single_bucket_kv_sup,"main-bucket"}},
                       {mfargs,
                           {single_bucket_kv_sup,start_link,["main-bucket"]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2019-03-13T10:24:55.717Z,ns_1@127.0.0.1:ns_heart_slow_status_updater<0.287.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "main-bucket" stats:
{error,no_samples}

[ns_server:debug,2019-03-13T10:24:55.723Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2019-03-13T10:24:55.724Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:info,2019-03-13T10:24:55.792Z,ns_1@127.0.0.1:<0.454.0>:ns_orchestrator:handle_info:467]Skipping janitor in state janitor_running
[ns_server:debug,2019-03-13T10:24:56.377Z,ns_1@127.0.0.1:janitor_agent-main-bucket<0.928.0>:dcp_sup:nuke:104]Nuking DCP replicators for bucket "main-bucket":
[]
[ns_server:info,2019-03-13T10:24:56.391Z,ns_1@127.0.0.1:<0.887.0>:ns_janitor:cleanup_with_membase_bucket_check_map:95]janitor decided to generate initial vbucket map
[ns_server:warn,2019-03-13T10:24:56.418Z,ns_1@127.0.0.1:kv_monitor<0.718.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["main-bucket"]
[ns_server:debug,2019-03-13T10:24:56.428Z,ns_1@127.0.0.1:<0.887.0>:mb_map:generate_map_old:378]Natural map score: {1024,0}
[ns_server:debug,2019-03-13T10:24:56.447Z,ns_1@127.0.0.1:<0.887.0>:mb_map:generate_map_old:385]Rnd maps scores: {1024,0}, {1024,0}
[ns_server:debug,2019-03-13T10:24:56.448Z,ns_1@127.0.0.1:<0.887.0>:mb_map:generate_map_old:392]Considering 1 maps:
[{1024,0}]
[ns_server:debug,2019-03-13T10:24:56.448Z,ns_1@127.0.0.1:<0.887.0>:mb_map:generate_map_old:397]Best map score: {1024,0} (true,true,true)
[ns_server:debug,2019-03-13T10:24:56.450Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{14,63719691896}}]}]
[ns_server:debug,2019-03-13T10:24:56.450Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([vbucket_map_history,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:debug,2019-03-13T10:24:56.462Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([buckets,
                               {local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>}]..)
[ns_server:info,2019-03-13T10:24:56.471Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 0 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.471Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.472Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 2 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.472Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 3 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.472Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 4 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.473Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 5 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.473Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 6 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.474Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 7 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.474Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 8 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.476Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 9 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.476Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 10 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.476Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 11 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-03-13T10:24:56.476Z,ns_1@127.0.0.1:capi_doc_replicator-main-bucket<0.900.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:info,2019-03-13T10:24:56.477Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 12 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.477Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 13 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.477Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 14 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.478Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 15 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.478Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 16 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.478Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 17 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.478Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 18 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.478Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 19 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.479Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 20 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.479Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 21 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.479Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 22 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.480Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 23 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.480Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 24 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.480Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 25 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.480Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 26 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.480Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 27 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.481Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 28 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.481Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 29 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.481Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 30 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.482Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 31 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.482Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 32 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.482Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 33 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.482Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 34 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.482Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 35 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.482Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 36 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.483Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 37 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.483Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 38 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.483Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 39 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.483Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 40 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.483Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 41 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.484Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 42 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.484Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 43 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.484Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 44 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.484Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 45 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.485Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 46 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.485Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 47 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.485Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 48 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.486Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 49 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.486Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 50 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.486Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 51 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.486Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 52 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.486Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 53 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.486Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 54 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.487Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 55 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.487Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 56 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.488Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 57 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.488Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 58 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.488Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 59 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.488Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 60 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.488Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 61 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.489Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 62 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.489Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 63 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.489Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 64 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.489Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 65 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.490Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 66 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.490Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 67 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.490Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 68 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.491Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 69 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.491Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 70 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.491Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 71 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.491Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 72 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.491Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 73 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.491Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 74 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.492Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 75 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.492Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 76 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-03-13T10:24:56.492Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
vbucket_map_history ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691896}}]},
 {[['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1',undefined],
   ['ns_1@127.0.0.1'|...],
   [...]|...],
  [{replication_topology,star},{tags,undefined},{max_slaves,10}]}]
[ns_server:info,2019-03-13T10:24:56.493Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 77 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.493Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 78 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-03-13T10:24:56.493Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{15,63719691896}}]}]
[ns_server:info,2019-03-13T10:24:56.493Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 79 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.493Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 80 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.493Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 81 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.493Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 82 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.494Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 83 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.494Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 84 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.494Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 85 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.494Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 86 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.494Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 87 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.494Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 88 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.495Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 89 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.495Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 90 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.495Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 91 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.495Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 92 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.495Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 93 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.495Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 94 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 95 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 96 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 97 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 98 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 99 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 100 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 101 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.496Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 102 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.497Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 103 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.497Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 104 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.497Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 105 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.497Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 106 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.498Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 107 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.498Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 108 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.498Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 109 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.498Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 110 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.498Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 111 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.498Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 112 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.499Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 113 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.499Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 114 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.499Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 115 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.499Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 116 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.499Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 117 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.499Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 118 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.500Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 119 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.500Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 120 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.500Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 121 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.501Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 122 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.501Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 123 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.504Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 124 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.504Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 125 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.504Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 126 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.504Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 127 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.505Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 128 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.505Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 129 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.505Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 130 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.505Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 131 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.506Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 132 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.506Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 133 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.506Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 134 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.506Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 135 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.506Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 136 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.507Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 137 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.507Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 138 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.507Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 139 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.507Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 140 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.507Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 141 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.507Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 142 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.508Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 143 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.511Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 144 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.511Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 145 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.511Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 146 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.516Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 147 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.516Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 148 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.516Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 149 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.516Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 150 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.517Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 151 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.517Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 152 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.518Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 153 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.518Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 154 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.519Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 155 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-03-13T10:24:56.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"809ca823974d9231aa4400d5407944e4">>,{6,63719691896}}],
 {configs,[{"main-bucket",
            [{map,[{0,[],['ns_1@127.0.0.1',undefined]},
                   {1,[],['ns_1@127.0.0.1',undefined]},
                   {2,[],['ns_1@127.0.0.1',undefined]},
                   {3,[],['ns_1@127.0.0.1',undefined]},
                   {4,[],['ns_1@127.0.0.1',undefined]},
                   {5,[],['ns_1@127.0.0.1',undefined]},
                   {6,[],['ns_1@127.0.0.1',undefined]},
                   {7,[],['ns_1@127.0.0.1',undefined]},
                   {8,[],['ns_1@127.0.0.1',undefined]},
                   {9,[],['ns_1@127.0.0.1',undefined]},
                   {10,[],['ns_1@127.0.0.1',undefined]},
                   {11,[],['ns_1@127.0.0.1',undefined]},
                   {12,[],['ns_1@127.0.0.1',undefined]},
                   {13,[],['ns_1@127.0.0.1',undefined]},
                   {14,[],['ns_1@127.0.0.1',undefined]},
                   {15,[],['ns_1@127.0.0.1',undefined]},
                   {16,[],['ns_1@127.0.0.1',undefined]},
                   {17,[],['ns_1@127.0.0.1',undefined]},
                   {18,[],['ns_1@127.0.0.1',undefined]},
                   {19,[],['ns_1@127.0.0.1',undefined]},
                   {20,[],['ns_1@127.0.0.1',undefined]},
                   {21,[],['ns_1@127.0.0.1',undefined]},
                   {22,[],['ns_1@127.0.0.1',undefined]},
                   {23,[],['ns_1@127.0.0.1',undefined]},
                   {24,[],['ns_1@127.0.0.1',undefined]},
                   {25,[],['ns_1@127.0.0.1',undefined]},
                   {26,[],['ns_1@127.0.0.1',undefined]},
                   {27,[],['ns_1@127.0.0.1',undefined]},
                   {28,[],['ns_1@127.0.0.1',undefined]},
                   {29,[],['ns_1@127.0.0.1',undefined]},
                   {30,[],['ns_1@127.0.0.1',undefined]},
                   {31,[],['ns_1@127.0.0.1',undefined]},
                   {32,[],['ns_1@127.0.0.1',undefined]},
                   {33,[],['ns_1@127.0.0.1',undefined]},
                   {34,[],['ns_1@127.0.0.1',undefined]},
                   {35,[],['ns_1@127.0.0.1',undefined]},
                   {36,[],['ns_1@127.0.0.1',undefined]},
                   {37,[],['ns_1@127.0.0.1',undefined]},
                   {38,[],['ns_1@127.0.0.1',undefined]},
                   {39,[],['ns_1@127.0.0.1',undefined]},
                   {40,[],['ns_1@127.0.0.1',undefined]},
                   {41,[],['ns_1@127.0.0.1',undefined]},
                   {42,[],['ns_1@127.0.0.1',undefined]},
                   {43,[],['ns_1@127.0.0.1',undefined]},
                   {44,[],['ns_1@127.0.0.1',undefined]},
                   {45,[],['ns_1@127.0.0.1',undefined]},
                   {46,[],['ns_1@127.0.0.1',undefined]},
                   {47,[],['ns_1@127.0.0.1',undefined]},
                   {48,[],['ns_1@127.0.0.1',undefined]},
                   {49,[],['ns_1@127.0.0.1',undefined]},
                   {50,[],['ns_1@127.0.0.1',undefined]},
                   {51,[],['ns_1@127.0.0.1',undefined]},
                   {52,[],['ns_1@127.0.0.1',undefined]},
                   {53,[],['ns_1@127.0.0.1',undefined]},
                   {54,[],['ns_1@127.0.0.1',undefined]},
                   {55,[],['ns_1@127.0.0.1',undefined]},
                   {56,[],['ns_1@127.0.0.1',undefined]},
                   {57,[],['ns_1@127.0.0.1',undefined]},
                   {58,[],['ns_1@127.0.0.1',undefined]},
                   {59,[],['ns_1@127.0.0.1',undefined]},
                   {60,[],['ns_1@127.0.0.1',undefined]},
                   {61,[],['ns_1@127.0.0.1',undefined]},
                   {62,[],['ns_1@127.0.0.1',undefined]},
                   {63,[],['ns_1@127.0.0.1',undefined]},
                   {64,[],['ns_1@127.0.0.1',undefined]},
                   {65,[],['ns_1@127.0.0.1',undefined]},
                   {66,[],['ns_1@127.0.0.1',undefined]},
                   {67,[],['ns_1@127.0.0.1',undefined]},
                   {68,[],['ns_1@127.0.0.1',undefined]},
                   {69,[],['ns_1@127.0.0.1',undefined]},
                   {70,[],['ns_1@127.0.0.1',undefined]},
                   {71,[],['ns_1@127.0.0.1',undefined]},
                   {72,[],['ns_1@127.0.0.1',undefined]},
                   {73,[],['ns_1@127.0.0.1',undefined]},
                   {74,[],['ns_1@127.0.0.1',undefined]},
                   {75,[],['ns_1@127.0.0.1',undefined]},
                   {76,[],['ns_1@127.0.0.1',undefined]},
                   {77,[],['ns_1@127.0.0.1',undefined]},
                   {78,[],['ns_1@127.0.0.1',undefined]},
                   {79,[],['ns_1@127.0.0.1',undefined]},
                   {80,[],['ns_1@127.0.0.1',undefined]},
                   {81,[],['ns_1@127.0.0.1',undefined]},
                   {82,[],['ns_1@127.0.0.1',undefined]},
                   {83,[],['ns_1@127.0.0.1',undefined]},
                   {84,[],['ns_1@127.0.0.1'|...]},
                   {85,[],[...]},
                   {86,[],...},
                   {87,...},
                   {...}|...]},
             {fastForwardMap,[]},
             {repl_type,dcp},
             {uuid,<<"74fdafa75a3427824a57ff13d8c62f4b">>},
             {auth_type,sasl},
             {replica_index,true},
             {ram_quota,268435456},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,off},
             {type,membase},
             {num_vbuckets,1024},
             {num_replicas,1},
             {replication_topology,star},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"}]}]}]
[ns_server:info,2019-03-13T10:24:56.519Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 156 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.520Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 157 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-03-13T10:24:56.520Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{16,63719691896}}]}]
[ns_server:info,2019-03-13T10:24:56.521Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 158 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.521Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 159 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.522Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 160 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:debug,2019-03-13T10:24:56.522Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
buckets ->
[[{<<"809ca823974d9231aa4400d5407944e4">>,{7,63719691896}}],
 {configs,[{"main-bucket",
            [{map,[]},
             {fastForwardMap,[]},
             {repl_type,dcp},
             {uuid,<<"74fdafa75a3427824a57ff13d8c62f4b">>},
             {auth_type,sasl},
             {replica_index,true},
             {ram_quota,268435456},
             {flush_enabled,false},
             {num_threads,3},
             {eviction_policy,value_only},
             {conflict_resolution_type,seqno},
             {storage_mode,couchstore},
             {max_ttl,0},
             {compression_mode,off},
             {type,membase},
             {num_vbuckets,1024},
             {num_replicas,1},
             {replication_topology,star},
             {servers,['ns_1@127.0.0.1']},
             {sasl_password,"*****"},
             {map_opts_hash,133465355}]}]}]
[ns_server:info,2019-03-13T10:24:56.522Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 161 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.522Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 162 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.522Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 163 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.523Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 164 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.523Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 165 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.524Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 166 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.524Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 167 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.524Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 168 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.524Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 169 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.524Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 170 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.524Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 171 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.525Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 172 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.525Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 173 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.525Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 174 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.525Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 175 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.525Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 176 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.525Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 177 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.525Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 178 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.526Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 179 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.526Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 180 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.526Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 181 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.526Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 182 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.526Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 183 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.527Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 184 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.527Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 185 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.527Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 186 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.527Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 187 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.527Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 188 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.528Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 189 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.528Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 190 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.528Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 191 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.528Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 192 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.529Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 193 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.529Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 194 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.529Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 195 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.529Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 196 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.530Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 197 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.531Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 198 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.531Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 199 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.531Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 200 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.531Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 201 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.531Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 202 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.531Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 203 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.532Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 204 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.532Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 205 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.532Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 206 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.532Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 207 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.532Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 208 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.532Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 209 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.532Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 210 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.533Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 211 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.533Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 212 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.533Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 213 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.533Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 214 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.533Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 215 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.533Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 216 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.533Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 217 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 218 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 219 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 220 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 221 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 222 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 223 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 224 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.534Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 225 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.535Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 226 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.535Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 227 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.535Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 228 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.535Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 229 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.535Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 230 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.535Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 231 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.535Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 232 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.536Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 233 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.536Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 234 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.536Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 235 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.536Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 236 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.536Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 237 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.536Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 238 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.536Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 239 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.537Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 240 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.537Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 241 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.537Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 242 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.537Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 243 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.537Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 244 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.537Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 245 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.538Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 246 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.538Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 247 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.538Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 248 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.538Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 249 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.538Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 250 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.538Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 251 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.539Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 252 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.539Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 253 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.539Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 254 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.540Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 255 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.540Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 256 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.540Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 257 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.540Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 258 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.540Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 259 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.545Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 260 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.545Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 261 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.545Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 262 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.545Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 263 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.545Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 264 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.545Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 265 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.545Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 266 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 267 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 268 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 269 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 270 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 271 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 272 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 273 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.546Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 274 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 275 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 276 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 277 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 278 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 279 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 280 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 281 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 282 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.547Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 283 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 284 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 285 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 286 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 287 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 288 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 289 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 290 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 291 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.548Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 292 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 293 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 294 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 295 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 296 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 297 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 298 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 299 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 300 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.549Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 301 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 302 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 303 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 304 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 305 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 306 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 307 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 308 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.550Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 309 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.551Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 310 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.551Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 311 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.551Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 312 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.555Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 313 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.555Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 314 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.555Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 315 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.555Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 316 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.555Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 317 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.555Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 318 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.555Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 319 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.556Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 320 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.556Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 321 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.556Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 322 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.557Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 323 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.557Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 324 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.557Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 325 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.557Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 326 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.557Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 327 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.557Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 328 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.557Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 329 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.558Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 330 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.558Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 331 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.558Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 332 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.558Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 333 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.558Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 334 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.558Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 335 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.558Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 336 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.559Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 337 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.559Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 338 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.559Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 339 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.559Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 340 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.560Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 341 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.560Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 342 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.560Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 343 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.560Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 344 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.560Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 345 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.561Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 346 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.561Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 347 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.561Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 348 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.561Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 349 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.561Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 350 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.561Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 351 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.562Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 352 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.562Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 353 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.562Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 354 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.563Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 355 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.563Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 356 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.563Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 357 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.563Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 358 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.563Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 359 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.564Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 360 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.564Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 361 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.564Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 362 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.565Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 363 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.565Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 364 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.565Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 365 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.565Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 366 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.565Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 367 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.566Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 368 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.566Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 369 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.566Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 370 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.566Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 371 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.566Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 372 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.567Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 373 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.567Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 374 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.567Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 375 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.567Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 376 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.567Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 377 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.568Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 378 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.568Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 379 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.568Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 380 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.568Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 381 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.568Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 382 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.569Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 383 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.569Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 384 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.569Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 385 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.569Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 386 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.569Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 387 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.569Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 388 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.569Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 389 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.570Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 390 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.570Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 391 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.570Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 392 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.570Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 393 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.570Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 394 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.570Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 395 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.570Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 396 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.571Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 397 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.571Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 398 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.571Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 399 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.571Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 400 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.571Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 401 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.571Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 402 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.571Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 403 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.572Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 404 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.572Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 405 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.572Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 406 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.572Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 407 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.572Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 408 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.572Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 409 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.573Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 410 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.573Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 411 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.573Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 412 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.573Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 413 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.573Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 414 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.573Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 415 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.573Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 416 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 417 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 418 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 419 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 420 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 421 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 422 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 423 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.574Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 424 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.575Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 425 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.575Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 426 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.575Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 427 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.575Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 428 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.575Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 429 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.575Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 430 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.576Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 431 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.576Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 432 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.576Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 433 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.576Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 434 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.576Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 435 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.576Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 436 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 437 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 438 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 439 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 440 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 441 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 442 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 443 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.577Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 444 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.578Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 445 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.578Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 446 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.578Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 447 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.578Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 448 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.578Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 449 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.578Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 450 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.578Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 451 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.579Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 452 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.579Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 453 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.579Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 454 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.579Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 455 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.579Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 456 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.580Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 457 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.580Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 458 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.580Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 459 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.580Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 460 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.581Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 461 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.582Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 462 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.583Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 463 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.584Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 464 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.584Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 465 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.584Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 466 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.584Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 467 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.585Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 468 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.585Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 469 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.586Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 470 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.586Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 471 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.586Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 472 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.586Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 473 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.587Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 474 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.587Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 475 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.587Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 476 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.588Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 477 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.588Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 478 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.588Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 479 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.588Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 480 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.589Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 481 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.589Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 482 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.589Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 483 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.589Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 484 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.589Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 485 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 486 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 487 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 488 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 489 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 490 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 491 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 492 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.590Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 493 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.591Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 494 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.591Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 495 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.591Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 496 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.591Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 497 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.591Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 498 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.591Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 499 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.591Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 500 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.592Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 501 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.592Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 502 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.592Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 503 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.592Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 504 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.592Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 505 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.592Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 506 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.592Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 507 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.593Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 508 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.593Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 509 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.593Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 510 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.593Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 511 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.593Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 512 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.593Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 513 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.593Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 514 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 515 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 516 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 517 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 518 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 519 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 520 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 521 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.594Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 522 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.595Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 523 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.595Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 524 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.595Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 525 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.595Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 526 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.595Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 527 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.595Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 528 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.595Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 529 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.596Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 530 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.596Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 531 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.596Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 532 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.596Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 533 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.596Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 534 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.596Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 535 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.596Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 536 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.597Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 537 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.597Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 538 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.597Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 539 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.598Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 540 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.598Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 541 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.598Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 542 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.598Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 543 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.599Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 544 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.599Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 545 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.599Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 546 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.599Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 547 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.599Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 548 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.599Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 549 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.600Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 550 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.600Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 551 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.600Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 552 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.600Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 553 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.601Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 554 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.601Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 555 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.601Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 556 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.601Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 557 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.601Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 558 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.601Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 559 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.601Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 560 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.602Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 561 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.602Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 562 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.602Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 563 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.602Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 564 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.602Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 565 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.602Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 566 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.603Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 567 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.603Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 568 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.603Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 569 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.603Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 570 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.603Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 571 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.603Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 572 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 573 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 574 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 575 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 576 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 577 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 578 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 579 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.604Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 580 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.605Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 581 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.605Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 582 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.605Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 583 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.605Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 584 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.605Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 585 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.605Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 586 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 587 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 588 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 589 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 590 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 591 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 592 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 593 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.606Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 594 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.607Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 595 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.607Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 596 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.607Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 597 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.607Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 598 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.607Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 599 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.607Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 600 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.607Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 601 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.608Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 602 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.608Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 603 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.608Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 604 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.608Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 605 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.608Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 606 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 607 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 608 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 609 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 610 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 611 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 612 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 613 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.609Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 614 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 615 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 616 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 617 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 618 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 619 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 620 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 621 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.610Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 622 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 623 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 624 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 625 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 626 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 627 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 628 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 629 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.611Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 630 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.612Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 631 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.612Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 632 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.612Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 633 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.612Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 634 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.612Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 635 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.612Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 636 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.613Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 637 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.613Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 638 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.613Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 639 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.613Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 640 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.613Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 641 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.613Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 642 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.614Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 643 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.614Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 644 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.615Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 645 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.615Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 646 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.616Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 647 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.617Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 648 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.617Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 649 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.617Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 650 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.618Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 651 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.618Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 652 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.619Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 653 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.619Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 654 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.619Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 655 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.619Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 656 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.619Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 657 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 658 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 659 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 660 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 661 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 662 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 663 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 664 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.620Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 665 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.621Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 666 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.621Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 667 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.621Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 668 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.621Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 669 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.621Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 670 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.621Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 671 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.621Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 672 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.622Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 673 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.622Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 674 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.622Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 675 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.622Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 676 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.622Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 677 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.622Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 678 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 679 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 680 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 681 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 682 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 683 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 684 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 685 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.623Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 686 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 687 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 688 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 689 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 690 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 691 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 692 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 693 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.624Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 694 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 695 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 696 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 697 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 698 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 699 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 700 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 701 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.625Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 702 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 703 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 704 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 705 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 706 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 707 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 708 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 709 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.626Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 710 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 711 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 712 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 713 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 714 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 715 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 716 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 717 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.627Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 718 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 719 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 720 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 721 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 722 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 723 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 724 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 725 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.628Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 726 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.629Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 727 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.629Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 728 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.629Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 729 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.629Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 730 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.629Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 731 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.629Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 732 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.629Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 733 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.630Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 734 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.630Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 735 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.630Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 736 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.630Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 737 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.631Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 738 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.631Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 739 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.631Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 740 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.631Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 741 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.631Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 742 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.632Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 743 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.632Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 744 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.632Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 745 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.632Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 746 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.633Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 747 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.633Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 748 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.633Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 749 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.633Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 750 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.634Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 751 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.634Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 752 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.634Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 753 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.634Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 754 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.634Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 755 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.634Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 756 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.635Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 757 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.635Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 758 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.635Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 759 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.635Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 760 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.635Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 761 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.635Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 762 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 763 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 764 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 765 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 766 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 767 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 768 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 769 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.636Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 770 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 771 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 772 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 773 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 774 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 775 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 776 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 777 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.637Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 778 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 779 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 780 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 781 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 782 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 783 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 784 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 785 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.638Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 786 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 787 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 788 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 789 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 790 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 791 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 792 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 793 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.639Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 794 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 795 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 796 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 797 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 798 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 799 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 800 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 801 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.640Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 802 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.641Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 803 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.641Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 804 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.641Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 805 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.641Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 806 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.641Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 807 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.641Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 808 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.641Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 809 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 810 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 811 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 812 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 813 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 814 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 815 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 816 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.642Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 817 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.643Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 818 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.643Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 819 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.643Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 820 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.643Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 821 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.643Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 822 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.643Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 823 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 824 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 825 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 826 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 827 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 828 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 829 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 830 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 831 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.644Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 832 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.645Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 833 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.645Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 834 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.645Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 835 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.645Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 836 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.645Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 837 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.645Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 838 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.645Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 839 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 840 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 841 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 842 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 843 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 844 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 845 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 846 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 847 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.646Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 848 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 849 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 850 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 851 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 852 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 853 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 854 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 855 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 856 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.647Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 857 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 858 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 859 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 860 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 861 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 862 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 863 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 864 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 865 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.648Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 866 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 867 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 868 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 869 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 870 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 871 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 872 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 873 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 874 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.649Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 875 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 876 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 877 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 878 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 879 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 880 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 881 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 882 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.650Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 883 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 884 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 885 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 886 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 887 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 888 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 889 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 890 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.651Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 891 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 892 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 893 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 894 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 895 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 896 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 897 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 898 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.652Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 899 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 900 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 901 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 902 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 903 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 904 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 905 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 906 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 907 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.653Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 908 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 909 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 910 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 911 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 912 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 913 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 914 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 915 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 916 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.654Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 917 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.655Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 918 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.655Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 919 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.655Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 920 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.655Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 921 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.655Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 922 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.655Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 923 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.655Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 924 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 925 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 926 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 927 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 928 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 929 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 930 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 931 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 932 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.656Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 933 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.657Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 934 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.657Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 935 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.657Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 936 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.657Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 937 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.657Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 938 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.657Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 939 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 940 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 941 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 942 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 943 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 944 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 945 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 946 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.658Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 947 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.659Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 948 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.659Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 949 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.659Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 950 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.659Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 951 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.659Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 952 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.659Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 953 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.659Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 954 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.660Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 955 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.660Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 956 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.660Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 957 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.660Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 958 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.660Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 959 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.660Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 960 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 961 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 962 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 963 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 964 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 965 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 966 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 967 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.661Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 968 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 969 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 970 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 971 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 972 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 973 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 974 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 975 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.662Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 976 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 977 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 978 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 979 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 980 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 981 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 982 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 983 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.663Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 984 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 985 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 986 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 987 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 988 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 989 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 990 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 991 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 992 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.664Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 993 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.665Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 994 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.665Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 995 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.665Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 996 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.665Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 997 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.665Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 998 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.665Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 999 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.666Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1000 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.666Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1001 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.666Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1002 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.666Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1003 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.666Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1004 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.666Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1005 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.666Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1006 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.667Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1007 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.667Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1008 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.667Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1009 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.667Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1010 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.667Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1011 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.667Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1012 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.667Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1013 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1014 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1015 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1016 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1017 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1018 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1019 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1020 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1021 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.668Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1022 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.669Z,ns_1@127.0.0.1:<0.1013.0>:ns_janitor:do_sanify_chain:447]Setting vbucket 1023 in "main-bucket" on 'ns_1@127.0.0.1' from missing to active.
[ns_server:info,2019-03-13T10:24:56.692Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 1023 state to active
[ns_server:info,2019-03-13T10:24:56.694Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 1022 state to active
[ns_server:info,2019-03-13T10:24:56.695Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1021 state to active
[ns_server:info,2019-03-13T10:24:56.697Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1020 state to active
[ns_server:info,2019-03-13T10:24:56.699Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1019 state to active
[ns_server:info,2019-03-13T10:24:56.700Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1018 state to active
[ns_server:info,2019-03-13T10:24:56.700Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1017 state to active
[ns_server:info,2019-03-13T10:24:56.702Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1016 state to active
[ns_server:info,2019-03-13T10:24:56.703Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1015 state to active
[ns_server:info,2019-03-13T10:24:56.704Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1014 state to active
[ns_server:info,2019-03-13T10:24:56.705Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1013 state to active
[ns_server:info,2019-03-13T10:24:56.707Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1012 state to active
[ns_server:info,2019-03-13T10:24:56.708Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1011 state to active
[ns_server:info,2019-03-13T10:24:56.709Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1010 state to active
[ns_server:info,2019-03-13T10:24:56.711Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1009 state to active
[ns_server:info,2019-03-13T10:24:56.713Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1008 state to active
[ns_server:info,2019-03-13T10:24:56.716Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1007 state to active
[ns_server:info,2019-03-13T10:24:56.719Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1006 state to active
[ns_server:info,2019-03-13T10:24:56.721Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1005 state to active
[ns_server:info,2019-03-13T10:24:56.722Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1004 state to active
[ns_server:info,2019-03-13T10:24:56.725Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1003 state to active
[ns_server:info,2019-03-13T10:24:56.727Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1002 state to active
[ns_server:info,2019-03-13T10:24:56.728Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1001 state to active
[ns_server:info,2019-03-13T10:24:56.729Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 1000 state to active
[ns_server:info,2019-03-13T10:24:56.731Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 999 state to active
[ns_server:info,2019-03-13T10:24:56.738Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 998 state to active
[ns_server:info,2019-03-13T10:24:56.740Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 997 state to active
[ns_server:info,2019-03-13T10:24:56.743Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 996 state to active
[ns_server:info,2019-03-13T10:24:56.744Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 995 state to active
[ns_server:info,2019-03-13T10:24:56.746Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 994 state to active
[ns_server:info,2019-03-13T10:24:56.748Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 993 state to active
[ns_server:info,2019-03-13T10:24:56.750Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 992 state to active
[ns_server:info,2019-03-13T10:24:56.755Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 991 state to active
[ns_server:info,2019-03-13T10:24:56.763Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 990 state to active
[ns_server:info,2019-03-13T10:24:56.764Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 989 state to active
[ns_server:info,2019-03-13T10:24:56.768Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 988 state to active
[ns_server:info,2019-03-13T10:24:56.770Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 987 state to active
[ns_server:info,2019-03-13T10:24:56.783Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 986 state to active
[ns_server:info,2019-03-13T10:24:56.789Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 985 state to active
[ns_server:info,2019-03-13T10:24:56.792Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 984 state to active
[ns_server:info,2019-03-13T10:24:56.802Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 983 state to active
[ns_server:info,2019-03-13T10:24:56.804Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 982 state to active
[ns_server:info,2019-03-13T10:24:56.807Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 981 state to active
[ns_server:info,2019-03-13T10:24:56.809Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 980 state to active
[ns_server:info,2019-03-13T10:24:56.814Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 979 state to active
[ns_server:info,2019-03-13T10:24:56.816Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 978 state to active
[ns_server:info,2019-03-13T10:24:56.817Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 977 state to active
[ns_server:info,2019-03-13T10:24:56.818Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 976 state to active
[ns_server:info,2019-03-13T10:24:56.819Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 975 state to active
[ns_server:info,2019-03-13T10:24:56.821Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 974 state to active
[ns_server:info,2019-03-13T10:24:56.822Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 973 state to active
[ns_server:info,2019-03-13T10:24:56.824Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 972 state to active
[ns_server:info,2019-03-13T10:24:56.826Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 971 state to active
[ns_server:info,2019-03-13T10:24:56.827Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 970 state to active
[ns_server:info,2019-03-13T10:24:56.829Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 969 state to active
[ns_server:info,2019-03-13T10:24:56.831Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 968 state to active
[ns_server:info,2019-03-13T10:24:56.833Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 967 state to active
[ns_server:info,2019-03-13T10:24:56.836Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 966 state to active
[ns_server:info,2019-03-13T10:24:56.837Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 965 state to active
[ns_server:info,2019-03-13T10:24:56.838Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 964 state to active
[ns_server:info,2019-03-13T10:24:56.839Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 963 state to active
[ns_server:info,2019-03-13T10:24:56.841Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 962 state to active
[ns_server:info,2019-03-13T10:24:56.841Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 961 state to active
[ns_server:info,2019-03-13T10:24:56.843Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 960 state to active
[ns_server:info,2019-03-13T10:24:56.844Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 959 state to active
[ns_server:info,2019-03-13T10:24:56.847Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 958 state to active
[ns_server:info,2019-03-13T10:24:56.848Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 957 state to active
[ns_server:info,2019-03-13T10:24:56.849Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 956 state to active
[ns_server:info,2019-03-13T10:24:56.851Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 955 state to active
[ns_server:info,2019-03-13T10:24:56.853Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 954 state to active
[ns_server:info,2019-03-13T10:24:56.856Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 953 state to active
[ns_server:info,2019-03-13T10:24:56.857Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 952 state to active
[ns_server:info,2019-03-13T10:24:56.859Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 951 state to active
[ns_server:info,2019-03-13T10:24:56.860Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 950 state to active
[ns_server:info,2019-03-13T10:24:56.861Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 949 state to active
[ns_server:info,2019-03-13T10:24:56.863Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 948 state to active
[ns_server:info,2019-03-13T10:24:56.864Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 947 state to active
[ns_server:info,2019-03-13T10:24:56.866Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 946 state to active
[ns_server:info,2019-03-13T10:24:56.867Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 945 state to active
[ns_server:info,2019-03-13T10:24:56.869Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 944 state to active
[ns_server:info,2019-03-13T10:24:56.870Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 943 state to active
[ns_server:info,2019-03-13T10:24:56.872Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 942 state to active
[ns_server:info,2019-03-13T10:24:56.874Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 941 state to active
[ns_server:info,2019-03-13T10:24:56.876Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 940 state to active
[ns_server:info,2019-03-13T10:24:56.877Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 939 state to active
[ns_server:info,2019-03-13T10:24:56.879Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 938 state to active
[ns_server:info,2019-03-13T10:24:56.880Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 937 state to active
[ns_server:info,2019-03-13T10:24:56.882Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 936 state to active
[ns_server:info,2019-03-13T10:24:56.884Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 935 state to active
[ns_server:info,2019-03-13T10:24:56.886Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 934 state to active
[ns_server:info,2019-03-13T10:24:56.888Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 933 state to active
[ns_server:info,2019-03-13T10:24:56.890Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 932 state to active
[ns_server:info,2019-03-13T10:24:56.897Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 931 state to active
[ns_server:info,2019-03-13T10:24:56.898Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 930 state to active
[ns_server:info,2019-03-13T10:24:56.899Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 929 state to active
[ns_server:info,2019-03-13T10:24:56.901Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 928 state to active
[ns_server:info,2019-03-13T10:24:56.903Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 927 state to active
[ns_server:info,2019-03-13T10:24:56.905Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 926 state to active
[ns_server:info,2019-03-13T10:24:56.907Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 925 state to active
[ns_server:info,2019-03-13T10:24:56.909Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 924 state to active
[ns_server:info,2019-03-13T10:24:56.910Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 923 state to active
[ns_server:info,2019-03-13T10:24:56.911Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 922 state to active
[ns_server:info,2019-03-13T10:24:56.914Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 921 state to active
[ns_server:info,2019-03-13T10:24:56.916Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 920 state to active
[ns_server:info,2019-03-13T10:24:56.919Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 919 state to active
[ns_server:info,2019-03-13T10:24:56.922Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 918 state to active
[ns_server:info,2019-03-13T10:24:56.927Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 917 state to active
[ns_server:info,2019-03-13T10:24:56.929Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 916 state to active
[ns_server:info,2019-03-13T10:24:56.931Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 915 state to active
[ns_server:info,2019-03-13T10:24:56.934Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 914 state to active
[ns_server:info,2019-03-13T10:24:56.935Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 913 state to active
[ns_server:debug,2019-03-13T10:24:56.935Z,ns_1@127.0.0.1:<0.481.0>:auto_failover:log_down_nodes_reason:382]Node 'ns_1@127.0.0.1' is considered down. Reason:"The data service did not respond for the duration of the auto-failover threshold. Either none of the buckets have warmed up or there is an issue with the data service. "
[ns_server:debug,2019-03-13T10:24:56.936Z,ns_1@127.0.0.1:<0.481.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
            0,up,false}
->{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
              0,half_down,false}
[ns_server:info,2019-03-13T10:24:56.937Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 912 state to active
[ns_server:info,2019-03-13T10:24:56.939Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 911 state to active
[ns_server:info,2019-03-13T10:24:56.941Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 910 state to active
[ns_server:info,2019-03-13T10:24:56.944Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 909 state to active
[ns_server:info,2019-03-13T10:24:56.946Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 908 state to active
[ns_server:info,2019-03-13T10:24:56.949Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 907 state to active
[ns_server:info,2019-03-13T10:24:56.950Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 906 state to active
[ns_server:info,2019-03-13T10:24:56.952Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 905 state to active
[ns_server:info,2019-03-13T10:24:56.955Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 904 state to active
[ns_server:info,2019-03-13T10:24:56.957Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 903 state to active
[ns_server:info,2019-03-13T10:24:56.958Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 902 state to active
[ns_server:info,2019-03-13T10:24:56.961Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 901 state to active
[ns_server:info,2019-03-13T10:24:56.964Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 900 state to active
[ns_server:info,2019-03-13T10:24:56.966Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 899 state to active
[ns_server:info,2019-03-13T10:24:56.968Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 898 state to active
[ns_server:info,2019-03-13T10:24:56.970Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 897 state to active
[ns_server:info,2019-03-13T10:24:56.971Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 896 state to active
[ns_server:info,2019-03-13T10:24:56.973Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 895 state to active
[ns_server:info,2019-03-13T10:24:56.975Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 894 state to active
[ns_server:info,2019-03-13T10:24:56.977Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 893 state to active
[ns_server:info,2019-03-13T10:24:56.978Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 892 state to active
[ns_server:info,2019-03-13T10:24:56.979Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 891 state to active
[ns_server:info,2019-03-13T10:24:56.992Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 890 state to active
[ns_server:info,2019-03-13T10:24:56.994Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 889 state to active
[ns_server:info,2019-03-13T10:24:56.995Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 888 state to active
[ns_server:info,2019-03-13T10:24:56.997Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 887 state to active
[ns_server:info,2019-03-13T10:24:56.998Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 886 state to active
[ns_server:info,2019-03-13T10:24:57.003Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 885 state to active
[ns_server:info,2019-03-13T10:24:57.004Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 884 state to active
[ns_server:info,2019-03-13T10:24:57.006Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 883 state to active
[ns_server:info,2019-03-13T10:24:57.008Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 882 state to active
[ns_server:info,2019-03-13T10:24:57.011Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 881 state to active
[ns_server:info,2019-03-13T10:24:57.013Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 880 state to active
[ns_server:info,2019-03-13T10:24:57.016Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 879 state to active
[ns_server:info,2019-03-13T10:24:57.019Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 878 state to active
[ns_server:info,2019-03-13T10:24:57.021Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 877 state to active
[ns_server:info,2019-03-13T10:24:57.022Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 876 state to active
[ns_server:info,2019-03-13T10:24:57.025Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 875 state to active
[ns_server:info,2019-03-13T10:24:57.027Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 874 state to active
[ns_server:info,2019-03-13T10:24:57.028Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 873 state to active
[ns_server:info,2019-03-13T10:24:57.030Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 872 state to active
[ns_server:info,2019-03-13T10:24:57.044Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 871 state to active
[ns_server:info,2019-03-13T10:24:57.045Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 870 state to active
[ns_server:info,2019-03-13T10:24:57.046Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 869 state to active
[ns_server:info,2019-03-13T10:24:57.047Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 868 state to active
[ns_server:info,2019-03-13T10:24:57.049Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 867 state to active
[ns_server:info,2019-03-13T10:24:57.050Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 866 state to active
[ns_server:info,2019-03-13T10:24:57.051Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 865 state to active
[ns_server:info,2019-03-13T10:24:57.054Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 864 state to active
[ns_server:info,2019-03-13T10:24:57.056Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 863 state to active
[ns_server:info,2019-03-13T10:24:57.057Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 862 state to active
[ns_server:info,2019-03-13T10:24:57.058Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 861 state to active
[ns_server:info,2019-03-13T10:24:57.059Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 860 state to active
[ns_server:info,2019-03-13T10:24:57.061Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 859 state to active
[ns_server:info,2019-03-13T10:24:57.064Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 858 state to active
[ns_server:info,2019-03-13T10:24:57.066Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 857 state to active
[ns_server:info,2019-03-13T10:24:57.067Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 856 state to active
[ns_server:info,2019-03-13T10:24:57.069Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 855 state to active
[ns_server:info,2019-03-13T10:24:57.071Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 854 state to active
[ns_server:info,2019-03-13T10:24:57.072Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 853 state to active
[ns_server:info,2019-03-13T10:24:57.074Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 852 state to active
[ns_server:info,2019-03-13T10:24:57.077Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 851 state to active
[ns_server:info,2019-03-13T10:24:57.078Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 850 state to active
[ns_server:info,2019-03-13T10:24:57.079Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 849 state to active
[ns_server:info,2019-03-13T10:24:57.090Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 848 state to active
[ns_server:info,2019-03-13T10:24:57.092Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 847 state to active
[ns_server:info,2019-03-13T10:24:57.093Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 846 state to active
[ns_server:info,2019-03-13T10:24:57.094Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 845 state to active
[ns_server:info,2019-03-13T10:24:57.096Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 844 state to active
[ns_server:info,2019-03-13T10:24:57.097Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 843 state to active
[ns_server:info,2019-03-13T10:24:57.098Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 842 state to active
[ns_server:info,2019-03-13T10:24:57.099Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 841 state to active
[ns_server:info,2019-03-13T10:24:57.100Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 840 state to active
[ns_server:info,2019-03-13T10:24:57.101Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 839 state to active
[ns_server:info,2019-03-13T10:24:57.103Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 838 state to active
[ns_server:info,2019-03-13T10:24:57.104Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 837 state to active
[ns_server:info,2019-03-13T10:24:57.105Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 836 state to active
[ns_server:info,2019-03-13T10:24:57.106Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 835 state to active
[ns_server:info,2019-03-13T10:24:57.108Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 834 state to active
[ns_server:info,2019-03-13T10:24:57.109Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 833 state to active
[ns_server:info,2019-03-13T10:24:57.110Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 832 state to active
[ns_server:info,2019-03-13T10:24:57.112Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 831 state to active
[ns_server:info,2019-03-13T10:24:57.113Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 830 state to active
[ns_server:info,2019-03-13T10:24:57.114Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 829 state to active
[ns_server:info,2019-03-13T10:24:57.115Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 828 state to active
[ns_server:info,2019-03-13T10:24:57.117Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 827 state to active
[ns_server:info,2019-03-13T10:24:57.118Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 826 state to active
[ns_server:info,2019-03-13T10:24:57.119Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 825 state to active
[ns_server:info,2019-03-13T10:24:57.120Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 824 state to active
[ns_server:info,2019-03-13T10:24:57.121Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 823 state to active
[ns_server:info,2019-03-13T10:24:57.122Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 822 state to active
[ns_server:info,2019-03-13T10:24:57.124Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 821 state to active
[ns_server:info,2019-03-13T10:24:57.125Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 820 state to active
[ns_server:info,2019-03-13T10:24:57.126Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 819 state to active
[ns_server:info,2019-03-13T10:24:57.127Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 818 state to active
[ns_server:info,2019-03-13T10:24:57.129Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 817 state to active
[ns_server:info,2019-03-13T10:24:57.130Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 816 state to active
[ns_server:info,2019-03-13T10:24:57.131Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 815 state to active
[ns_server:info,2019-03-13T10:24:57.132Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 814 state to active
[ns_server:info,2019-03-13T10:24:57.133Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 813 state to active
[ns_server:info,2019-03-13T10:24:57.134Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 812 state to active
[ns_server:info,2019-03-13T10:24:57.135Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 811 state to active
[ns_server:info,2019-03-13T10:24:57.136Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 810 state to active
[ns_server:info,2019-03-13T10:24:57.137Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 809 state to active
[ns_server:info,2019-03-13T10:24:57.137Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 808 state to active
[ns_server:info,2019-03-13T10:24:57.138Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 807 state to active
[ns_server:info,2019-03-13T10:24:57.139Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 806 state to active
[ns_server:info,2019-03-13T10:24:57.140Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 805 state to active
[ns_server:info,2019-03-13T10:24:57.141Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 804 state to active
[ns_server:info,2019-03-13T10:24:57.142Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 803 state to active
[ns_server:info,2019-03-13T10:24:57.143Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 802 state to active
[ns_server:info,2019-03-13T10:24:57.144Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 801 state to active
[ns_server:info,2019-03-13T10:24:57.145Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 800 state to active
[ns_server:info,2019-03-13T10:24:57.146Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 799 state to active
[ns_server:info,2019-03-13T10:24:57.148Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 798 state to active
[ns_server:info,2019-03-13T10:24:57.150Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 797 state to active
[ns_server:info,2019-03-13T10:24:57.151Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 796 state to active
[ns_server:info,2019-03-13T10:24:57.153Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 795 state to active
[ns_server:info,2019-03-13T10:24:57.154Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 794 state to active
[ns_server:info,2019-03-13T10:24:57.155Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 793 state to active
[ns_server:info,2019-03-13T10:24:57.156Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 792 state to active
[ns_server:info,2019-03-13T10:24:57.157Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 791 state to active
[ns_server:info,2019-03-13T10:24:57.159Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 790 state to active
[ns_server:info,2019-03-13T10:24:57.160Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 789 state to active
[ns_server:info,2019-03-13T10:24:57.161Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 788 state to active
[ns_server:info,2019-03-13T10:24:57.162Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 787 state to active
[ns_server:info,2019-03-13T10:24:57.164Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 786 state to active
[ns_server:info,2019-03-13T10:24:57.165Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 785 state to active
[ns_server:info,2019-03-13T10:24:57.166Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 784 state to active
[ns_server:info,2019-03-13T10:24:57.167Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 783 state to active
[ns_server:info,2019-03-13T10:24:57.169Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 782 state to active
[ns_server:info,2019-03-13T10:24:57.170Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 781 state to active
[ns_server:info,2019-03-13T10:24:57.171Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 780 state to active
[ns_server:info,2019-03-13T10:24:57.172Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 779 state to active
[ns_server:info,2019-03-13T10:24:57.173Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 778 state to active
[ns_server:info,2019-03-13T10:24:57.174Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 777 state to active
[ns_server:info,2019-03-13T10:24:57.176Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 776 state to active
[ns_server:info,2019-03-13T10:24:57.177Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 775 state to active
[ns_server:info,2019-03-13T10:24:57.179Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 774 state to active
[ns_server:info,2019-03-13T10:24:57.180Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 773 state to active
[ns_server:info,2019-03-13T10:24:57.182Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 772 state to active
[ns_server:info,2019-03-13T10:24:57.185Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 771 state to active
[ns_server:info,2019-03-13T10:24:57.186Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 770 state to active
[ns_server:info,2019-03-13T10:24:57.189Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 769 state to active
[ns_server:info,2019-03-13T10:24:57.189Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 768 state to active
[ns_server:info,2019-03-13T10:24:57.191Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 767 state to active
[ns_server:info,2019-03-13T10:24:57.192Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 766 state to active
[ns_server:info,2019-03-13T10:24:57.195Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 765 state to active
[ns_server:info,2019-03-13T10:24:57.196Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 764 state to active
[ns_server:info,2019-03-13T10:24:57.198Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 763 state to active
[ns_server:info,2019-03-13T10:24:57.199Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 762 state to active
[ns_server:info,2019-03-13T10:24:57.201Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 761 state to active
[ns_server:info,2019-03-13T10:24:57.203Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 760 state to active
[ns_server:info,2019-03-13T10:24:57.205Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 759 state to active
[ns_server:info,2019-03-13T10:24:57.206Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 758 state to active
[ns_server:info,2019-03-13T10:24:57.207Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 757 state to active
[ns_server:info,2019-03-13T10:24:57.209Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 756 state to active
[ns_server:info,2019-03-13T10:24:57.210Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 755 state to active
[ns_server:info,2019-03-13T10:24:57.212Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 754 state to active
[ns_server:info,2019-03-13T10:24:57.214Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 753 state to active
[ns_server:info,2019-03-13T10:24:57.222Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 752 state to active
[ns_server:info,2019-03-13T10:24:57.224Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 751 state to active
[ns_server:info,2019-03-13T10:24:57.226Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 750 state to active
[ns_server:info,2019-03-13T10:24:57.227Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 749 state to active
[ns_server:info,2019-03-13T10:24:57.230Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 748 state to active
[ns_server:info,2019-03-13T10:24:57.233Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 747 state to active
[ns_server:info,2019-03-13T10:24:57.236Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 746 state to active
[ns_server:info,2019-03-13T10:24:57.239Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 745 state to active
[ns_server:info,2019-03-13T10:24:57.241Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 744 state to active
[ns_server:info,2019-03-13T10:24:57.243Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 743 state to active
[ns_server:info,2019-03-13T10:24:57.245Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 742 state to active
[ns_server:info,2019-03-13T10:24:57.247Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 741 state to active
[ns_server:info,2019-03-13T10:24:57.248Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 740 state to active
[ns_server:info,2019-03-13T10:24:57.249Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 739 state to active
[ns_server:info,2019-03-13T10:24:57.251Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 738 state to active
[ns_server:info,2019-03-13T10:24:57.252Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 737 state to active
[ns_server:info,2019-03-13T10:24:57.254Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 736 state to active
[ns_server:info,2019-03-13T10:24:57.256Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 735 state to active
[ns_server:info,2019-03-13T10:24:57.259Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 734 state to active
[ns_server:info,2019-03-13T10:24:57.261Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 733 state to active
[ns_server:info,2019-03-13T10:24:57.273Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 732 state to active
[ns_server:info,2019-03-13T10:24:57.276Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 731 state to active
[ns_server:info,2019-03-13T10:24:57.277Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 730 state to active
[ns_server:info,2019-03-13T10:24:57.277Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 729 state to active
[ns_server:info,2019-03-13T10:24:57.279Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 728 state to active
[ns_server:info,2019-03-13T10:24:57.287Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 727 state to active
[ns_server:info,2019-03-13T10:24:57.289Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 726 state to active
[ns_server:info,2019-03-13T10:24:57.290Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 725 state to active
[ns_server:info,2019-03-13T10:24:57.291Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 724 state to active
[ns_server:info,2019-03-13T10:24:57.292Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 723 state to active
[ns_server:info,2019-03-13T10:24:57.293Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 722 state to active
[ns_server:info,2019-03-13T10:24:57.294Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 721 state to active
[ns_server:info,2019-03-13T10:24:57.297Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 720 state to active
[ns_server:info,2019-03-13T10:24:57.299Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 719 state to active
[ns_server:info,2019-03-13T10:24:57.301Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 718 state to active
[ns_server:info,2019-03-13T10:24:57.303Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 717 state to active
[ns_server:info,2019-03-13T10:24:57.305Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 716 state to active
[ns_server:info,2019-03-13T10:24:57.307Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 715 state to active
[ns_server:info,2019-03-13T10:24:57.308Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 714 state to active
[ns_server:info,2019-03-13T10:24:57.310Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 713 state to active
[ns_server:info,2019-03-13T10:24:57.311Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 712 state to active
[ns_server:info,2019-03-13T10:24:57.312Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 711 state to active
[ns_server:info,2019-03-13T10:24:57.314Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 710 state to active
[ns_server:info,2019-03-13T10:24:57.317Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 709 state to active
[ns_server:info,2019-03-13T10:24:57.319Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 708 state to active
[ns_server:info,2019-03-13T10:24:57.321Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 707 state to active
[ns_server:info,2019-03-13T10:24:57.323Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 706 state to active
[ns_server:info,2019-03-13T10:24:57.324Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 705 state to active
[ns_server:info,2019-03-13T10:24:57.326Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 704 state to active
[ns_server:info,2019-03-13T10:24:57.328Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 703 state to active
[ns_server:info,2019-03-13T10:24:57.330Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 702 state to active
[ns_server:info,2019-03-13T10:24:57.331Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 701 state to active
[ns_server:info,2019-03-13T10:24:57.333Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 700 state to active
[ns_server:info,2019-03-13T10:24:57.335Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 699 state to active
[ns_server:info,2019-03-13T10:24:57.337Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 698 state to active
[ns_server:info,2019-03-13T10:24:57.340Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 697 state to active
[ns_server:info,2019-03-13T10:24:57.341Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 696 state to active
[ns_server:info,2019-03-13T10:24:57.343Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 695 state to active
[ns_server:info,2019-03-13T10:24:57.345Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 694 state to active
[ns_server:info,2019-03-13T10:24:57.347Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 693 state to active
[ns_server:info,2019-03-13T10:24:57.349Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 692 state to active
[ns_server:info,2019-03-13T10:24:57.350Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 691 state to active
[ns_server:info,2019-03-13T10:24:57.351Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 690 state to active
[ns_server:info,2019-03-13T10:24:57.358Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 689 state to active
[ns_server:info,2019-03-13T10:24:57.360Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 688 state to active
[ns_server:info,2019-03-13T10:24:57.372Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 687 state to active
[ns_server:info,2019-03-13T10:24:57.374Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 686 state to active
[ns_server:info,2019-03-13T10:24:57.375Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 685 state to active
[ns_server:info,2019-03-13T10:24:57.377Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 684 state to active
[ns_server:info,2019-03-13T10:24:57.379Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 683 state to active
[ns_server:info,2019-03-13T10:24:57.383Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 682 state to active
[ns_server:info,2019-03-13T10:24:57.384Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 681 state to active
[ns_server:info,2019-03-13T10:24:57.386Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 680 state to active
[ns_server:info,2019-03-13T10:24:57.390Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 679 state to active
[ns_server:info,2019-03-13T10:24:57.391Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 678 state to active
[ns_server:info,2019-03-13T10:24:57.394Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 677 state to active
[ns_server:info,2019-03-13T10:24:57.395Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 676 state to active
[ns_server:info,2019-03-13T10:24:57.411Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 675 state to active
[ns_server:info,2019-03-13T10:24:57.412Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 674 state to active
[ns_server:info,2019-03-13T10:24:57.414Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 673 state to active
[ns_server:info,2019-03-13T10:24:57.417Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 672 state to active
[ns_server:info,2019-03-13T10:24:57.418Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 671 state to active
[ns_server:info,2019-03-13T10:24:57.419Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 670 state to active
[ns_server:warn,2019-03-13T10:24:57.422Z,ns_1@127.0.0.1:kv_monitor<0.718.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["main-bucket"]
[ns_server:info,2019-03-13T10:24:57.422Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 669 state to active
[ns_server:info,2019-03-13T10:24:57.424Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 668 state to active
[ns_server:info,2019-03-13T10:24:57.426Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 667 state to active
[ns_server:info,2019-03-13T10:24:57.427Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 666 state to active
[ns_server:info,2019-03-13T10:24:57.429Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 665 state to active
[ns_server:info,2019-03-13T10:24:57.431Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 664 state to active
[ns_server:info,2019-03-13T10:24:57.432Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 663 state to active
[ns_server:info,2019-03-13T10:24:57.435Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 662 state to active
[ns_server:info,2019-03-13T10:24:57.438Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 661 state to active
[ns_server:info,2019-03-13T10:24:57.440Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 660 state to active
[ns_server:info,2019-03-13T10:24:57.442Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 659 state to active
[ns_server:info,2019-03-13T10:24:57.443Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 658 state to active
[ns_server:info,2019-03-13T10:24:57.445Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 657 state to active
[ns_server:info,2019-03-13T10:24:57.448Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 656 state to active
[ns_server:info,2019-03-13T10:24:57.450Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 655 state to active
[ns_server:info,2019-03-13T10:24:57.451Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 654 state to active
[ns_server:info,2019-03-13T10:24:57.452Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 653 state to active
[ns_server:info,2019-03-13T10:24:57.455Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 652 state to active
[ns_server:info,2019-03-13T10:24:57.466Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 651 state to active
[ns_server:info,2019-03-13T10:24:57.468Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 650 state to active
[ns_server:info,2019-03-13T10:24:57.469Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 649 state to active
[ns_server:info,2019-03-13T10:24:57.471Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 648 state to active
[ns_server:info,2019-03-13T10:24:57.472Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 647 state to active
[ns_server:info,2019-03-13T10:24:57.482Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 646 state to active
[ns_server:info,2019-03-13T10:24:57.484Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 645 state to active
[ns_server:info,2019-03-13T10:24:57.485Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 644 state to active
[ns_server:info,2019-03-13T10:24:57.486Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 643 state to active
[ns_server:info,2019-03-13T10:24:57.487Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 642 state to active
[ns_server:info,2019-03-13T10:24:57.489Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 641 state to active
[ns_server:info,2019-03-13T10:24:57.490Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 640 state to active
[ns_server:info,2019-03-13T10:24:57.491Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 639 state to active
[ns_server:info,2019-03-13T10:24:57.492Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 638 state to active
[ns_server:info,2019-03-13T10:24:57.494Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 637 state to active
[ns_server:info,2019-03-13T10:24:57.496Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 636 state to active
[ns_server:info,2019-03-13T10:24:57.497Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 635 state to active
[ns_server:info,2019-03-13T10:24:57.498Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 634 state to active
[ns_server:info,2019-03-13T10:24:57.499Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 633 state to active
[ns_server:info,2019-03-13T10:24:57.501Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 632 state to active
[ns_server:info,2019-03-13T10:24:57.502Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 631 state to active
[ns_server:info,2019-03-13T10:24:57.504Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 630 state to active
[ns_server:info,2019-03-13T10:24:57.505Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 629 state to active
[ns_server:info,2019-03-13T10:24:57.506Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 628 state to active
[ns_server:info,2019-03-13T10:24:57.507Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 627 state to active
[ns_server:info,2019-03-13T10:24:57.508Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 626 state to active
[ns_server:info,2019-03-13T10:24:57.510Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 625 state to active
[ns_server:info,2019-03-13T10:24:57.512Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 624 state to active
[ns_server:info,2019-03-13T10:24:57.514Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 623 state to active
[ns_server:info,2019-03-13T10:24:57.515Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 622 state to active
[ns_server:info,2019-03-13T10:24:57.516Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 621 state to active
[ns_server:info,2019-03-13T10:24:57.517Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 620 state to active
[ns_server:info,2019-03-13T10:24:57.519Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 619 state to active
[ns_server:info,2019-03-13T10:24:57.520Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 618 state to active
[ns_server:info,2019-03-13T10:24:57.520Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 617 state to active
[ns_server:info,2019-03-13T10:24:57.521Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 616 state to active
[ns_server:info,2019-03-13T10:24:57.523Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 615 state to active
[ns_server:info,2019-03-13T10:24:57.524Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 614 state to active
[ns_server:info,2019-03-13T10:24:57.525Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 613 state to active
[ns_server:info,2019-03-13T10:24:57.526Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 612 state to active
[ns_server:info,2019-03-13T10:24:57.527Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 611 state to active
[ns_server:info,2019-03-13T10:24:57.529Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 610 state to active
[ns_server:info,2019-03-13T10:24:57.529Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 609 state to active
[ns_server:info,2019-03-13T10:24:57.531Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 608 state to active
[ns_server:info,2019-03-13T10:24:57.533Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 607 state to active
[ns_server:info,2019-03-13T10:24:57.535Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 606 state to active
[ns_server:info,2019-03-13T10:24:57.537Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 605 state to active
[ns_server:info,2019-03-13T10:24:57.538Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 604 state to active
[ns_server:info,2019-03-13T10:24:57.541Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 603 state to active
[ns_server:info,2019-03-13T10:24:57.543Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 602 state to active
[ns_server:info,2019-03-13T10:24:57.544Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 601 state to active
[ns_server:info,2019-03-13T10:24:57.545Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 600 state to active
[ns_server:info,2019-03-13T10:24:57.546Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 599 state to active
[ns_server:info,2019-03-13T10:24:57.547Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 598 state to active
[ns_server:info,2019-03-13T10:24:57.548Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 597 state to active
[ns_server:info,2019-03-13T10:24:57.549Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 596 state to active
[ns_server:info,2019-03-13T10:24:57.550Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 595 state to active
[ns_server:info,2019-03-13T10:24:57.551Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 594 state to active
[ns_server:info,2019-03-13T10:24:57.553Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 593 state to active
[ns_server:info,2019-03-13T10:24:57.554Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 592 state to active
[ns_server:info,2019-03-13T10:24:57.555Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 591 state to active
[ns_server:info,2019-03-13T10:24:57.557Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 590 state to active
[ns_server:info,2019-03-13T10:24:57.558Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 589 state to active
[ns_server:info,2019-03-13T10:24:57.559Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 588 state to active
[ns_server:info,2019-03-13T10:24:57.561Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 587 state to active
[ns_server:info,2019-03-13T10:24:57.562Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 586 state to active
[ns_server:info,2019-03-13T10:24:57.564Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 585 state to active
[ns_server:info,2019-03-13T10:24:57.565Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 584 state to active
[ns_server:info,2019-03-13T10:24:57.566Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 583 state to active
[ns_server:info,2019-03-13T10:24:57.568Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 582 state to active
[ns_server:info,2019-03-13T10:24:57.570Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 581 state to active
[ns_server:info,2019-03-13T10:24:57.572Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 580 state to active
[ns_server:info,2019-03-13T10:24:57.574Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 579 state to active
[ns_server:info,2019-03-13T10:24:57.575Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 578 state to active
[ns_server:info,2019-03-13T10:24:57.576Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 577 state to active
[ns_server:info,2019-03-13T10:24:57.582Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 576 state to active
[ns_server:info,2019-03-13T10:24:57.585Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 575 state to active
[ns_server:info,2019-03-13T10:24:57.586Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 574 state to active
[ns_server:info,2019-03-13T10:24:57.588Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 573 state to active
[ns_server:info,2019-03-13T10:24:57.589Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 572 state to active
[ns_server:info,2019-03-13T10:24:57.591Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 571 state to active
[ns_server:info,2019-03-13T10:24:57.591Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 570 state to active
[ns_server:info,2019-03-13T10:24:57.593Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 569 state to active
[ns_server:info,2019-03-13T10:24:57.594Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 568 state to active
[ns_server:info,2019-03-13T10:24:57.596Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 567 state to active
[ns_server:info,2019-03-13T10:24:57.598Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 566 state to active
[ns_server:info,2019-03-13T10:24:57.600Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 565 state to active
[ns_server:info,2019-03-13T10:24:57.601Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 564 state to active
[ns_server:info,2019-03-13T10:24:57.603Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 563 state to active
[ns_server:info,2019-03-13T10:24:57.604Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 562 state to active
[ns_server:info,2019-03-13T10:24:57.606Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 561 state to active
[ns_server:info,2019-03-13T10:24:57.609Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 560 state to active
[ns_server:info,2019-03-13T10:24:57.614Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 559 state to active
[ns_server:info,2019-03-13T10:24:57.618Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 558 state to active
[ns_server:info,2019-03-13T10:24:57.621Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 557 state to active
[ns_server:info,2019-03-13T10:24:57.624Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 556 state to active
[ns_server:info,2019-03-13T10:24:57.636Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 555 state to active
[ns_server:info,2019-03-13T10:24:57.637Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 554 state to active
[ns_server:info,2019-03-13T10:24:57.638Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 553 state to active
[ns_server:info,2019-03-13T10:24:57.638Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 552 state to active
[ns_server:info,2019-03-13T10:24:57.639Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 551 state to active
[ns_server:info,2019-03-13T10:24:57.640Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 550 state to active
[ns_server:info,2019-03-13T10:24:57.641Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 549 state to active
[ns_server:info,2019-03-13T10:24:57.644Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 548 state to active
[ns_server:info,2019-03-13T10:24:57.645Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 547 state to active
[ns_server:info,2019-03-13T10:24:57.647Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 546 state to active
[ns_server:info,2019-03-13T10:24:57.649Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 545 state to active
[ns_server:info,2019-03-13T10:24:57.651Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 544 state to active
[ns_server:info,2019-03-13T10:24:57.652Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 543 state to active
[ns_server:info,2019-03-13T10:24:57.653Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 542 state to active
[ns_server:info,2019-03-13T10:24:57.654Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 541 state to active
[ns_server:info,2019-03-13T10:24:57.655Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 540 state to active
[ns_server:info,2019-03-13T10:24:57.656Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 539 state to active
[ns_server:info,2019-03-13T10:24:57.658Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 538 state to active
[ns_server:info,2019-03-13T10:24:57.659Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 537 state to active
[ns_server:info,2019-03-13T10:24:57.660Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 536 state to active
[ns_server:info,2019-03-13T10:24:57.661Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 535 state to active
[ns_server:info,2019-03-13T10:24:57.662Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 534 state to active
[ns_server:info,2019-03-13T10:24:57.664Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 533 state to active
[ns_server:info,2019-03-13T10:24:57.666Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 532 state to active
[ns_server:info,2019-03-13T10:24:57.668Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 531 state to active
[ns_server:info,2019-03-13T10:24:57.670Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 530 state to active
[ns_server:info,2019-03-13T10:24:57.671Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 529 state to active
[ns_server:info,2019-03-13T10:24:57.674Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 528 state to active
[ns_server:info,2019-03-13T10:24:57.675Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 527 state to active
[ns_server:info,2019-03-13T10:24:57.676Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 526 state to active
[ns_server:info,2019-03-13T10:24:57.678Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 525 state to active
[ns_server:info,2019-03-13T10:24:57.679Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 524 state to active
[ns_server:info,2019-03-13T10:24:57.680Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 523 state to active
[ns_server:info,2019-03-13T10:24:57.684Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 522 state to active
[ns_server:info,2019-03-13T10:24:57.685Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 521 state to active
[ns_server:info,2019-03-13T10:24:57.691Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 520 state to active
[ns_server:info,2019-03-13T10:24:57.693Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 519 state to active
[ns_server:info,2019-03-13T10:24:57.694Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 518 state to active
[ns_server:info,2019-03-13T10:24:57.696Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 517 state to active
[ns_server:info,2019-03-13T10:24:57.709Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 516 state to active
[ns_server:info,2019-03-13T10:24:57.713Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 515 state to active
[ns_server:info,2019-03-13T10:24:57.716Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 514 state to active
[ns_server:info,2019-03-13T10:24:57.721Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 513 state to active
[ns_server:info,2019-03-13T10:24:57.727Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 512 state to active
[ns_server:info,2019-03-13T10:24:57.729Z,ns_1@127.0.0.1:<0.937.0>:ns_memcached:do_handle_call:564]Changed vbucket 511 state to active
[ns_server:info,2019-03-13T10:24:57.731Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 510 state to active
[ns_server:info,2019-03-13T10:24:57.734Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 509 state to active
[ns_server:info,2019-03-13T10:24:57.736Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 508 state to active
[ns_server:info,2019-03-13T10:24:57.743Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 507 state to active
[ns_server:info,2019-03-13T10:24:57.745Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 506 state to active
[ns_server:info,2019-03-13T10:24:57.747Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 505 state to active
[ns_server:info,2019-03-13T10:24:57.748Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 504 state to active
[ns_server:info,2019-03-13T10:24:57.751Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 503 state to active
[ns_server:info,2019-03-13T10:24:57.753Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 502 state to active
[ns_server:info,2019-03-13T10:24:57.755Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 501 state to active
[ns_server:info,2019-03-13T10:24:57.757Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 500 state to active
[ns_server:info,2019-03-13T10:24:57.759Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 499 state to active
[ns_server:info,2019-03-13T10:24:57.765Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 498 state to active
[ns_server:info,2019-03-13T10:24:57.770Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 497 state to active
[ns_server:info,2019-03-13T10:24:57.778Z,ns_1@127.0.0.1:<0.938.0>:ns_memcached:do_handle_call:564]Changed vbucket 496 state to active
[ns_server:info,2019-03-13T10:24:57.779Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 495 state to active
[ns_server:info,2019-03-13T10:24:57.781Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 494 state to active
[ns_server:info,2019-03-13T10:24:57.783Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 493 state to active
[ns_server:info,2019-03-13T10:24:57.787Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 492 state to active
[ns_server:info,2019-03-13T10:24:57.788Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 491 state to active
[ns_server:info,2019-03-13T10:24:57.790Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 490 state to active
[ns_server:info,2019-03-13T10:24:57.792Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 489 state to active
[ns_server:info,2019-03-13T10:24:57.793Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 488 state to active
[ns_server:info,2019-03-13T10:24:57.796Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 487 state to active
[ns_server:info,2019-03-13T10:24:57.798Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 486 state to active
[ns_server:info,2019-03-13T10:24:57.800Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 485 state to active
[ns_server:info,2019-03-13T10:24:57.802Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 484 state to active
[ns_server:info,2019-03-13T10:24:57.803Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 483 state to active
[ns_server:info,2019-03-13T10:24:57.804Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 482 state to active
[ns_server:info,2019-03-13T10:24:57.806Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 481 state to active
[ns_server:info,2019-03-13T10:24:57.807Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 480 state to active
[ns_server:info,2019-03-13T10:24:57.808Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 479 state to active
[ns_server:info,2019-03-13T10:24:57.809Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 478 state to active
[ns_server:info,2019-03-13T10:24:57.811Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 477 state to active
[ns_server:info,2019-03-13T10:24:57.813Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 476 state to active
[ns_server:info,2019-03-13T10:24:57.815Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 475 state to active
[ns_server:info,2019-03-13T10:24:57.823Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 474 state to active
[ns_server:info,2019-03-13T10:24:57.825Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 473 state to active
[ns_server:info,2019-03-13T10:24:57.828Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 472 state to active
[ns_server:info,2019-03-13T10:24:57.829Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 471 state to active
[ns_server:info,2019-03-13T10:24:57.831Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 470 state to active
[ns_server:info,2019-03-13T10:24:57.833Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 469 state to active
[ns_server:info,2019-03-13T10:24:57.835Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 468 state to active
[ns_server:info,2019-03-13T10:24:57.836Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 467 state to active
[ns_server:info,2019-03-13T10:24:57.838Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 466 state to active
[ns_server:info,2019-03-13T10:24:57.849Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 465 state to active
[ns_server:info,2019-03-13T10:24:57.850Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 464 state to active
[ns_server:info,2019-03-13T10:24:57.851Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 463 state to active
[ns_server:info,2019-03-13T10:24:57.853Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 462 state to active
[ns_server:info,2019-03-13T10:24:57.855Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 461 state to active
[ns_server:info,2019-03-13T10:24:57.857Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 460 state to active
[ns_server:info,2019-03-13T10:24:57.858Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 459 state to active
[ns_server:info,2019-03-13T10:24:57.860Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 458 state to active
[ns_server:info,2019-03-13T10:24:57.861Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 457 state to active
[ns_server:info,2019-03-13T10:24:57.862Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 456 state to active
[ns_server:info,2019-03-13T10:24:57.870Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 455 state to active
[ns_server:info,2019-03-13T10:24:57.872Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 454 state to active
[ns_server:info,2019-03-13T10:24:57.874Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 453 state to active
[ns_server:info,2019-03-13T10:24:57.875Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 452 state to active
[ns_server:info,2019-03-13T10:24:57.882Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 451 state to active
[ns_server:info,2019-03-13T10:24:57.883Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 450 state to active
[ns_server:info,2019-03-13T10:24:57.887Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 449 state to active
[ns_server:info,2019-03-13T10:24:57.889Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 448 state to active
[ns_server:info,2019-03-13T10:24:57.890Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 447 state to active
[ns_server:info,2019-03-13T10:24:57.892Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 446 state to active
[ns_server:info,2019-03-13T10:24:57.894Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 445 state to active
[ns_server:info,2019-03-13T10:24:57.896Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 444 state to active
[ns_server:info,2019-03-13T10:24:57.898Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 443 state to active
[ns_server:info,2019-03-13T10:24:57.901Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 442 state to active
[ns_server:info,2019-03-13T10:24:57.902Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 441 state to active
[ns_server:info,2019-03-13T10:24:57.905Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 440 state to active
[ns_server:info,2019-03-13T10:24:57.907Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 439 state to active
[ns_server:info,2019-03-13T10:24:57.909Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 438 state to active
[ns_server:info,2019-03-13T10:24:57.910Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 437 state to active
[ns_server:info,2019-03-13T10:24:57.911Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 436 state to active
[ns_server:info,2019-03-13T10:24:57.912Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 435 state to active
[ns_server:info,2019-03-13T10:24:57.915Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 434 state to active
[ns_server:info,2019-03-13T10:24:57.917Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 433 state to active
[ns_server:info,2019-03-13T10:24:57.918Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 432 state to active
[ns_server:info,2019-03-13T10:24:57.920Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 431 state to active
[ns_server:info,2019-03-13T10:24:57.921Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 430 state to active
[ns_server:info,2019-03-13T10:24:57.922Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 429 state to active
[ns_server:info,2019-03-13T10:24:57.924Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 428 state to active
[ns_server:info,2019-03-13T10:24:57.925Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 427 state to active
[ns_server:info,2019-03-13T10:24:57.926Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 426 state to active
[ns_server:info,2019-03-13T10:24:57.928Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 425 state to active
[ns_server:info,2019-03-13T10:24:57.929Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 424 state to active
[ns_server:info,2019-03-13T10:24:57.930Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 423 state to active
[ns_server:info,2019-03-13T10:24:57.932Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 422 state to active
[ns_server:info,2019-03-13T10:24:57.934Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 421 state to active
[ns_server:debug,2019-03-13T10:24:57.934Z,ns_1@127.0.0.1:<0.481.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
            0,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
              1,half_down,false}
[ns_server:info,2019-03-13T10:24:57.935Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 420 state to active
[ns_server:info,2019-03-13T10:24:57.937Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 419 state to active
[ns_server:info,2019-03-13T10:24:57.938Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 418 state to active
[ns_server:info,2019-03-13T10:24:57.939Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 417 state to active
[ns_server:info,2019-03-13T10:24:57.941Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 416 state to active
[ns_server:info,2019-03-13T10:24:57.942Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 415 state to active
[ns_server:info,2019-03-13T10:24:57.943Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 414 state to active
[ns_server:info,2019-03-13T10:24:57.945Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 413 state to active
[ns_server:info,2019-03-13T10:24:57.947Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 412 state to active
[ns_server:info,2019-03-13T10:24:57.949Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 411 state to active
[ns_server:info,2019-03-13T10:24:57.950Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 410 state to active
[ns_server:info,2019-03-13T10:24:57.962Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 409 state to active
[ns_server:info,2019-03-13T10:24:57.965Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 408 state to active
[ns_server:info,2019-03-13T10:24:57.969Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 407 state to active
[ns_server:info,2019-03-13T10:24:57.972Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 406 state to active
[ns_server:info,2019-03-13T10:24:57.975Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 405 state to active
[ns_server:info,2019-03-13T10:24:57.978Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 404 state to active
[ns_server:info,2019-03-13T10:24:57.981Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 403 state to active
[ns_server:info,2019-03-13T10:24:57.984Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 402 state to active
[ns_server:info,2019-03-13T10:24:57.986Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 401 state to active
[ns_server:info,2019-03-13T10:24:57.988Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 400 state to active
[ns_server:info,2019-03-13T10:24:57.989Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 399 state to active
[ns_server:info,2019-03-13T10:24:57.990Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 398 state to active
[ns_server:info,2019-03-13T10:24:58.002Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 397 state to active
[ns_server:info,2019-03-13T10:24:58.006Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 396 state to active
[ns_server:info,2019-03-13T10:24:58.011Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 395 state to active
[ns_server:info,2019-03-13T10:24:58.015Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 394 state to active
[ns_server:info,2019-03-13T10:24:58.019Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 393 state to active
[ns_server:info,2019-03-13T10:24:58.022Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 392 state to active
[ns_server:info,2019-03-13T10:24:58.023Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 391 state to active
[ns_server:info,2019-03-13T10:24:58.026Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 390 state to active
[ns_server:info,2019-03-13T10:24:58.027Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 389 state to active
[ns_server:info,2019-03-13T10:24:58.029Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 388 state to active
[ns_server:info,2019-03-13T10:24:58.031Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 387 state to active
[ns_server:info,2019-03-13T10:24:58.033Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 386 state to active
[ns_server:info,2019-03-13T10:24:58.036Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 385 state to active
[ns_server:info,2019-03-13T10:24:58.038Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 384 state to active
[ns_server:info,2019-03-13T10:24:58.041Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 383 state to active
[ns_server:info,2019-03-13T10:24:58.043Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 382 state to active
[ns_server:info,2019-03-13T10:24:58.046Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 381 state to active
[ns_server:info,2019-03-13T10:24:58.050Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 380 state to active
[ns_server:info,2019-03-13T10:24:58.053Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 379 state to active
[ns_server:info,2019-03-13T10:24:58.055Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 378 state to active
[ns_server:info,2019-03-13T10:24:58.056Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 377 state to active
[ns_server:info,2019-03-13T10:24:58.058Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 376 state to active
[ns_server:info,2019-03-13T10:24:58.060Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 375 state to active
[ns_server:info,2019-03-13T10:24:58.064Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 374 state to active
[ns_server:info,2019-03-13T10:24:58.066Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 373 state to active
[ns_server:info,2019-03-13T10:24:58.069Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 372 state to active
[ns_server:info,2019-03-13T10:24:58.071Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 371 state to active
[ns_server:info,2019-03-13T10:24:58.075Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 370 state to active
[ns_server:info,2019-03-13T10:24:58.079Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 369 state to active
[ns_server:info,2019-03-13T10:24:58.084Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 368 state to active
[ns_server:info,2019-03-13T10:24:58.086Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 367 state to active
[ns_server:info,2019-03-13T10:24:58.090Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 366 state to active
[ns_server:info,2019-03-13T10:24:58.093Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 365 state to active
[ns_server:info,2019-03-13T10:24:58.096Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 364 state to active
[ns_server:info,2019-03-13T10:24:58.097Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 363 state to active
[ns_server:info,2019-03-13T10:24:58.099Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 362 state to active
[ns_server:info,2019-03-13T10:24:58.100Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 361 state to active
[ns_server:info,2019-03-13T10:24:58.103Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 360 state to active
[ns_server:info,2019-03-13T10:24:58.107Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 359 state to active
[ns_server:info,2019-03-13T10:24:58.109Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 358 state to active
[ns_server:info,2019-03-13T10:24:58.112Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 357 state to active
[ns_server:info,2019-03-13T10:24:58.115Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 356 state to active
[ns_server:info,2019-03-13T10:24:58.118Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 355 state to active
[ns_server:info,2019-03-13T10:24:58.122Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 354 state to active
[ns_server:info,2019-03-13T10:24:58.124Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 353 state to active
[ns_server:info,2019-03-13T10:24:58.127Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 352 state to active
[ns_server:info,2019-03-13T10:24:58.131Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 351 state to active
[ns_server:info,2019-03-13T10:24:58.134Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 350 state to active
[ns_server:info,2019-03-13T10:24:58.136Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 349 state to active
[ns_server:info,2019-03-13T10:24:58.138Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 348 state to active
[ns_server:info,2019-03-13T10:24:58.141Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 347 state to active
[ns_server:info,2019-03-13T10:24:58.142Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 346 state to active
[ns_server:info,2019-03-13T10:24:58.144Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 345 state to active
[ns_server:info,2019-03-13T10:24:58.121Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 344 state to active
[ns_server:info,2019-03-13T10:24:58.123Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 343 state to active
[ns_server:info,2019-03-13T10:24:58.125Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 342 state to active
[ns_server:info,2019-03-13T10:24:58.128Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 341 state to active
[ns_server:info,2019-03-13T10:24:58.131Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 340 state to active
[ns_server:info,2019-03-13T10:24:58.133Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 339 state to active
[ns_server:info,2019-03-13T10:24:58.135Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 338 state to active
[ns_server:info,2019-03-13T10:24:58.136Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 337 state to active
[ns_server:info,2019-03-13T10:24:58.138Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 336 state to active
[ns_server:info,2019-03-13T10:24:58.140Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 335 state to active
[ns_server:info,2019-03-13T10:24:58.142Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 334 state to active
[ns_server:info,2019-03-13T10:24:58.144Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 333 state to active
[ns_server:info,2019-03-13T10:24:58.148Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 332 state to active
[ns_server:info,2019-03-13T10:24:58.151Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 331 state to active
[ns_server:info,2019-03-13T10:24:58.155Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 330 state to active
[ns_server:info,2019-03-13T10:24:58.157Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 329 state to active
[ns_server:info,2019-03-13T10:24:58.159Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 328 state to active
[ns_server:info,2019-03-13T10:24:58.161Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 327 state to active
[ns_server:info,2019-03-13T10:24:58.163Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 326 state to active
[ns_server:info,2019-03-13T10:24:58.165Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 325 state to active
[ns_server:info,2019-03-13T10:24:58.167Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 324 state to active
[ns_server:info,2019-03-13T10:24:58.169Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 323 state to active
[ns_server:info,2019-03-13T10:24:58.172Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 322 state to active
[ns_server:info,2019-03-13T10:24:58.174Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 321 state to active
[ns_server:info,2019-03-13T10:24:58.177Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 320 state to active
[ns_server:info,2019-03-13T10:24:58.181Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 319 state to active
[ns_server:info,2019-03-13T10:24:58.182Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 318 state to active
[ns_server:info,2019-03-13T10:24:58.184Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 317 state to active
[ns_server:info,2019-03-13T10:24:58.187Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 316 state to active
[ns_server:info,2019-03-13T10:24:58.189Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 315 state to active
[ns_server:info,2019-03-13T10:24:58.190Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 314 state to active
[ns_server:info,2019-03-13T10:24:58.192Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 313 state to active
[ns_server:info,2019-03-13T10:24:58.193Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 312 state to active
[ns_server:info,2019-03-13T10:24:58.194Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 311 state to active
[ns_server:info,2019-03-13T10:24:58.196Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 310 state to active
[ns_server:info,2019-03-13T10:24:58.199Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 309 state to active
[ns_server:info,2019-03-13T10:24:58.200Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 308 state to active
[ns_server:info,2019-03-13T10:24:58.201Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 307 state to active
[ns_server:info,2019-03-13T10:24:58.203Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 306 state to active
[ns_server:info,2019-03-13T10:24:58.204Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 305 state to active
[ns_server:info,2019-03-13T10:24:58.206Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 304 state to active
[ns_server:info,2019-03-13T10:24:58.208Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 303 state to active
[ns_server:info,2019-03-13T10:24:58.211Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 302 state to active
[ns_server:info,2019-03-13T10:24:58.213Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 301 state to active
[ns_server:info,2019-03-13T10:24:58.215Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 300 state to active
[ns_server:info,2019-03-13T10:24:58.217Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 299 state to active
[ns_server:info,2019-03-13T10:24:58.219Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 298 state to active
[ns_server:info,2019-03-13T10:24:58.222Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 297 state to active
[ns_server:info,2019-03-13T10:24:58.224Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 296 state to active
[ns_server:info,2019-03-13T10:24:58.226Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 295 state to active
[ns_server:info,2019-03-13T10:24:58.229Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 294 state to active
[ns_server:info,2019-03-13T10:24:58.232Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 293 state to active
[ns_server:info,2019-03-13T10:24:58.233Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 292 state to active
[ns_server:info,2019-03-13T10:24:58.234Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 291 state to active
[ns_server:info,2019-03-13T10:24:58.237Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 290 state to active
[ns_server:info,2019-03-13T10:24:58.240Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 289 state to active
[ns_server:info,2019-03-13T10:24:58.242Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 288 state to active
[ns_server:info,2019-03-13T10:24:58.243Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 287 state to active
[ns_server:info,2019-03-13T10:24:58.245Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 286 state to active
[ns_server:info,2019-03-13T10:24:58.247Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 285 state to active
[ns_server:info,2019-03-13T10:24:58.255Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 284 state to active
[ns_server:info,2019-03-13T10:24:58.258Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 283 state to active
[ns_server:info,2019-03-13T10:24:58.260Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 282 state to active
[ns_server:info,2019-03-13T10:24:58.262Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 281 state to active
[ns_server:info,2019-03-13T10:24:58.264Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 280 state to active
[ns_server:info,2019-03-13T10:24:58.266Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 279 state to active
[ns_server:info,2019-03-13T10:24:58.270Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 278 state to active
[ns_server:info,2019-03-13T10:24:58.272Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 277 state to active
[ns_server:info,2019-03-13T10:24:58.274Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 276 state to active
[ns_server:info,2019-03-13T10:24:58.276Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 275 state to active
[ns_server:info,2019-03-13T10:24:58.281Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 274 state to active
[ns_server:info,2019-03-13T10:24:58.283Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 273 state to active
[ns_server:info,2019-03-13T10:24:58.286Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 272 state to active
[ns_server:info,2019-03-13T10:24:58.289Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 271 state to active
[ns_server:info,2019-03-13T10:24:58.290Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 270 state to active
[ns_server:info,2019-03-13T10:24:58.293Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 269 state to active
[ns_server:info,2019-03-13T10:24:58.296Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 268 state to active
[ns_server:info,2019-03-13T10:24:58.300Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 267 state to active
[ns_server:info,2019-03-13T10:24:58.303Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 266 state to active
[ns_server:info,2019-03-13T10:24:58.307Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 265 state to active
[ns_server:info,2019-03-13T10:24:58.310Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 264 state to active
[ns_server:info,2019-03-13T10:24:58.312Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 263 state to active
[ns_server:info,2019-03-13T10:24:58.317Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 262 state to active
[ns_server:info,2019-03-13T10:24:58.318Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 261 state to active
[ns_server:info,2019-03-13T10:24:58.320Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 260 state to active
[ns_server:info,2019-03-13T10:24:58.324Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 259 state to active
[ns_server:info,2019-03-13T10:24:58.327Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 258 state to active
[ns_server:info,2019-03-13T10:24:58.329Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 257 state to active
[ns_server:info,2019-03-13T10:24:58.332Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 256 state to active
[ns_server:info,2019-03-13T10:24:58.333Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 255 state to active
[ns_server:info,2019-03-13T10:24:58.336Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 254 state to active
[ns_server:info,2019-03-13T10:24:58.355Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 253 state to active
[ns_server:info,2019-03-13T10:24:58.365Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 252 state to active
[ns_server:info,2019-03-13T10:24:58.373Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 251 state to active
[ns_server:info,2019-03-13T10:24:58.378Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 250 state to active
[ns_server:info,2019-03-13T10:24:58.381Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 249 state to active
[ns_server:info,2019-03-13T10:24:58.382Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 248 state to active
[ns_server:info,2019-03-13T10:24:58.384Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 247 state to active
[ns_server:info,2019-03-13T10:24:58.385Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 246 state to active
[ns_server:info,2019-03-13T10:24:58.387Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 245 state to active
[ns_server:warn,2019-03-13T10:24:58.387Z,ns_1@127.0.0.1:kv_monitor<0.718.0>:kv_monitor:get_buckets:275]The following buckets are not ready: ["main-bucket"]
[ns_server:info,2019-03-13T10:24:58.389Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 244 state to active
[ns_server:info,2019-03-13T10:24:58.392Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 243 state to active
[ns_server:info,2019-03-13T10:24:58.393Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 242 state to active
[ns_server:info,2019-03-13T10:24:58.395Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 241 state to active
[ns_server:info,2019-03-13T10:24:58.397Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 240 state to active
[ns_server:info,2019-03-13T10:24:58.399Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 239 state to active
[ns_server:info,2019-03-13T10:24:58.401Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 238 state to active
[ns_server:info,2019-03-13T10:24:58.404Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 237 state to active
[ns_server:info,2019-03-13T10:24:58.406Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 236 state to active
[ns_server:info,2019-03-13T10:24:58.408Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 235 state to active
[ns_server:info,2019-03-13T10:24:58.409Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 234 state to active
[ns_server:info,2019-03-13T10:24:58.411Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 233 state to active
[ns_server:info,2019-03-13T10:24:58.413Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 232 state to active
[ns_server:info,2019-03-13T10:24:58.416Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 231 state to active
[ns_server:info,2019-03-13T10:24:58.419Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 230 state to active
[ns_server:info,2019-03-13T10:24:58.421Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 229 state to active
[ns_server:info,2019-03-13T10:24:58.423Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 228 state to active
[ns_server:info,2019-03-13T10:24:58.425Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 227 state to active
[ns_server:info,2019-03-13T10:24:58.426Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 226 state to active
[ns_server:info,2019-03-13T10:24:58.427Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 225 state to active
[ns_server:info,2019-03-13T10:24:58.429Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 224 state to active
[ns_server:info,2019-03-13T10:24:58.432Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 223 state to active
[ns_server:info,2019-03-13T10:24:58.435Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 222 state to active
[ns_server:info,2019-03-13T10:24:58.437Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 221 state to active
[ns_server:info,2019-03-13T10:24:58.439Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 220 state to active
[ns_server:info,2019-03-13T10:24:58.441Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 219 state to active
[ns_server:info,2019-03-13T10:24:58.443Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 218 state to active
[ns_server:info,2019-03-13T10:24:58.445Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 217 state to active
[ns_server:info,2019-03-13T10:24:58.447Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 216 state to active
[ns_server:info,2019-03-13T10:24:58.449Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 215 state to active
[ns_server:info,2019-03-13T10:24:58.450Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 214 state to active
[ns_server:info,2019-03-13T10:24:58.452Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 213 state to active
[ns_server:info,2019-03-13T10:24:58.454Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 212 state to active
[ns_server:info,2019-03-13T10:24:58.456Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 211 state to active
[ns_server:info,2019-03-13T10:24:58.457Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 210 state to active
[ns_server:info,2019-03-13T10:24:58.458Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 209 state to active
[ns_server:info,2019-03-13T10:24:58.459Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 208 state to active
[ns_server:info,2019-03-13T10:24:58.460Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 207 state to active
[ns_server:info,2019-03-13T10:24:58.461Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 206 state to active
[ns_server:info,2019-03-13T10:24:58.462Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 205 state to active
[ns_server:info,2019-03-13T10:24:58.464Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 204 state to active
[ns_server:info,2019-03-13T10:24:58.467Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 203 state to active
[ns_server:info,2019-03-13T10:24:58.469Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 202 state to active
[ns_server:info,2019-03-13T10:24:58.471Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 201 state to active
[ns_server:info,2019-03-13T10:24:58.473Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 200 state to active
[ns_server:info,2019-03-13T10:24:58.474Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 199 state to active
[ns_server:info,2019-03-13T10:24:58.476Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 198 state to active
[ns_server:info,2019-03-13T10:24:58.477Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 197 state to active
[ns_server:info,2019-03-13T10:24:58.479Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 196 state to active
[ns_server:info,2019-03-13T10:24:58.482Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 195 state to active
[ns_server:info,2019-03-13T10:24:58.484Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 194 state to active
[ns_server:info,2019-03-13T10:24:58.485Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 193 state to active
[ns_server:info,2019-03-13T10:24:58.487Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 192 state to active
[ns_server:info,2019-03-13T10:24:58.489Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 191 state to active
[ns_server:info,2019-03-13T10:24:58.492Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 190 state to active
[ns_server:info,2019-03-13T10:24:58.494Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 189 state to active
[ns_server:info,2019-03-13T10:24:58.496Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 188 state to active
[ns_server:info,2019-03-13T10:24:58.499Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 187 state to active
[ns_server:info,2019-03-13T10:24:58.500Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 186 state to active
[ns_server:info,2019-03-13T10:24:58.503Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 185 state to active
[ns_server:info,2019-03-13T10:24:58.504Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 184 state to active
[ns_server:info,2019-03-13T10:24:58.506Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 183 state to active
[ns_server:info,2019-03-13T10:24:58.508Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 182 state to active
[ns_server:info,2019-03-13T10:24:58.510Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 181 state to active
[ns_server:info,2019-03-13T10:24:58.512Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 180 state to active
[ns_server:info,2019-03-13T10:24:58.513Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 179 state to active
[ns_server:info,2019-03-13T10:24:58.515Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 178 state to active
[ns_server:info,2019-03-13T10:24:58.517Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 177 state to active
[ns_server:info,2019-03-13T10:24:58.519Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 176 state to active
[ns_server:info,2019-03-13T10:24:58.523Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 175 state to active
[ns_server:info,2019-03-13T10:24:58.525Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 174 state to active
[ns_server:info,2019-03-13T10:24:58.526Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 173 state to active
[ns_server:info,2019-03-13T10:24:58.528Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 172 state to active
[ns_server:info,2019-03-13T10:24:58.530Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 171 state to active
[ns_server:info,2019-03-13T10:24:58.533Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 170 state to active
[ns_server:info,2019-03-13T10:24:58.534Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 169 state to active
[ns_server:info,2019-03-13T10:24:58.535Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 168 state to active
[ns_server:info,2019-03-13T10:24:58.536Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 167 state to active
[ns_server:info,2019-03-13T10:24:58.538Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 166 state to active
[ns_server:info,2019-03-13T10:24:58.540Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 165 state to active
[ns_server:info,2019-03-13T10:24:58.541Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 164 state to active
[ns_server:info,2019-03-13T10:24:58.543Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 163 state to active
[ns_server:info,2019-03-13T10:24:58.545Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 162 state to active
[ns_server:info,2019-03-13T10:24:58.547Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 161 state to active
[ns_server:info,2019-03-13T10:24:58.548Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 160 state to active
[ns_server:info,2019-03-13T10:24:58.550Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 159 state to active
[ns_server:info,2019-03-13T10:24:58.551Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 158 state to active
[ns_server:info,2019-03-13T10:24:58.553Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 157 state to active
[ns_server:info,2019-03-13T10:24:58.556Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 156 state to active
[ns_server:info,2019-03-13T10:24:58.558Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 155 state to active
[ns_server:info,2019-03-13T10:24:58.559Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 154 state to active
[ns_server:info,2019-03-13T10:24:58.561Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 153 state to active
[ns_server:info,2019-03-13T10:24:58.563Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 152 state to active
[ns_server:info,2019-03-13T10:24:58.566Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 151 state to active
[ns_server:info,2019-03-13T10:24:58.568Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 150 state to active
[ns_server:info,2019-03-13T10:24:58.571Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 149 state to active
[ns_server:info,2019-03-13T10:24:58.573Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 148 state to active
[ns_server:info,2019-03-13T10:24:58.576Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 147 state to active
[ns_server:info,2019-03-13T10:24:58.581Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 146 state to active
[ns_server:info,2019-03-13T10:24:58.584Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 145 state to active
[ns_server:info,2019-03-13T10:24:58.586Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 144 state to active
[ns_server:info,2019-03-13T10:24:58.588Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 143 state to active
[ns_server:info,2019-03-13T10:24:58.592Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 142 state to active
[ns_server:info,2019-03-13T10:24:58.595Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 141 state to active
[ns_server:info,2019-03-13T10:24:58.597Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 140 state to active
[ns_server:info,2019-03-13T10:24:58.598Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 139 state to active
[ns_server:info,2019-03-13T10:24:58.600Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 138 state to active
[ns_server:info,2019-03-13T10:24:58.604Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 137 state to active
[ns_server:info,2019-03-13T10:24:58.607Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 136 state to active
[ns_server:info,2019-03-13T10:24:58.608Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 135 state to active
[ns_server:info,2019-03-13T10:24:58.611Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 134 state to active
[ns_server:info,2019-03-13T10:24:58.614Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 133 state to active
[ns_server:info,2019-03-13T10:24:58.615Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 132 state to active
[ns_server:info,2019-03-13T10:24:58.617Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 131 state to active
[ns_server:info,2019-03-13T10:24:58.619Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 130 state to active
[ns_server:info,2019-03-13T10:24:58.621Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 129 state to active
[ns_server:info,2019-03-13T10:24:58.623Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 128 state to active
[ns_server:info,2019-03-13T10:24:58.627Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 127 state to active
[ns_server:info,2019-03-13T10:24:58.629Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 126 state to active
[ns_server:info,2019-03-13T10:24:58.630Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 125 state to active
[ns_server:info,2019-03-13T10:24:58.634Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 124 state to active
[ns_server:info,2019-03-13T10:24:58.635Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 123 state to active
[ns_server:info,2019-03-13T10:24:58.638Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 122 state to active
[ns_server:info,2019-03-13T10:24:58.640Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 121 state to active
[ns_server:info,2019-03-13T10:24:58.642Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 120 state to active
[ns_server:info,2019-03-13T10:24:58.644Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 119 state to active
[ns_server:info,2019-03-13T10:24:58.646Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 118 state to active
[ns_server:info,2019-03-13T10:24:58.658Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 117 state to active
[ns_server:info,2019-03-13T10:24:58.659Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 116 state to active
[ns_server:info,2019-03-13T10:24:58.661Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 115 state to active
[ns_server:info,2019-03-13T10:24:58.662Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 114 state to active
[ns_server:info,2019-03-13T10:24:58.664Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 113 state to active
[ns_server:info,2019-03-13T10:24:58.665Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 112 state to active
[ns_server:info,2019-03-13T10:24:58.665Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 111 state to active
[ns_server:info,2019-03-13T10:24:58.669Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 110 state to active
[ns_server:info,2019-03-13T10:24:58.671Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 109 state to active
[ns_server:info,2019-03-13T10:24:58.672Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 108 state to active
[ns_server:info,2019-03-13T10:24:58.673Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 107 state to active
[ns_server:info,2019-03-13T10:24:58.674Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 106 state to active
[ns_server:info,2019-03-13T10:24:58.675Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 105 state to active
[ns_server:info,2019-03-13T10:24:58.679Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 104 state to active
[ns_server:info,2019-03-13T10:24:58.683Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 103 state to active
[ns_server:info,2019-03-13T10:24:58.685Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 102 state to active
[ns_server:info,2019-03-13T10:24:58.687Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 101 state to active
[ns_server:info,2019-03-13T10:24:58.688Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 100 state to active
[ns_server:info,2019-03-13T10:24:58.689Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 99 state to active
[ns_server:info,2019-03-13T10:24:58.690Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 98 state to active
[ns_server:info,2019-03-13T10:24:58.693Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 97 state to active
[ns_server:info,2019-03-13T10:24:58.694Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 96 state to active
[ns_server:info,2019-03-13T10:24:58.695Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 95 state to active
[ns_server:info,2019-03-13T10:24:58.698Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 94 state to active
[ns_server:info,2019-03-13T10:24:58.699Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 93 state to active
[ns_server:info,2019-03-13T10:24:58.700Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 92 state to active
[ns_server:info,2019-03-13T10:24:58.702Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 91 state to active
[ns_server:info,2019-03-13T10:24:58.705Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 90 state to active
[ns_server:info,2019-03-13T10:24:58.707Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 89 state to active
[ns_server:info,2019-03-13T10:24:58.709Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 88 state to active
[ns_server:info,2019-03-13T10:24:58.710Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 87 state to active
[ns_server:info,2019-03-13T10:24:58.712Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 86 state to active
[ns_server:info,2019-03-13T10:24:58.718Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 85 state to active
[ns_server:info,2019-03-13T10:24:58.721Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 84 state to active
[ns_server:info,2019-03-13T10:24:58.723Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 83 state to active
[ns_server:info,2019-03-13T10:24:58.725Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 82 state to active
[ns_server:info,2019-03-13T10:24:58.727Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 81 state to active
[ns_server:info,2019-03-13T10:24:58.728Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 80 state to active
[ns_server:info,2019-03-13T10:24:58.731Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 79 state to active
[ns_server:info,2019-03-13T10:24:58.736Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 78 state to active
[ns_server:info,2019-03-13T10:24:58.737Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 77 state to active
[ns_server:info,2019-03-13T10:24:58.739Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 76 state to active
[ns_server:info,2019-03-13T10:24:58.740Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 75 state to active
[ns_server:info,2019-03-13T10:24:58.742Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 74 state to active
[ns_server:info,2019-03-13T10:24:58.743Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 73 state to active
[ns_server:info,2019-03-13T10:24:58.746Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 72 state to active
[ns_server:info,2019-03-13T10:24:58.748Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 71 state to active
[ns_server:info,2019-03-13T10:24:58.749Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 70 state to active
[ns_server:info,2019-03-13T10:24:58.750Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 69 state to active
[ns_server:info,2019-03-13T10:24:58.751Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 68 state to active
[ns_server:info,2019-03-13T10:24:58.753Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 67 state to active
[ns_server:info,2019-03-13T10:24:58.756Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 66 state to active
[ns_server:info,2019-03-13T10:24:58.758Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 65 state to active
[ns_server:info,2019-03-13T10:24:58.760Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 64 state to active
[ns_server:info,2019-03-13T10:24:58.766Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 63 state to active
[ns_server:info,2019-03-13T10:24:58.767Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 62 state to active
[ns_server:info,2019-03-13T10:24:58.769Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 61 state to active
[ns_server:info,2019-03-13T10:24:58.770Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 60 state to active
[ns_server:info,2019-03-13T10:24:58.771Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 59 state to active
[ns_server:info,2019-03-13T10:24:58.772Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 58 state to active
[ns_server:info,2019-03-13T10:24:58.773Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 57 state to active
[ns_server:info,2019-03-13T10:24:58.774Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 56 state to active
[ns_server:info,2019-03-13T10:24:58.776Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 55 state to active
[ns_server:info,2019-03-13T10:24:58.778Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 54 state to active
[ns_server:info,2019-03-13T10:24:58.780Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 53 state to active
[ns_server:info,2019-03-13T10:24:58.781Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 52 state to active
[ns_server:info,2019-03-13T10:24:58.782Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 51 state to active
[ns_server:info,2019-03-13T10:24:58.784Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 50 state to active
[ns_server:info,2019-03-13T10:24:58.785Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 49 state to active
[ns_server:info,2019-03-13T10:24:58.786Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 48 state to active
[ns_server:info,2019-03-13T10:24:58.788Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 47 state to active
[ns_server:info,2019-03-13T10:24:58.789Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 46 state to active
[ns_server:info,2019-03-13T10:24:58.792Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 45 state to active
[ns_server:info,2019-03-13T10:24:58.794Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 44 state to active
[ns_server:info,2019-03-13T10:24:58.795Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 43 state to active
[ns_server:info,2019-03-13T10:24:58.797Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 42 state to active
[ns_server:info,2019-03-13T10:24:58.799Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 41 state to active
[ns_server:info,2019-03-13T10:24:58.800Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 40 state to active
[ns_server:info,2019-03-13T10:24:58.808Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 39 state to active
[ns_server:info,2019-03-13T10:24:58.810Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 38 state to active
[ns_server:info,2019-03-13T10:24:58.812Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 37 state to active
[ns_server:info,2019-03-13T10:24:58.814Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 36 state to active
[ns_server:info,2019-03-13T10:24:58.817Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 35 state to active
[ns_server:info,2019-03-13T10:24:58.818Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 34 state to active
[ns_server:info,2019-03-13T10:24:58.819Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 33 state to active
[ns_server:info,2019-03-13T10:24:58.820Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 32 state to active
[ns_server:info,2019-03-13T10:24:58.822Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 31 state to active
[ns_server:info,2019-03-13T10:24:58.824Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 30 state to active
[ns_server:info,2019-03-13T10:24:58.825Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 29 state to active
[ns_server:info,2019-03-13T10:24:58.827Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 28 state to active
[ns_server:info,2019-03-13T10:24:58.828Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 27 state to active
[ns_server:info,2019-03-13T10:24:58.829Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 26 state to active
[ns_server:info,2019-03-13T10:24:58.830Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 25 state to active
[ns_server:info,2019-03-13T10:24:58.831Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 24 state to active
[ns_server:info,2019-03-13T10:24:58.832Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 23 state to active
[ns_server:info,2019-03-13T10:24:58.834Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 22 state to active
[ns_server:info,2019-03-13T10:24:58.834Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 21 state to active
[ns_server:info,2019-03-13T10:24:58.837Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 20 state to active
[ns_server:info,2019-03-13T10:24:58.838Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 19 state to active
[ns_server:info,2019-03-13T10:24:58.839Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 18 state to active
[ns_server:info,2019-03-13T10:24:58.841Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 17 state to active
[ns_server:info,2019-03-13T10:24:58.842Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 16 state to active
[ns_server:info,2019-03-13T10:24:58.843Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 15 state to active
[ns_server:info,2019-03-13T10:24:58.845Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 14 state to active
[ns_server:info,2019-03-13T10:24:58.846Z,ns_1@127.0.0.1:<0.936.0>:ns_memcached:do_handle_call:564]Changed vbucket 13 state to active
[ns_server:info,2019-03-13T10:24:58.849Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 12 state to active
[ns_server:info,2019-03-13T10:24:58.851Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 11 state to active
[ns_server:info,2019-03-13T10:24:58.854Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 10 state to active
[ns_server:info,2019-03-13T10:24:58.855Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 9 state to active
[ns_server:info,2019-03-13T10:24:58.857Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 8 state to active
[ns_server:info,2019-03-13T10:24:58.858Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 7 state to active
[ns_server:info,2019-03-13T10:24:58.862Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 6 state to active
[ns_server:info,2019-03-13T10:24:58.865Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 5 state to active
[ns_server:info,2019-03-13T10:24:58.868Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 4 state to active
[ns_server:info,2019-03-13T10:24:58.869Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 3 state to active
[ns_server:info,2019-03-13T10:24:58.872Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 2 state to active
[ns_server:info,2019-03-13T10:24:58.876Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 1 state to active
[ns_server:info,2019-03-13T10:24:58.880Z,ns_1@127.0.0.1:<0.935.0>:ns_memcached:do_handle_call:564]Changed vbucket 0 state to active
[ns_server:info,2019-03-13T10:24:58.895Z,ns_1@127.0.0.1:ns_memcached-main-bucket<0.916.0>:ns_memcached:handle_call:298]Enabling traffic to bucket "main-bucket"
[ns_server:info,2019-03-13T10:24:58.896Z,ns_1@127.0.0.1:ns_memcached-main-bucket<0.916.0>:ns_memcached:handle_call:302]Bucket "main-bucket" marked as warmed in 3 seconds
[ns_server:debug,2019-03-13T10:24:58.901Z,ns_1@127.0.0.1:<0.481.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
            1,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
              2,half_down,false}
[ns_server:debug,2019-03-13T10:24:59.915Z,ns_1@127.0.0.1:<0.481.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
            2,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
              3,half_down,false}
[ns_server:info,2019-03-13T10:24:59.968Z,ns_1@127.0.0.1:ns_doctor<0.295.0>:ns_doctor:update_status:322]The following buckets became ready on node 'ns_1@127.0.0.1': ["main-bucket"]
[ns_server:debug,2019-03-13T10:25:00.777Z,ns_1@127.0.0.1:cleanup_process<0.1176.0>:service_janitor:maybe_init_topology_aware_service:80]Doing initial topology change for service `index'
[ns_server:debug,2019-03-13T10:25:00.787Z,ns_1@127.0.0.1:service_rebalancer-index<0.1177.0>:service_agent:wait_for_agents:73]Waiting for the service agents for service index to come up on nodes:
['ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:25:00.788Z,ns_1@127.0.0.1:service_rebalancer-index<0.1177.0>:service_agent:wait_for_agents_loop:91]All service agents are ready for index
[rebalance:info,2019-03-13T10:25:00.789Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.1191.0>:service_rebalancer:rebalance:110]Rebalancing service index with id <<"64e1a6cfea66e26ac05c9f070e8c19a2">>.
KeepNodes: ['ns_1@127.0.0.1']
EjectNodes: []
DeltaNodes: []
[ns_server:debug,2019-03-13T10:25:00.791Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.1191.0>:service_rebalancer:rebalance:115]Got node infos:
[{'ns_1@127.0.0.1',[{node_id,<<"809ca823974d9231aa4400d5407944e4">>},
                    {priority,3},
                    {opaque,null}]}]
[ns_server:debug,2019-03-13T10:25:00.826Z,ns_1@127.0.0.1:service_rebalancer-index-worker<0.1191.0>:service_rebalancer:rebalance:124]Using node 'ns_1@127.0.0.1' as a leader
[ns_server:debug,2019-03-13T10:25:00.872Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2019-03-13T10:25:00.873Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{17,63719691900}}]}]
[ns_server:debug,2019-03-13T10:25:00.873Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691900}}]}|
 <<"{\"MasterId\":\"809ca823974d9231aa4400d5407944e4\",\"RebalId\":\"64e1a6cfea66e26ac05c9f070e8c19a2\",\"Source\":0,\"Error\":\"\"}">>]
[ns_server:debug,2019-03-13T10:25:00.919Z,ns_1@127.0.0.1:<0.481.0>:auto_failover_logic:log_master_activity:172]Incremented down state:
{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
            3,half_down,false}
->{node_state,{'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>},
              4,half_down,false}
[ns_server:debug,2019-03-13T10:25:00.985Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{18,63719691900}}]}]
[ns_server:debug,2019-03-13T10:25:00.985Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/rebalance/RebalanceToken">>}]..)
[ns_server:debug,2019-03-13T10:25:00.985Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/rebalance/RebalanceToken">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691900}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:25:01.017Z,ns_1@127.0.0.1:service_rebalancer-index<0.1177.0>:service_rebalancer:run_rebalance:71]Worker terminated: {'EXIT',<0.1191.0>,normal}
[ns_server:debug,2019-03-13T10:25:01.028Z,ns_1@127.0.0.1:cleanup_process<0.1176.0>:service_janitor:maybe_init_topology_aware_service:83]Initial rebalance for `index` finished successfully
[ns_server:debug,2019-03-13T10:25:01.031Z,ns_1@127.0.0.1:terse_bucket_info_uploader-main-bucket<0.919.0>:terse_bucket_info_uploader:flush_refresh_msgs:83]Flushed 1 refresh messages
[ns_server:debug,2019-03-13T10:25:01.033Z,ns_1@127.0.0.1:cleanup_process<0.1176.0>:service_janitor:maybe_init_simple_service:71]Created initial service map for service `n1ql'
[ns_server:debug,2019-03-13T10:25:01.036Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{19,63719691901}}]}]
[ns_server:debug,2019-03-13T10:25:01.037Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {service_map,index},
                               {service_map,n1ql}]..)
[ns_server:debug,2019-03-13T10:25:01.037Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,index} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691901}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:25:01.042Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{20,63719691901}}]}]
[ns_server:debug,2019-03-13T10:25:01.044Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{service_map,n1ql} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691901}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2019-03-13T10:25:01.313Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{21,63719691901}}]}]
[ns_server:debug,2019-03-13T10:25:01.313Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/info/versionToken">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691901}}]}|
 <<"{\"Version\":3}">>]
[ns_server:debug,2019-03-13T10:25:01.316Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,<<"/indexing/info/versionToken">>}]..)
[ns_server:debug,2019-03-13T10:25:01.922Z,ns_1@127.0.0.1:<0.481.0>:auto_failover_logic:log_master_activity:170]Transitioned node {'ns_1@127.0.0.1',<<"809ca823974d9231aa4400d5407944e4">>} state half_down -> up
[ns_server:debug,2019-03-13T10:25:03.026Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2019-03-13T10:25:03.042Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.768.0>
[ns_server:debug,2019-03-13T10:25:03.042Z,ns_1@127.0.0.1:<0.768.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:25:03.043Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.768.0>
[ns_server:debug,2019-03-13T10:25:03.093Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_storage:handle_call:115]Writing interactively saved doc {docv2,
                                 {user,{{ok,"<ud>dbuser</ud>"},local}},
                                 [{roles,
                                   [{bucket_full_access,[any]},ro_admin]},
                                  {name,{ok,"<ud>Database User</ud>"}}],
                                 [{rev,{1,<<147,136,163,91>>}},
                                  {deleted,false},
                                  {last_modified,1552472703092}]}
[ns_server:debug,2019-03-13T10:25:03.094Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_storage:handle_call:115]Writing interactively saved doc {docv2,
                                    {auth,{"<ud>dbuser</ud>",local}},
                                    [{<<"plain">>,"*****"},
                                     {<<"sha512">>,
                                      {[{<<"h">>,"*****"},
                                        {<<"s">>,
                                         <<"cXw8jLrL/mk3fM9qo6lzwIw2UlZkBbJzzmGafBf8Izwj1pbPuHCnWl4jVGNT9sdN7G6dFtzQbQ0f25BjCmYp9g==">>},
                                        {<<"i">>,4000}]}},
                                     {<<"sha256">>,
                                      {[{<<"h">>,"*****"},
                                        {<<"s">>,
                                         <<"HYM/pjpkZZ4NXvZCpwrZC0byElgUOpGep3ax2ay/pRo=">>},
                                        {<<"i">>,4000}]}},
                                     {<<"sha1">>,
                                      {[{<<"h">>,"*****"},
                                        {<<"s">>,
                                         <<"qeQ1E784cDIlGY6qpiLk8Dpl3qo=">>},
                                        {<<"i">>,4000}]}}],
                                    [{rev,{1,<<29,152,231,88>>}},
                                     {deleted,false},
                                     {last_modified,1552472703094}]}
[ns_server:debug,2019-03-13T10:25:03.096Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[6,0],
                                                                {0,1169890317},
                                                                true,
                                                                [{"main-bucket",
                                                                  <<"74fdafa75a3427824a57ff13d8c62f4b">>}]} to {[6,
                                                                                                                 0],
                                                                                                                {1,
                                                                                                                 1169890317},
                                                                                                                true,
                                                                                                                [{"main-bucket",
                                                                                                                  <<"74fdafa75a3427824a57ff13d8c62f4b">>}]}
[ns_server:debug,2019-03-13T10:25:03.099Z,ns_1@127.0.0.1:ns_audit<0.376.0>:ns_audit:handle_call:110]Audit set_user: [{full_name,<<"<ud>Database User</ud>">>},
                 {roles,[<<"ro_admin">>,<<"bucket_full_access[*]">>]},
                 {identity,{[{domain,local},{user,<<"<ud>dbuser</ud>">>}]}},
                 {real_userid,{[{domain,builtin},
                                {user,<<"<ud>admin</ud>">>}]}},
                 {remote,{[{ip,<<"172.20.0.2">>},{port,50868}]}},
                 {timestamp,<<"2019-03-13T10:25:03.096Z">>}]
[ns_server:debug,2019-03-13T10:25:03.102Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2019-03-13T10:25:03.107Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2019-03-13T10:25:03.110Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.245.0>
[ns_server:debug,2019-03-13T10:25:03.110Z,ns_1@127.0.0.1:memcached_permissions<0.245.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:25:03.117Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.245.0>
[ns_server:debug,2019-03-13T10:25:03.125Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2019-03-13T10:25:03.134Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2019-03-13T10:25:03.184Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.242.0>
[ns_server:debug,2019-03-13T10:25:03.184Z,ns_1@127.0.0.1:memcached_passwords<0.242.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{auth,'_'},'_','_'},[],['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:25:03.187Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.242.0>
[ns_server:debug,2019-03-13T10:25:03.196Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2019-03-13T10:25:03.205Z,ns_1@127.0.0.1:memcached_refresh<0.173.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[ns_server:debug,2019-03-13T10:25:03.746Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2019-03-13T10:25:03.748Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@",admin}
[ns_server:debug,2019-03-13T10:25:14.089Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>admin</ud>",admin}
[ns_server:debug,2019-03-13T10:25:14.101Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:354]Suspended by process <0.1204.0>
[ns_server:debug,2019-03-13T10:25:14.101Z,ns_1@127.0.0.1:<0.1204.0>:replicated_dets:select_from_dets_locked:399]Starting select with {users_storage,[{{docv2,{user,{'_','_'}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2019-03-13T10:25:14.105Z,ns_1@127.0.0.1:users_storage<0.181.0>:replicated_dets:handle_call:361]Released by process <0.1204.0>
[ns_server:debug,2019-03-13T10:25:20.574Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:25:20.575Z,ns_1@127.0.0.1:<0.2076.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:25:20.576Z,ns_1@127.0.0.1:<0.2078.0>:compaction_new_daemon:bucket_needs_compaction:971]`main-bucket` data size is 223327, disk size is 4241408
[ns_server:debug,2019-03-13T10:25:20.576Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:25:20.577Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:25:20.579Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:25:20.585Z,ns_1@127.0.0.1:<0.2079.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:25:20.586Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:25:20.586Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:25:23.807Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@index-cbauth",admin}
[ns_server:debug,2019-03-13T10:25:36.411Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@cbq-engine-cbauth",admin}
[ns_server:debug,2019-03-13T10:25:36.655Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"<ud>dbuser</ud>",local}
[ns_server:debug,2019-03-13T10:25:37.146Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{22,63719691937}}]}]
[ns_server:debug,2019-03-13T10:25:37.146Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/5070926143484941682/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691937}}]}|
 <<"{\"DefnId\":5070926143484941682,\"BucketUUID\":\"74fdafa75a3427824a57ff13d8c62f4b\",\"Definitions\":{\"809ca823974d9231aa4400d5407944e4\":[{\"defnId\":5070926143484941682,\"name\":\"sg_syncDocs_x1\",\"using\":\"GSI\",\"bucket\":\"main-bucket\",\"secExprs\":[\"(meta().`id`)\"],\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"where\":\"((meta().`id`) like \\\"\\\\\\\\_sync:%\\\")\",\"desc\":[false],\"deferred\":true,\"residentRatio\":"...>>]
[ns_server:debug,2019-03-13T10:25:37.151Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/5070926143484941682/0">>}]..)
[ns_server:debug,2019-03-13T10:25:37.597Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{23,63719691937}}]}]
[ns_server:debug,2019-03-13T10:25:37.597Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/9412130430611319344/0">>}]..)
[ns_server:debug,2019-03-13T10:25:37.598Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/9412130430611319344/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691937}}]}|
 <<"{\"DefnId\":9412130430611319344,\"BucketUUID\":\"74fdafa75a3427824a57ff13d8c62f4b\",\"Definitions\":{\"809ca823974d9231aa4400d5407944e4\":[{\"defnId\":9412130430611319344,\"name\":\"sg_access_x1\",\"using\":\"GSI\",\"bucket\":\"main-bucket\",\"secExprs\":[\"(all (array (`op`.`name`) for `op` in object_pairs((((meta().`xattrs`).`_sync`).`access`)) end))\"],\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"desc\":[false"...>>]
[ns_server:debug,2019-03-13T10:25:37.885Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/5070926143484941682/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691937}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:25:37.885Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{24,63719691937}}]}]
[ns_server:debug,2019-03-13T10:25:37.884Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/5070926143484941682/0">>}]..)
[ns_server:debug,2019-03-13T10:25:37.907Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/9412130430611319344/0">>}]..)
[ns_server:debug,2019-03-13T10:25:37.907Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/9412130430611319344/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691937}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:25:37.907Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{25,63719691937}}]}]
[ns_server:debug,2019-03-13T10:25:38.122Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{26,63719691938}}]}]
[ns_server:debug,2019-03-13T10:25:38.125Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/15487761953577109451/0">>}]..)
[ns_server:debug,2019-03-13T10:25:38.122Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/15487761953577109451/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691938}}]}|
 <<"{\"DefnId\":15487761953577109451,\"BucketUUID\":\"74fdafa75a3427824a57ff13d8c62f4b\",\"Definitions\":{\"809ca823974d9231aa4400d5407944e4\":[{\"defnId\":15487761953577109451,\"name\":\"sg_roleAccess_x1\",\"using\":\"GSI\",\"bucket\":\"main-bucket\",\"secExprs\":[\"(all (array (`op`.`name`) for `op` in object_pairs((((meta().`xattrs`).`_sync`).`role_access`)) end))\"],\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"d"...>>]
[ns_server:debug,2019-03-13T10:25:38.630Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{27,63719691938}}]}]
[ns_server:debug,2019-03-13T10:25:38.631Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/1463642697455786761/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691938}}]}|
 <<"{\"DefnId\":1463642697455786761,\"BucketUUID\":\"74fdafa75a3427824a57ff13d8c62f4b\",\"Definitions\":{\"809ca823974d9231aa4400d5407944e4\":[{\"defnId\":1463642697455786761,\"name\":\"sg_channels_x1\",\"using\":\"GSI\",\"bucket\":\"main-bucket\",\"secExprs\":[\"(all (array [(`op`.`name`), least((((meta().`xattrs`).`_sync`).`sequence`), ((`op`.`val`).`seq`)), ifmissing(((`op`.`val`).`rev`), null), ifmissing(((`op`."...>>]
[ns_server:debug,2019-03-13T10:25:38.631Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/1463642697455786761/0">>}]..)
[ns_server:debug,2019-03-13T10:25:38.896Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/1463642697455786761/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691938}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:25:38.896Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{28,63719691938}}]}]
[ns_server:debug,2019-03-13T10:25:38.900Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/1463642697455786761/0">>}]..)
[ns_server:debug,2019-03-13T10:25:38.910Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/15487761953577109451/0">>}]..)
[ns_server:debug,2019-03-13T10:25:38.923Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/15487761953577109451/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691938}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:25:38.923Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{29,63719691938}}]}]
[ns_server:debug,2019-03-13T10:25:39.259Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{30,63719691939}}]}]
[ns_server:debug,2019-03-13T10:25:39.259Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/16489805899542037717/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691939}}]}|
 <<"{\"DefnId\":16489805899542037717,\"BucketUUID\":\"74fdafa75a3427824a57ff13d8c62f4b\",\"Definitions\":{\"809ca823974d9231aa4400d5407944e4\":[{\"defnId\":16489805899542037717,\"name\":\"sg_allDocs_x1\",\"using\":\"GSI\",\"bucket\":\"main-bucket\",\"secExprs\":[\"(((meta().`xattrs`).`_sync`).`sequence`)\",\"(((meta().`xattrs`).`_sync`).`rev`)\",\"(((meta().`xattrs`).`_sync`).`flags`)\",\"(((meta().`xattrs`).`_sync`).`del"...>>]
[ns_server:debug,2019-03-13T10:25:39.262Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/16489805899542037717/0">>}]..)
[ns_server:debug,2019-03-13T10:25:39.887Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/9611986325896916243/0">>}]..)
[ns_server:debug,2019-03-13T10:25:39.888Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{31,63719691939}}]}]
[ns_server:debug,2019-03-13T10:25:39.888Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/9611986325896916243/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691939}}]}|
 <<"{\"DefnId\":9611986325896916243,\"BucketUUID\":\"74fdafa75a3427824a57ff13d8c62f4b\",\"Definitions\":{\"809ca823974d9231aa4400d5407944e4\":[{\"defnId\":9611986325896916243,\"name\":\"sg_tombstones_x1\",\"using\":\"GSI\",\"bucket\":\"main-bucket\",\"secExprs\":[\"(((meta().`xattrs`).`_sync`).`tombstoned_at`)\"],\"exprType\":\"N1QL\",\"partitionScheme\":\"SINGLE\",\"desc\":[false],\"deferred\":true,\"retainDeletedXATTR\":true,\"re"...>>]
[ns_server:debug,2019-03-13T10:25:39.993Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{32,63719691939}}]}]
[ns_server:debug,2019-03-13T10:25:39.993Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/build/16489805899542037717">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691939}}]}|
 <<"{\"Name\":\"\",\"Bucket\":\"\",\"DefnId\":16489805899542037717}">>]
[ns_server:debug,2019-03-13T10:25:39.996Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/build/16489805899542037717">>}]..)
[ns_server:debug,2019-03-13T10:25:40.004Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{33,63719691939}}]}]
[ns_server:debug,2019-03-13T10:25:40.004Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/build/9412130430611319344">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691939}}]}|
 <<"{\"Name\":\"\",\"Bucket\":\"\",\"DefnId\":9412130430611319344}">>]
[ns_server:debug,2019-03-13T10:25:40.011Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/build/9412130430611319344">>}]..)
[ns_server:debug,2019-03-13T10:25:40.023Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{34,63719691940}}]}]
[ns_server:debug,2019-03-13T10:25:40.024Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/build/1463642697455786761">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691940}}]}|
 <<"{\"Name\":\"\",\"Bucket\":\"\",\"DefnId\":1463642697455786761}">>]
[ns_server:debug,2019-03-13T10:25:40.025Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/build/1463642697455786761">>}]..)
[ns_server:debug,2019-03-13T10:25:40.031Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{35,63719691940}}]}]
[ns_server:debug,2019-03-13T10:25:40.032Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/build/9611986325896916243">>}]..)
[ns_server:debug,2019-03-13T10:25:40.031Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/build/9611986325896916243">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691940}}]}|
 <<"{\"Name\":\"\",\"Bucket\":\"\",\"DefnId\":9611986325896916243}">>]
[ns_server:debug,2019-03-13T10:25:40.035Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{36,63719691940}}]}]
[ns_server:debug,2019-03-13T10:25:40.036Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/build/15487761953577109451">>}]..)
[ns_server:debug,2019-03-13T10:25:40.036Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/build/15487761953577109451">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691940}}]}|
 <<"{\"Name\":\"\",\"Bucket\":\"\",\"DefnId\":15487761953577109451}">>]
[ns_server:debug,2019-03-13T10:25:40.037Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/9611986325896916243/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691940}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:25:40.038Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{37,63719691940}}]}]
[ns_server:debug,2019-03-13T10:25:40.039Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/9611986325896916243/0">>}]..)
[ns_server:debug,2019-03-13T10:25:40.045Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{38,63719691940}}]}]
[ns_server:debug,2019-03-13T10:25:40.045Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/build/5070926143484941682">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{1,63719691940}}]}|
 <<"{\"Name\":\"\",\"Bucket\":\"\",\"DefnId\":5070926143484941682}">>]
[ns_server:debug,2019-03-13T10:25:40.047Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/build/5070926143484941682">>}]..)
[ns_server:debug,2019-03-13T10:25:40.051Z,ns_1@127.0.0.1:ns_config_rep<0.257.0>:ns_config_rep:do_push_keys:330]Replicating some config keys ([{local_changes_count,
                                   <<"809ca823974d9231aa4400d5407944e4">>},
                               {metakv,
                                   <<"/indexing/ddl/commandToken/create/16489805899542037717/0">>}]..)
[ns_server:debug,2019-03-13T10:25:40.055Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{metakv,<<"/indexing/ddl/commandToken/create/16489805899542037717/0">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{2,63719691940}}]}|
 '_deleted']
[ns_server:debug,2019-03-13T10:25:40.055Z,ns_1@127.0.0.1:ns_config_log<0.164.0>:ns_config_log:log_common:227]config change:
{local_changes_count,<<"809ca823974d9231aa4400d5407944e4">>} ->
[{'_vclock',[{<<"809ca823974d9231aa4400d5407944e4">>,{39,63719691940}}]}]
[ns_server:debug,2019-03-13T10:25:41.008Z,ns_1@127.0.0.1:compiled_roles_cache<0.183.0>:menelaus_roles:build_compiled_roles:822]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2019-03-13T10:25:50.578Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:25:50.580Z,ns_1@127.0.0.1:<0.3629.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:25:50.586Z,ns_1@127.0.0.1:<0.3631.0>:compaction_new_daemon:bucket_needs_compaction:971]`main-bucket` data size is 223327, disk size is 4241408
[ns_server:debug,2019-03-13T10:25:50.587Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:25:50.587Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:25:50.587Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:25:50.651Z,ns_1@127.0.0.1:<0.3633.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:25:50.671Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:25:50.672Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:26:20.588Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:26:20.590Z,ns_1@127.0.0.1:<0.4869.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:26:20.596Z,ns_1@127.0.0.1:<0.4871.0>:compaction_new_daemon:bucket_needs_compaction:971]`main-bucket` data size is 224137, disk size is 4262068
[ns_server:debug,2019-03-13T10:26:20.596Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:26:20.597Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:26:20.674Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:26:20.690Z,ns_1@127.0.0.1:<0.4880.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:26:20.692Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:26:20.693Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:26:50.598Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:26:50.599Z,ns_1@127.0.0.1:<0.6115.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:26:50.602Z,ns_1@127.0.0.1:<0.6117.0>:compaction_new_daemon:bucket_needs_compaction:971]`main-bucket` data size is 224137, disk size is 4262068
[ns_server:debug,2019-03-13T10:26:50.602Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:26:50.602Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:26:50.694Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:26:50.700Z,ns_1@127.0.0.1:<0.6132.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:26:50.701Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:26:50.702Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:27:20.604Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_kv) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:27:20.606Z,ns_1@127.0.0.1:<0.7361.0>:compaction_new_daemon:spawn_scheduled_kv_compactor:471]Start compaction of vbuckets for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:27:20.610Z,ns_1@127.0.0.1:<0.7363.0>:compaction_new_daemon:bucket_needs_compaction:971]`main-bucket` data size is 224137, disk size is 4262068
[ns_server:debug,2019-03-13T10:27:20.610Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:27:20.610Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2019-03-13T10:27:20.704Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_scheduler_message:1311]Starting compaction (compact_views) for the following buckets: 
[<<"main-bucket">>]
[ns_server:info,2019-03-13T10:27:20.711Z,ns_1@127.0.0.1:<0.7378.0>:compaction_new_daemon:spawn_scheduled_views_compactor:497]Start compaction of indexes for bucket main-bucket with config: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2019-03-13T10:27:20.711Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_new_daemon:process_compactors_exit:1352]Finished compaction iteration.
[ns_server:debug,2019-03-13T10:27:20.712Z,ns_1@127.0.0.1:compaction_new_daemon<0.423.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
